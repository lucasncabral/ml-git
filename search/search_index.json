{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ML-Git \u00b6 1 ML-Git is a tool which provides a Distributed Version Control system to enable efficient dataset management. Like its name emphasizes, it is inspired in git concepts and workflows, ML-Git enables the following operations: Manage a repository of different datasets, labels and models. Distribute these ML artifacts between members of a team or across organizations. Apply the right data governance and security models to their artifacts. If you are seeking to learn more about ML-Git, access ML-Git Page . How to install \u00b6 Prerequisites: Git Python 3.6.1+ Pip 20.1.1+ From repository: pip install ml-git From source code: Download ML-Git from repository and execute commands below: cd ml-git/ pip install . How to uninstall \u00b6 pip uninstall ml-git How to configure \u00b6 1 - As ML-Git leverages git to manage ML entities metadata, it is necessary to configure user name and email address: git config --global user.name \"Your User\" git config --global user.email \"your_email@example.com\" 2 - OPTIONAL CONFIGURATIONS - 2.1 - Some ML-Git commands have a wizard to help you during their execution. Those commands have the --wizard option available to enable this wizard. However, you can configure the wizard to be enabled by default on all supported commands by running the following command: ``` ml-git repository config --set-wizard=enabled ``` 2.2 - You can also allow commands and options to be autocompleted with a [Tab] key press. For that, take a look at the following link ML-Git Shell Completion Support . 3 - Storage: ML-Git needs a configured storage to store data from managed artifacts. Please take a look at the ML-Git architecture and internals documentation to better understand how ML-Git works internally with data. To configure the storage see documentation about supported storages and how to configure each one. 4 - ML-Git project: An ML-Git project is an initialized directory that will contain a configuration file to be used by ML-Git in managing entities. To configure it you can use the basic steps to configure the project described in first project documentation. Usage \u00b6 ml-git --help Usage: ml-git [OPTIONS] COMMAND [ARGS]... Options: --version Show the version and exit. Commands: clone Clone an ml-git repository ML_GIT_REPOSITORY_URL datasets Management of datasets within this ml-git repository. labels Management of labels sets within this ml-git repository. models Management of models within this ml-git repository. repository Management of this ml-git repository. Basic commands \u00b6 ml-git clone <repository-url> ml-git clone https://github.com/user/ml_git_configuration_file_example.git If you prefer to create a new directory to clone into: ml-git clone https://github.com/user/ml_git_configuration_file_example.git my-project-dir If you prefer keep git tracking files in the project: ml-git clone https://github.com/user/ml_git_configuration_file_example.git --track ml-git <ml-entity> create This command will help you to start a new project, it creates your project artifact metadata: ml-git datasets create --categories=\"computer-vision, images\" --bucket-name=your_bucket --import=../import-path --mutability=strict dataset-ex Demonstration video: ml-git <ml-entity> status Show changes in project workspace: ml-git datasets status dataset-ex Demonstration video: ml-git <ml-entity> add Add new files to index: ml-git datasets add dataset-ex To increment version: ml-git datasets add dataset-ex --bumpversion Add an specific file: ml-git datasets add dataset-ex data/file_name.ex Demonstration video: ml-git <ml-entity> commit Consolidate added files in the index to repository: ml-git datasets commit dataset-ex Demonstration video: ml-git <ml-entity> push Upload metadata to remote repository and send chunks to storage: ml-git datasets push dataset-ex Demonstration video: ml-git <ml-entity> checkout Change workspace and metadata to versioned ml-entity tag: ml-git datasets checkout computer-vision__images__dataset-ex__1 Demonstration video: More about commands in documentation How to contribute \u00b6 Your contributions are always welcome! Fork the repository into your own GitHub Clone the repository to your local machine Create a new branch for your changes using the following pattern (feature | bugfix | hotfix)/branch_name . Example: feature/sftp_storage_implementation Make changes and test Push the changes to your repository Create a Pull Request from your forked repository to the ML-Git repository with comprehensive description of changes Another way to contribute with the community is creating an issue to track your ideas, doubts, enhancements, tasks, or bugs found. If an issue with the same topic already exists, discuss on the issue. Links \u00b6 ML-Git API documentation - Find the commands that are available in our api, usage examples and more. Working with tabular data - Find suggestions on how to use ml-git with tabular data. ml-git data specialization plugins - Dynamically link third-party packages to add specialized behaviors for the data type.","title":"Introduction"},{"location":"#ml-git","text":"1 ML-Git is a tool which provides a Distributed Version Control system to enable efficient dataset management. Like its name emphasizes, it is inspired in git concepts and workflows, ML-Git enables the following operations: Manage a repository of different datasets, labels and models. Distribute these ML artifacts between members of a team or across organizations. Apply the right data governance and security models to their artifacts. If you are seeking to learn more about ML-Git, access ML-Git Page .","title":"ML-Git"},{"location":"#how-to-install","text":"Prerequisites: Git Python 3.6.1+ Pip 20.1.1+ From repository: pip install ml-git From source code: Download ML-Git from repository and execute commands below: cd ml-git/ pip install .","title":"How to install"},{"location":"#how-to-uninstall","text":"pip uninstall ml-git","title":"How to uninstall"},{"location":"#how-to-configure","text":"1 - As ML-Git leverages git to manage ML entities metadata, it is necessary to configure user name and email address: git config --global user.name \"Your User\" git config --global user.email \"your_email@example.com\" 2 - OPTIONAL CONFIGURATIONS - 2.1 - Some ML-Git commands have a wizard to help you during their execution. Those commands have the --wizard option available to enable this wizard. However, you can configure the wizard to be enabled by default on all supported commands by running the following command: ``` ml-git repository config --set-wizard=enabled ``` 2.2 - You can also allow commands and options to be autocompleted with a [Tab] key press. For that, take a look at the following link ML-Git Shell Completion Support . 3 - Storage: ML-Git needs a configured storage to store data from managed artifacts. Please take a look at the ML-Git architecture and internals documentation to better understand how ML-Git works internally with data. To configure the storage see documentation about supported storages and how to configure each one. 4 - ML-Git project: An ML-Git project is an initialized directory that will contain a configuration file to be used by ML-Git in managing entities. To configure it you can use the basic steps to configure the project described in first project documentation.","title":"How to configure"},{"location":"#usage","text":"ml-git --help Usage: ml-git [OPTIONS] COMMAND [ARGS]... Options: --version Show the version and exit. Commands: clone Clone an ml-git repository ML_GIT_REPOSITORY_URL datasets Management of datasets within this ml-git repository. labels Management of labels sets within this ml-git repository. models Management of models within this ml-git repository. repository Management of this ml-git repository.","title":"Usage"},{"location":"#basic-commands","text":"ml-git clone <repository-url> ml-git clone https://github.com/user/ml_git_configuration_file_example.git If you prefer to create a new directory to clone into: ml-git clone https://github.com/user/ml_git_configuration_file_example.git my-project-dir If you prefer keep git tracking files in the project: ml-git clone https://github.com/user/ml_git_configuration_file_example.git --track ml-git <ml-entity> create This command will help you to start a new project, it creates your project artifact metadata: ml-git datasets create --categories=\"computer-vision, images\" --bucket-name=your_bucket --import=../import-path --mutability=strict dataset-ex Demonstration video: ml-git <ml-entity> status Show changes in project workspace: ml-git datasets status dataset-ex Demonstration video: ml-git <ml-entity> add Add new files to index: ml-git datasets add dataset-ex To increment version: ml-git datasets add dataset-ex --bumpversion Add an specific file: ml-git datasets add dataset-ex data/file_name.ex Demonstration video: ml-git <ml-entity> commit Consolidate added files in the index to repository: ml-git datasets commit dataset-ex Demonstration video: ml-git <ml-entity> push Upload metadata to remote repository and send chunks to storage: ml-git datasets push dataset-ex Demonstration video: ml-git <ml-entity> checkout Change workspace and metadata to versioned ml-entity tag: ml-git datasets checkout computer-vision__images__dataset-ex__1 Demonstration video: More about commands in documentation","title":"Basic commands"},{"location":"#how-to-contribute","text":"Your contributions are always welcome! Fork the repository into your own GitHub Clone the repository to your local machine Create a new branch for your changes using the following pattern (feature | bugfix | hotfix)/branch_name . Example: feature/sftp_storage_implementation Make changes and test Push the changes to your repository Create a Pull Request from your forked repository to the ML-Git repository with comprehensive description of changes Another way to contribute with the community is creating an issue to track your ideas, doubts, enhancements, tasks, or bugs found. If an issue with the same topic already exists, discuss on the issue.","title":"How to contribute"},{"location":"#links","text":"ML-Git API documentation - Find the commands that are available in our api, usage examples and more. Working with tabular data - Find suggestions on how to use ml-git with tabular data. ml-git data specialization plugins - Dynamically link third-party packages to add specialized behaviors for the data type.","title":"Links"},{"location":"advanced_scenarios/","text":"Additional use cases \u00b6 As you get familiar with ML-Git, you might feel the necessity of use advanced ML-Git features to solve your problems. Thus, this section aims to provide advanced scenarios and additional use cases. Keeping track of a dataset \u00b6 Often, users can share the same dataset. As the dataset improve, you will need to keep track of the changes. It is very simple to keep check what is new in a shared repository. You just need to navigate to the root of your project. Then, you can execute the command update , it will update the metadata repository, allowing visibility of what has been changed since the last update. For example, new ML entity and/or new versions. ml-git repository update In case something new exists in this repository, you will see a output like: INFO - Metadata Manager: Pull [/home/Documents/my-mlgit-project-config/.ml-git/datasets/metadata] INFO - Metadata Manager: Pull [/home/Documents/my-mlgit-project-config/.ml-git/labels/metadata] Then, you can checkout the new available data. Linking labels to a dataset \u00b6 ML-Git provides support for users link an entitity to another. In this example, we show how to link labels to a dataset. To accomplish this use case, you will need to have a dataset versioned by ML-Git in your repository. First, you need to configure your remote repository. Then, you can configure your storage. It is a similarly process as you did to configure your repository and storage for your dataset. ml-git repository remote labels add git@github.com:example/your-mlgit-labels.git ml-git repository storage add mlgit-labels --endpoint-url=<minio-endpoint-url> ml-git labels init Even, we are using a different bucket to store the labels data. It would be possible to store both datasets and labels into the same bucket. If you look at your config file using the command: ml-git repository config You should see something similar to the following config file: config: {'batch_size': 20, 'cache_path': '', 'datasets': {'git': 'git@github.com:example/your-mlgit-datasets.git'}, 'index_path': '', 'labels': {'git': 'git@github.com:example/your-mlgit-labels.git'}, 'metadata_path': '', 'mlgit_conf': 'config.yaml', 'mlgit_path': '.ml-git', 'models': {'git': ''}, 'object_path': '', 'refs_path': '', 'storages': {'s3h': {'mlgit-datasets': {'aws-credentials': {'profile': 'default'}, 'endpoint-url': <minio-endpoint-url>, 'region': 'us-east-1'}}}, 's3h': {'mlgit-labels': {'aws-credentials': {'profile': 'default'}, 'endpoint-url': <minio-endpoint-url>, 'region': 'us-east-1'}}}, 'verbose': 'info'} Then, you can create your first set of labels. As an example, we will use your-labels. ML-Git expects any set of labels to be specified under the labels/ directory of your project. Also, it expects a specification file with the name of the labels . ml-git labels create your-labels --categories=\"computer-vision, labels\" --mutability=mutable --storage-type=s3h --bucket-name=mlgit-labels --version=1 After create the entity, you can create the README.md describing your set of labels. Below, we show an example of caption labels for the your-labels directory and file structure: your-labels/ \u251c\u2500\u2500 README.md \u251c\u2500\u2500 annotations \u2502 \u251c\u2500\u2500 captions_train2014.json \u2502 \u2514\u2500\u2500 captions_val2014.json \u2514\u2500\u2500 your-labels.spec Now, you are ready to version the new set of labels. For this, do: ml-git labels add your-labels ml-git labels commit your-labels --dataset=your-datasets ml-git labels push your-labels The commands are very similar to dataset operations. However, you can note one particular change in the commit command. There is an option \" --dataset \" which is used to tell ML-Git that the labels should be linked to the specified dataset. With the following command, it is possible to see what datasets are associated with this labels ml-git labels show your-labels The output will looks like: -- labels : your-labels -- categories: - computer-vision - captions dataset: sha: 607fa818da716c3313a6855eb3bbd4587e412816 tag: computer-vision__images__mscoco__1 manifest: files: MANIFEST.yaml storage: s3h://mlgit-datasets name: your-captions version: 1 As you can see, there is a new section \" dataset \" that has been added by ML-Git with the sha & tag fields. It can be used to checkout the exact version of the dataset for that set of labels. Uploading labels related to a dataset: Adding special credentials AWS \u00b6 Depending the project you are working on, you might need to use special credentials to restrict access to your entities (e.g., datases) stored inside a S3/MinIO bucket. The easiest way to configure and use a different credentials for the AWS storage is installing the AWS command-line interface (awscli). First, install the awscli. Then, run the following command: aws configure --profile=mlgit You will need to inform the fields listed below: AWS Access Key ID [None]: your-access-key AWS Secret Access Key [None]: your-secret-access-key Default region name [None]: bucket-region Default output format [None]: json These commands will create the files ~/.aws/credentials and ~/.aws/config. Below, you can see a short video on how to configure the AWS profile: After you have created your special credentials (e.g., mlgit profile) You can use this profile as parameter to add your storages. Following, you can see an exaple of how to attach the profile to the storage mlgit-datasets. ml-git repository storage add mlgit-datasets --credentials=mlgit --endpoint-url=<minio-endpoint-url> Resources Inicialization using script \u00b6 You can find the script following the step below. It remotely creates the configurations, and during the execution it will generate a config repository containing the configurations pointing to the metadata repository (GitHub) and storage (AWS S3 or Azure Blob). If you are using Linux , execute on the terminal: cd ml-git ./scripts/resources_initialization/resources_initialization.sh If you are using Windows , execute on the CMD or Powershell: cd ml-git .\\scripts\\resources_initialization\\resources_initialization.bat At the end of executing this script, you will be able to directly execute a clone command to download your ML-Git project. Checking Data Integrity \u00b6 If at some point you want to check the integrity of the metadata repository (e.g. computer shutdown during a process), simply type the following command: ml-git datasets fsck That command will walk through the internal ML-Git directories (index & local repository) and will check the integrity of all blobs under its management. It will return the list of blobs that are corrupted. Checking Data Integrity:","title":"Additional Use Cases"},{"location":"advanced_scenarios/#additional-use-cases","text":"As you get familiar with ML-Git, you might feel the necessity of use advanced ML-Git features to solve your problems. Thus, this section aims to provide advanced scenarios and additional use cases.","title":"Additional use cases"},{"location":"advanced_scenarios/#keeping-track-of-a-dataset","text":"Often, users can share the same dataset. As the dataset improve, you will need to keep track of the changes. It is very simple to keep check what is new in a shared repository. You just need to navigate to the root of your project. Then, you can execute the command update , it will update the metadata repository, allowing visibility of what has been changed since the last update. For example, new ML entity and/or new versions. ml-git repository update In case something new exists in this repository, you will see a output like: INFO - Metadata Manager: Pull [/home/Documents/my-mlgit-project-config/.ml-git/datasets/metadata] INFO - Metadata Manager: Pull [/home/Documents/my-mlgit-project-config/.ml-git/labels/metadata] Then, you can checkout the new available data.","title":"Keeping track of a dataset"},{"location":"advanced_scenarios/#linking-labels-to-a-dataset","text":"ML-Git provides support for users link an entitity to another. In this example, we show how to link labels to a dataset. To accomplish this use case, you will need to have a dataset versioned by ML-Git in your repository. First, you need to configure your remote repository. Then, you can configure your storage. It is a similarly process as you did to configure your repository and storage for your dataset. ml-git repository remote labels add git@github.com:example/your-mlgit-labels.git ml-git repository storage add mlgit-labels --endpoint-url=<minio-endpoint-url> ml-git labels init Even, we are using a different bucket to store the labels data. It would be possible to store both datasets and labels into the same bucket. If you look at your config file using the command: ml-git repository config You should see something similar to the following config file: config: {'batch_size': 20, 'cache_path': '', 'datasets': {'git': 'git@github.com:example/your-mlgit-datasets.git'}, 'index_path': '', 'labels': {'git': 'git@github.com:example/your-mlgit-labels.git'}, 'metadata_path': '', 'mlgit_conf': 'config.yaml', 'mlgit_path': '.ml-git', 'models': {'git': ''}, 'object_path': '', 'refs_path': '', 'storages': {'s3h': {'mlgit-datasets': {'aws-credentials': {'profile': 'default'}, 'endpoint-url': <minio-endpoint-url>, 'region': 'us-east-1'}}}, 's3h': {'mlgit-labels': {'aws-credentials': {'profile': 'default'}, 'endpoint-url': <minio-endpoint-url>, 'region': 'us-east-1'}}}, 'verbose': 'info'} Then, you can create your first set of labels. As an example, we will use your-labels. ML-Git expects any set of labels to be specified under the labels/ directory of your project. Also, it expects a specification file with the name of the labels . ml-git labels create your-labels --categories=\"computer-vision, labels\" --mutability=mutable --storage-type=s3h --bucket-name=mlgit-labels --version=1 After create the entity, you can create the README.md describing your set of labels. Below, we show an example of caption labels for the your-labels directory and file structure: your-labels/ \u251c\u2500\u2500 README.md \u251c\u2500\u2500 annotations \u2502 \u251c\u2500\u2500 captions_train2014.json \u2502 \u2514\u2500\u2500 captions_val2014.json \u2514\u2500\u2500 your-labels.spec Now, you are ready to version the new set of labels. For this, do: ml-git labels add your-labels ml-git labels commit your-labels --dataset=your-datasets ml-git labels push your-labels The commands are very similar to dataset operations. However, you can note one particular change in the commit command. There is an option \" --dataset \" which is used to tell ML-Git that the labels should be linked to the specified dataset. With the following command, it is possible to see what datasets are associated with this labels ml-git labels show your-labels The output will looks like: -- labels : your-labels -- categories: - computer-vision - captions dataset: sha: 607fa818da716c3313a6855eb3bbd4587e412816 tag: computer-vision__images__mscoco__1 manifest: files: MANIFEST.yaml storage: s3h://mlgit-datasets name: your-captions version: 1 As you can see, there is a new section \" dataset \" that has been added by ML-Git with the sha & tag fields. It can be used to checkout the exact version of the dataset for that set of labels. Uploading labels related to a dataset:","title":"Linking labels to a dataset"},{"location":"advanced_scenarios/#adding-special-credentials-aws","text":"Depending the project you are working on, you might need to use special credentials to restrict access to your entities (e.g., datases) stored inside a S3/MinIO bucket. The easiest way to configure and use a different credentials for the AWS storage is installing the AWS command-line interface (awscli). First, install the awscli. Then, run the following command: aws configure --profile=mlgit You will need to inform the fields listed below: AWS Access Key ID [None]: your-access-key AWS Secret Access Key [None]: your-secret-access-key Default region name [None]: bucket-region Default output format [None]: json These commands will create the files ~/.aws/credentials and ~/.aws/config. Below, you can see a short video on how to configure the AWS profile: After you have created your special credentials (e.g., mlgit profile) You can use this profile as parameter to add your storages. Following, you can see an exaple of how to attach the profile to the storage mlgit-datasets. ml-git repository storage add mlgit-datasets --credentials=mlgit --endpoint-url=<minio-endpoint-url>","title":"Adding special credentials AWS"},{"location":"advanced_scenarios/#resources-inicialization-using-script","text":"You can find the script following the step below. It remotely creates the configurations, and during the execution it will generate a config repository containing the configurations pointing to the metadata repository (GitHub) and storage (AWS S3 or Azure Blob). If you are using Linux , execute on the terminal: cd ml-git ./scripts/resources_initialization/resources_initialization.sh If you are using Windows , execute on the CMD or Powershell: cd ml-git .\\scripts\\resources_initialization\\resources_initialization.bat At the end of executing this script, you will be able to directly execute a clone command to download your ML-Git project.","title":"Resources Inicialization using script  "},{"location":"advanced_scenarios/#checking-data-integrity","text":"If at some point you want to check the integrity of the metadata repository (e.g. computer shutdown during a process), simply type the following command: ml-git datasets fsck That command will walk through the internal ML-Git directories (index & local repository) and will check the integrity of all blobs under its management. It will return the list of blobs that are corrupted. Checking Data Integrity:","title":"Checking Data Integrity"},{"location":"aws_s3_configuration/","text":"S3 bucket configuration \u00b6 This section explains how to configure the settings that the ML-Git uses to interact with your bucket. This requires that you have the following data: Profile Name Access Key ID Secret Access Key Region Name Output Format The Access Key ID and Secret Access Key are your credentials. The Region Name identifies the AWS Region whose servers you want to send your requests. The Output Format specifies how the results are formatted. ML-Git allows you to have your bucket directly on AWS infrastructure or through MinIO. This document is divided into two sections wich describe how configure each one of these. AWS \u00b6 Internally ML-Git uses Boto3 to communicate with AWS services. Boto3 is the Amazon Web Services (AWS) SDK for Python. It enables Python developers to create, configure, and manage AWS services. Boto3 looks at various configuration locations until it finds configuration values. The following lookup order is used searching through sources for configuration values: Environment variables The ~/.aws/config file Note: If, when creating a storage, you define a specific profile to be used, Boto3 will only search for that profile in the ~/.aws/config file. You can configure the AWS in three ways (environment variables, through the console or with the AWS Command Line Interface ). These are described in the following sections. 1 - Environment Variables Linux or macOS : export AWS_ACCESS_KEY_ID=your-access-key export AWS_SECRET_ACCESS_KEY=your-secret-access-key export AWS_DEFAULT_REGION=us-west-2 Windows : setx AWS_ACCESS_KEY_ID your-access-key setx AWS_SECRET_ACCESS_KEY your-secret-access-key setx AWS_DEFAULT_REGION us-west-2 2 - Console From the home directory (UserProfile) execute: mkdir .aws You need to create two files to store the sensitive credential information (\\~/.aws/credentials) separated from the less sensitive configuration options (\\~/.aws/config). To create these two files type the following commands: For config file: echo \" [your-profile-name] region=bucket-region output=json \" > .aws/config For credentials file: echo \" [your-profile-name] aws_access_key_id = your-access-key aws_secret_access_key = your-secret-access-key \" > .aws/credentials 3 - AWS CLI For general use, the aws configure command is the fastest way to set up but requires the AWS CLI installed. To install and configure type the following commands: pip install awscli aws configure AWS Access Key ID [None]: your-access-key AWS Secret Access Key [None]: your-secret-access-key Default region name [None]: bucket-region Default output format [None]: json These commands will create the files ~/.aws/credentials and ~/.aws/config. Demonstrating AWS Configure","title":"S3"},{"location":"aws_s3_configuration/#s3-bucket-configuration","text":"This section explains how to configure the settings that the ML-Git uses to interact with your bucket. This requires that you have the following data: Profile Name Access Key ID Secret Access Key Region Name Output Format The Access Key ID and Secret Access Key are your credentials. The Region Name identifies the AWS Region whose servers you want to send your requests. The Output Format specifies how the results are formatted. ML-Git allows you to have your bucket directly on AWS infrastructure or through MinIO. This document is divided into two sections wich describe how configure each one of these.","title":"S3 bucket configuration"},{"location":"aws_s3_configuration/#aws","text":"Internally ML-Git uses Boto3 to communicate with AWS services. Boto3 is the Amazon Web Services (AWS) SDK for Python. It enables Python developers to create, configure, and manage AWS services. Boto3 looks at various configuration locations until it finds configuration values. The following lookup order is used searching through sources for configuration values: Environment variables The ~/.aws/config file Note: If, when creating a storage, you define a specific profile to be used, Boto3 will only search for that profile in the ~/.aws/config file. You can configure the AWS in three ways (environment variables, through the console or with the AWS Command Line Interface ). These are described in the following sections. 1 - Environment Variables Linux or macOS : export AWS_ACCESS_KEY_ID=your-access-key export AWS_SECRET_ACCESS_KEY=your-secret-access-key export AWS_DEFAULT_REGION=us-west-2 Windows : setx AWS_ACCESS_KEY_ID your-access-key setx AWS_SECRET_ACCESS_KEY your-secret-access-key setx AWS_DEFAULT_REGION us-west-2 2 - Console From the home directory (UserProfile) execute: mkdir .aws You need to create two files to store the sensitive credential information (\\~/.aws/credentials) separated from the less sensitive configuration options (\\~/.aws/config). To create these two files type the following commands: For config file: echo \" [your-profile-name] region=bucket-region output=json \" > .aws/config For credentials file: echo \" [your-profile-name] aws_access_key_id = your-access-key aws_secret_access_key = your-secret-access-key \" > .aws/credentials 3 - AWS CLI For general use, the aws configure command is the fastest way to set up but requires the AWS CLI installed. To install and configure type the following commands: pip install awscli aws configure AWS Access Key ID [None]: your-access-key AWS Secret Access Key [None]: your-secret-access-key Default region name [None]: bucket-region Default output format [None]: json These commands will create the files ~/.aws/credentials and ~/.aws/config. Demonstrating AWS Configure","title":"AWS"},{"location":"azure_configurations/","text":"Azure container configuration \u00b6 ML-Git allows the user to choose to have their data stored in an Azure Blob Storage that provides massively scalable storage for unstructured data like images, videos, or documents. This section explains how to configure the settings that ML-Git uses to interact with your Azure container. To establish the connection between ML-Git and Azure services, you will need a connection string which can be found on the Azure portal. See the image below: With this connection string in hand, you can configure your environment in two ways (this order is the one used by ML-Git to get your credentials): Environment Variable Azure CLI 1. Environment Variable \u00b6 You can add the connection string to your system's set of variables. ML-Git will look for the variable AZURE_STORAGE_CONNECTION_STRING . To add the system variable, run the following command: Windows : setx AZURE_STORAGE_CONNECTION_STRING \"<yourconnectionstring>\" Linux or macOS : export AZURE_STORAGE_CONNECTION_STRING=\"<yourconnectionstring>\" 2. Azure CLI \u00b6 The Azure command-line interface (Azure CLI) is a set of commands used to create and manage Azure resources. The Azure CLI is available across Azure services and is designed to get you working quickly with Azure, with an emphasis on automation. Azure CLI uses a file to store the configurations that are used by its services. To add settings to the file, simply run the following command: az configure If tou prefer, you can create a configuration file through the console. The configuration file itself is located at $AZURE_CONFIG_DIR/config . The default value of AZURE_CONFIG_DIR is $HOME/.azure on Linux and macOS, and %USERPROFILE%\\.azure on Windows. From the home directory (UserProfile) execute: mkdir .azure You need to create the config file with the connection string value: echo \" [storage] connection_string = \"<yourconnectionstring>\" \" > .azure/config","title":"Azure"},{"location":"azure_configurations/#azure-container-configuration","text":"ML-Git allows the user to choose to have their data stored in an Azure Blob Storage that provides massively scalable storage for unstructured data like images, videos, or documents. This section explains how to configure the settings that ML-Git uses to interact with your Azure container. To establish the connection between ML-Git and Azure services, you will need a connection string which can be found on the Azure portal. See the image below: With this connection string in hand, you can configure your environment in two ways (this order is the one used by ML-Git to get your credentials): Environment Variable Azure CLI","title":"Azure container configuration"},{"location":"azure_configurations/#1-environment-variable","text":"You can add the connection string to your system's set of variables. ML-Git will look for the variable AZURE_STORAGE_CONNECTION_STRING . To add the system variable, run the following command: Windows : setx AZURE_STORAGE_CONNECTION_STRING \"<yourconnectionstring>\" Linux or macOS : export AZURE_STORAGE_CONNECTION_STRING=\"<yourconnectionstring>\"","title":" 1. Environment Variable "},{"location":"azure_configurations/#2-azure-cli","text":"The Azure command-line interface (Azure CLI) is a set of commands used to create and manage Azure resources. The Azure CLI is available across Azure services and is designed to get you working quickly with Azure, with an emphasis on automation. Azure CLI uses a file to store the configurations that are used by its services. To add settings to the file, simply run the following command: az configure If tou prefer, you can create a configuration file through the console. The configuration file itself is located at $AZURE_CONFIG_DIR/config . The default value of AZURE_CONFIG_DIR is $HOME/.azure on Linux and macOS, and %USERPROFILE%\\.azure on Windows. From the home directory (UserProfile) execute: mkdir .azure You need to create the config file with the connection string value: echo \" [storage] connection_string = \"<yourconnectionstring>\" \" > .azure/config","title":" 2. Azure CLI "},{"location":"centralized_cache_and_objects/","text":"Centralized cache \u00b6 Centralized cache is a configuration mode that allows cached files to be shared between multiple users on the same machine, reducing the total cost of disk space. Currently, this feature works only in Linux and derivative machines. :warning: Caution: We encourage the use of centralized cache just with mutability set as strict . It is necessary to deactivate the feature fs.protected_hardlinks , because ML-Git uses hardlink to share cache files. Be aware that changing this setting is a risky operation, as malicious people can exploit this (see the extract below). Do this only if you really need to use the Centralized Cache feature. Remember to revert this change if you will stop to use Centralized Cache. Please read this extract from kernel.org about protected_hardlinks setting: protected_hardlinks: A long-standing class of security issues is the hardlink-based time-of-check-time-of-use race, most commonly seen in world-writable directories like /tmp. The common method of exploitation of this flaw is to cross privilege boundaries when following a given hardlink (i.e. a root process follows a hardlink created by another user). Additionally, on systems without separated partitions, this stops unauthorized users from \"pinning\" vulnerable setuid/setgid files against being upgraded by the administrator, or linking to special files. When set to \"0\", hardlink creation behavior is unrestricted. When set to \"1\" hardlinks cannot be created by users if they do not already own the source file, or do not have read/write access to it. This protection is based on the restrictions in Openwall and grsecurity. Changing fs.protected_hardlinks: Execute in terminal: sudo gedit /etc/sysctl.conf Search for: #fs.protected_hardlinks = 0 and uncomment (remove \u2018#\u2019). If you didn't find it, add a line with #fs.protected_hardlinks = 0 to this file Then execute: sudo sysctl -p Requirements \u00b6 Machine's root user (administrator). Configuration steps \u00b6 1 - Create a common directory for each entity with read and write permission for all users: sudo mkdir -p /srv/mlgit/cache/dataset sudo mkdir -p /srv/mlgit/cache/labels sudo mkdir -p /srv/mlgit/cache/model Change permissions: sudo chmod -R a+rwX /srv/mlgit/cache/dataset sudo chmod -R a+rwX /srv/mlgit/cache/labels sudo chmod -R a+rwX /srv/mlgit/cache/model 2 - With the project ml-git initialized change .ml-git/config.yaml : datasets: git: '' cache_path: 'Cache path directory created on step 1 for dataset entity' labels: git: '' cache_path: 'Path directory created on step 1 for labels entity' models: git: '' cache_path: 'Path directory created on step 1 for model entity' storages: {} Centralized objects \u00b6 Centralized objects is a configuration that allow to user share ml-git\u2019s data between machine\u2019s users, avoiding downloading times. Requirements \u00b6 Machine's root user (administrator). Configuration steps \u00b6 1 - Create a common directory for each entity with read and write permission for all users: For Windows users mkdir \\a %ALLUSERSPROFILE%\\mlgit\\objects\\dataset mkdir \\a %ALLUSERSPROFILE%\\mlgit\\objects\\labels mkdir \\a %ALLUSERSPROFILE%\\mlgit\\objects\\model or mkgit \\a C:\\ProgramData\\mlgit\\objects\\dataset mkgit \\a C:\\ProgramData\\mlgit\\objects\\labels mkgit \\a C:\\ProgramData\\mlgit\\objects\\model For Linux and derivatives users sudo mkdir -p /srv/mlgit/objects/dataset sudo mkdir -p /srv/mlgit/objects/labels sudo mkdir -p /srv/mlgit/objects/model Change permissions sudo chmod -R a+rwX /srv/mlgit/objects/dataset sudo chmod -R a+rwX /srv/mlgit/objects/labels sudo chmod -R a+rwX /srv/mlgit/objects/model 2 - With the project ml-git initialized change .ml-git/config.yaml : datasets: git: '' objects_path: 'Path directory created on step 1 for dataset entity' labels: git: '' objects_path: 'Path directory created on step 1 for labels entity' models: git: '' objects_path: 'Path directory created on step 1 for model entity' storages: {}","title":"Cache and Objects"},{"location":"centralized_cache_and_objects/#centralized-cache","text":"Centralized cache is a configuration mode that allows cached files to be shared between multiple users on the same machine, reducing the total cost of disk space. Currently, this feature works only in Linux and derivative machines. :warning: Caution: We encourage the use of centralized cache just with mutability set as strict . It is necessary to deactivate the feature fs.protected_hardlinks , because ML-Git uses hardlink to share cache files. Be aware that changing this setting is a risky operation, as malicious people can exploit this (see the extract below). Do this only if you really need to use the Centralized Cache feature. Remember to revert this change if you will stop to use Centralized Cache. Please read this extract from kernel.org about protected_hardlinks setting: protected_hardlinks: A long-standing class of security issues is the hardlink-based time-of-check-time-of-use race, most commonly seen in world-writable directories like /tmp. The common method of exploitation of this flaw is to cross privilege boundaries when following a given hardlink (i.e. a root process follows a hardlink created by another user). Additionally, on systems without separated partitions, this stops unauthorized users from \"pinning\" vulnerable setuid/setgid files against being upgraded by the administrator, or linking to special files. When set to \"0\", hardlink creation behavior is unrestricted. When set to \"1\" hardlinks cannot be created by users if they do not already own the source file, or do not have read/write access to it. This protection is based on the restrictions in Openwall and grsecurity. Changing fs.protected_hardlinks: Execute in terminal: sudo gedit /etc/sysctl.conf Search for: #fs.protected_hardlinks = 0 and uncomment (remove \u2018#\u2019). If you didn't find it, add a line with #fs.protected_hardlinks = 0 to this file Then execute: sudo sysctl -p","title":"Centralized cache"},{"location":"centralized_cache_and_objects/#requirements","text":"Machine's root user (administrator).","title":"Requirements"},{"location":"centralized_cache_and_objects/#configuration-steps","text":"1 - Create a common directory for each entity with read and write permission for all users: sudo mkdir -p /srv/mlgit/cache/dataset sudo mkdir -p /srv/mlgit/cache/labels sudo mkdir -p /srv/mlgit/cache/model Change permissions: sudo chmod -R a+rwX /srv/mlgit/cache/dataset sudo chmod -R a+rwX /srv/mlgit/cache/labels sudo chmod -R a+rwX /srv/mlgit/cache/model 2 - With the project ml-git initialized change .ml-git/config.yaml : datasets: git: '' cache_path: 'Cache path directory created on step 1 for dataset entity' labels: git: '' cache_path: 'Path directory created on step 1 for labels entity' models: git: '' cache_path: 'Path directory created on step 1 for model entity' storages: {}","title":"Configuration steps"},{"location":"centralized_cache_and_objects/#centralized-objects","text":"Centralized objects is a configuration that allow to user share ml-git\u2019s data between machine\u2019s users, avoiding downloading times.","title":"Centralized objects"},{"location":"centralized_cache_and_objects/#requirements_1","text":"Machine's root user (administrator).","title":"Requirements"},{"location":"centralized_cache_and_objects/#configuration-steps_1","text":"1 - Create a common directory for each entity with read and write permission for all users: For Windows users mkdir \\a %ALLUSERSPROFILE%\\mlgit\\objects\\dataset mkdir \\a %ALLUSERSPROFILE%\\mlgit\\objects\\labels mkdir \\a %ALLUSERSPROFILE%\\mlgit\\objects\\model or mkgit \\a C:\\ProgramData\\mlgit\\objects\\dataset mkgit \\a C:\\ProgramData\\mlgit\\objects\\labels mkgit \\a C:\\ProgramData\\mlgit\\objects\\model For Linux and derivatives users sudo mkdir -p /srv/mlgit/objects/dataset sudo mkdir -p /srv/mlgit/objects/labels sudo mkdir -p /srv/mlgit/objects/model Change permissions sudo chmod -R a+rwX /srv/mlgit/objects/dataset sudo chmod -R a+rwX /srv/mlgit/objects/labels sudo chmod -R a+rwX /srv/mlgit/objects/model 2 - With the project ml-git initialized change .ml-git/config.yaml : datasets: git: '' objects_path: 'Path directory created on step 1 for dataset entity' labels: git: '' objects_path: 'Path directory created on step 1 for labels entity' models: git: '' objects_path: 'Path directory created on step 1 for model entity' storages: {}","title":"Configuration steps"},{"location":"developer_info/","text":"Contributing to ML-Git \u00b6 The ML-Git project welcomes, and depends, on contributions from developers and users in the open source community. Contributions can be made in a number of ways. The main way to contribute is following the next steps: Fork the repository into your own GitHub Clone the repository to your local machine Create a new branch for your changes using the following pattern (feature | bugfix | hotfix)/branch_name. Example: feature/sftp_storage_implementation Make changes and test Push the changes to your repository Create a Pull Request from your forked repository to the ML-Git repository with comprehensive description of changes Another way to contribute with the community is creating an issue to track your ideas, doubts, enhancements, tasks, or bugs found. If an issue with the same topic already exists, discuss on the issue. Installing for Development \u00b6 To be able to contribute with our project, you will need to have the following requirements in your machine: Python 3.6.1+ Pipenv Git Docker (required only for Integration Tests execution) Running the Tests \u00b6 After developing, you must run the unit and integration tests. To be able to do that: Install Docker: Windows Linux The Integration Tests script starts a MinIO container on your local machine (port 9000) to be used as storage during tests execution. [Optional] Install and configure Make to run tests easily: Windows Linux Configure git: git config --global user.name \"First Name and Last Name\" git config --global user.email \"your_name@example.com\" Running Unit Tests \u00b6 You can run unit tests through: Using Make \u00b6 Execute on terminal: cd ml-git make test.unit Without Make \u00b6 Linux Execute on terminal: cd ml-git sh ./scripts/run_unit_tests.sh Windows Execute on Powershell or CMD: cd ml-git .\\ scripts \\ run_unit_tests . bat Running Integration Tests \u00b6 You can run integration tests through: Using Make \u00b6 Execute on terminal: cd ml-git make test.integration Without Make \u00b6 Linux Execute on terminal: cd ml-git sh ./scripts/run_integration_tests.sh Windows Execute on Powershell or CMD: cd ml-git .\\ scripts \\ run_integration_tests . bat Google Drive Integration Test \u00b6 To run google drive integration test you need to: 1. Create directory tests/integration/credentials-json Put your credentials file with name credentials.json in the folder you created in step 1 Example of credentials.json: {\"installed\":{\"client_id\":\"fake_client_id \",\"project_id\":\"project\",\"auth_uri\":\"https://accounts.google.com/o/oauth2/auth\",\"token_uri\":\"https://oauth2.googleapis.com/token\",\"auth_provider_x509_cert_url\":\"https://www.googleapis.com/oauth2/v1/certs\",\"client_secret\":\"fake_client_secret \",\"redirect_uris\":[\"urn:ietf:wg:oauth:2.0:oob\",\"http://localhost\"]}} Create a folder with name mlgit/test-folder in your GDrive Create files mlgit/B and mlgit/test-folder/A with any content, make sure that files aren't Google Files. You should have the following structure in your drive: YourDrive | \u251c\u2500\u2500 mlgit \u2502 \u251c\u2500\u2500 B \u2502 \u2514\u2500\u2500 test-folder \u2502 \u2514\u2500\u2500 A Create tests/integration/gdrive-files-links.json with shared links of mlgit/B and mlgit/test-folder . Example of gdrive-files-links.json: { \"test-folder\": \"https://drive.google.com/drive/folders/1MvWrQtPVDuJ5-XB82dMwRI8XflBZ?usp=sharing\", \"B\": \"https://drive.google.com/file/d/1uy6Kao8byRqTPv-Plw8tuhITyh5N1Uua/view?usp=sharing\" } The Google Drive Integration Tests are set to not run by default (as they require extra setup, as mentioned earlier). To include the integration tests for Google Drive storage during an integration tests run, you should execute: Using Make \u00b6 Execute on terminal: cd ml-git make test.integration.gdrive Without Make \u00b6 Linux Execute on terminal: cd ml-git sh ./scripts/run_integration_tests.sh --gdrive Windows Execute on Powershell or CMD: cd ml-git .\\ scripts \\ run_integration_tests . bat - -gdrive Executing a Single Test File \u00b6 To execute a specific integration tests file, execute the run_integration_tests script accordingly with your operating system and pass the test file path relative to integration tests folder (tests/integration/). See the below examples running test_01_init.py located at ml-git/tests/integration/test_01_init.py : Linux: cd ml-git sh ./scripts/run_integration_tests.sh test_01_init.py Windows: cd ml-git .\\ scripts \\ run_integration_tests . bat test_01_init . py","title":"Contributing to ML-Git"},{"location":"developer_info/#contributing-to-ml-git","text":"The ML-Git project welcomes, and depends, on contributions from developers and users in the open source community. Contributions can be made in a number of ways. The main way to contribute is following the next steps: Fork the repository into your own GitHub Clone the repository to your local machine Create a new branch for your changes using the following pattern (feature | bugfix | hotfix)/branch_name. Example: feature/sftp_storage_implementation Make changes and test Push the changes to your repository Create a Pull Request from your forked repository to the ML-Git repository with comprehensive description of changes Another way to contribute with the community is creating an issue to track your ideas, doubts, enhancements, tasks, or bugs found. If an issue with the same topic already exists, discuss on the issue.","title":"Contributing to ML-Git"},{"location":"developer_info/#installing-for-development","text":"To be able to contribute with our project, you will need to have the following requirements in your machine: Python 3.6.1+ Pipenv Git Docker (required only for Integration Tests execution)","title":"Installing for Development"},{"location":"developer_info/#running-the-tests","text":"After developing, you must run the unit and integration tests. To be able to do that: Install Docker: Windows Linux The Integration Tests script starts a MinIO container on your local machine (port 9000) to be used as storage during tests execution. [Optional] Install and configure Make to run tests easily: Windows Linux Configure git: git config --global user.name \"First Name and Last Name\" git config --global user.email \"your_name@example.com\"","title":"Running the Tests"},{"location":"developer_info/#running-unit-tests","text":"You can run unit tests through:","title":"Running Unit Tests"},{"location":"developer_info/#using-make","text":"Execute on terminal: cd ml-git make test.unit","title":"Using Make"},{"location":"developer_info/#without-make","text":"Linux Execute on terminal: cd ml-git sh ./scripts/run_unit_tests.sh Windows Execute on Powershell or CMD: cd ml-git .\\ scripts \\ run_unit_tests . bat","title":"Without Make"},{"location":"developer_info/#running-integration-tests","text":"You can run integration tests through:","title":"Running Integration Tests"},{"location":"developer_info/#using-make_1","text":"Execute on terminal: cd ml-git make test.integration","title":"Using Make"},{"location":"developer_info/#without-make_1","text":"Linux Execute on terminal: cd ml-git sh ./scripts/run_integration_tests.sh Windows Execute on Powershell or CMD: cd ml-git .\\ scripts \\ run_integration_tests . bat","title":"Without Make"},{"location":"developer_info/#google-drive-integration-test","text":"To run google drive integration test you need to: 1. Create directory tests/integration/credentials-json Put your credentials file with name credentials.json in the folder you created in step 1 Example of credentials.json: {\"installed\":{\"client_id\":\"fake_client_id \",\"project_id\":\"project\",\"auth_uri\":\"https://accounts.google.com/o/oauth2/auth\",\"token_uri\":\"https://oauth2.googleapis.com/token\",\"auth_provider_x509_cert_url\":\"https://www.googleapis.com/oauth2/v1/certs\",\"client_secret\":\"fake_client_secret \",\"redirect_uris\":[\"urn:ietf:wg:oauth:2.0:oob\",\"http://localhost\"]}} Create a folder with name mlgit/test-folder in your GDrive Create files mlgit/B and mlgit/test-folder/A with any content, make sure that files aren't Google Files. You should have the following structure in your drive: YourDrive | \u251c\u2500\u2500 mlgit \u2502 \u251c\u2500\u2500 B \u2502 \u2514\u2500\u2500 test-folder \u2502 \u2514\u2500\u2500 A Create tests/integration/gdrive-files-links.json with shared links of mlgit/B and mlgit/test-folder . Example of gdrive-files-links.json: { \"test-folder\": \"https://drive.google.com/drive/folders/1MvWrQtPVDuJ5-XB82dMwRI8XflBZ?usp=sharing\", \"B\": \"https://drive.google.com/file/d/1uy6Kao8byRqTPv-Plw8tuhITyh5N1Uua/view?usp=sharing\" } The Google Drive Integration Tests are set to not run by default (as they require extra setup, as mentioned earlier). To include the integration tests for Google Drive storage during an integration tests run, you should execute:","title":"Google Drive Integration Test"},{"location":"developer_info/#using-make_2","text":"Execute on terminal: cd ml-git make test.integration.gdrive","title":"Using Make"},{"location":"developer_info/#without-make_2","text":"Linux Execute on terminal: cd ml-git sh ./scripts/run_integration_tests.sh --gdrive Windows Execute on Powershell or CMD: cd ml-git .\\ scripts \\ run_integration_tests . bat - -gdrive","title":"Without Make"},{"location":"developer_info/#executing-a-single-test-file","text":"To execute a specific integration tests file, execute the run_integration_tests script accordingly with your operating system and pass the test file path relative to integration tests folder (tests/integration/). See the below examples running test_01_init.py located at ml-git/tests/integration/test_01_init.py : Linux: cd ml-git sh ./scripts/run_integration_tests.sh test_01_init.py Windows: cd ml-git .\\ scripts \\ run_integration_tests . bat test_01_init . py","title":"Executing a Single Test File"},{"location":"downloadable_environment/","text":"Downloadable environment \u00b6 About \u00b6 This image enables new users to get started with ML-Git in a lightweight Linux-based image without worrying about configurations. The image also include a git repository with a predefined dataset and a minio instance populated with the dataset's data. How to use: \u00b6 Ensure that you have Docker installed. Inside root of ML-Git directory build the image locally with the following command: make docker.build or docker build -t mlgit_docker_env -f docker/Dockerfile . Run the Docker container to launch the built image: make docker.run or docker run -it -p 8888:8888 --name mlgit_env mlgit_docker_env Port 8888 will be used to start the jupyter notebook web service. Using the ML-Git with environment (inside docker container): \u00b6 The container has an ML-Git project initialized inside the directory workspace, the content of the versioned tag is an image from mnist database . You can execute the command checkout directly to tag: ml-git datasets checkout handwritten__digits__mnist__1 Summary of files in image: \u00b6 local_server.git (local git repository, used to store metadafiles). data (directory used by the bucket to store project data). init.sh (script that run basic command to use ml-git). minio (minio executable). local_ml_git_config_server.git (local git repositoy with configuration files, used by ml-git clone). ml-git (source code of ml-git). workspace (initialized ml-git project).","title":"Docker Environment"},{"location":"downloadable_environment/#downloadable-environment","text":"","title":"Downloadable environment"},{"location":"downloadable_environment/#about","text":"This image enables new users to get started with ML-Git in a lightweight Linux-based image without worrying about configurations. The image also include a git repository with a predefined dataset and a minio instance populated with the dataset's data.","title":"About"},{"location":"downloadable_environment/#how-to-use","text":"Ensure that you have Docker installed. Inside root of ML-Git directory build the image locally with the following command: make docker.build or docker build -t mlgit_docker_env -f docker/Dockerfile . Run the Docker container to launch the built image: make docker.run or docker run -it -p 8888:8888 --name mlgit_env mlgit_docker_env Port 8888 will be used to start the jupyter notebook web service.","title":"How to use:"},{"location":"downloadable_environment/#using-the-ml-git-with-environment-inside-docker-container","text":"The container has an ML-Git project initialized inside the directory workspace, the content of the versioned tag is an image from mnist database . You can execute the command checkout directly to tag: ml-git datasets checkout handwritten__digits__mnist__1","title":"Using the ML-Git with environment (inside docker container):"},{"location":"downloadable_environment/#summary-of-files-in-image","text":"local_server.git (local git repository, used to store metadafiles). data (directory used by the bucket to store project data). init.sh (script that run basic command to use ml-git). minio (minio executable). local_ml_git_config_server.git (local git repositoy with configuration files, used by ml-git clone). ml-git (source code of ml-git). workspace (initialized ml-git project).","title":"Summary of files in image:"},{"location":"first_project/","text":"Your 1st ML artefacts under ML-Git management \u00b6 We will divide this quick howto into 6 main sections: ML-Git repository configuration / intialization This section explains how to initialize and configure a repository for ML-Git, considering the scenarios of the storage be an S3 or a MinIO. Uploading a dataset Having a repository initialized, this section explains how to create and upload a dataset to the storage. Adding data to a dataset This section explains how to add new data to an entity already versioned by ML-Git. Uploading labels associated to a dataset This section describes how to upload a set of labels by associating the dataset to which these labels refer. Uploading models This section explains how to create and upload your models. Downloading a dataset This section describes how to download a versioned data set using ML-Git. Checking data integrity This section explains how to check the integrity of the metadata repository. At the end of each section there is a video to demonstrate the ML-Git usage. Initial configuration of ML-Git \u00b6 Make sure you have created your own git repository (more information) for dataset metadata and a S3 bucket or a MinIO server for the dataset actual data. If you haven't created it yet, you can use the resources initialization script which aims to facilitate the creation of resources (buckets and repositories). After that, create a ML-Git project. To do this, use the following commands (note that 'mlgit-project' is the project name used as example): mkdir mlgit-project && cd mlgit-project (or clone an existing repo from Github or Github Enterprise) ml-git repository init Now, we need to configure our project with the remote configurations. This section is divided into two parts according to the storage: Setting up a ml-git project with S3 and Setting up a ml-git project with MinIO . After configuring the project with the bucket, the remote ones, the credentials that will be used, and the other configurations that were performed in this section, a good practice is to make the version of the .ml-git folder that was generated in a git repository. That way in future projects or if you want to share with someone you can use the command ml-git clone to import the project's settings, without having to configure it for each new project. Setting up an ML-Git project with MinIO \u00b6 In addition to creating the MinIO server, it is necessary to configure the settings that the ML-Git uses to interact with your bucket, see how to configure MinIO for this. For a basic ML-Git repository, you need to add a remote repository for metadata and the MinIO bucket configuration. ml-git repository remote datasets add git@github.com:example/your-mlgit-datasets.git ml-git repository storage add mlgit-datasets --credentials=mlgit --endpoint-url=<minio-endpoint-url> Last but not least, initialize the metadata repository. ml-git datasets init Setting up an ML-Git project with MinIO: Setting up an ML-Git project with S3 \u00b6 Similar to the MinIO setup, in addition to creating the bucket in S3, it is necessary to configure the settings that the ML-Git uses to interact with your bucket, see how to configure a S3 bucket for more details. For a basic ML-Git repository, you need to add a remote repository for metadata and a S3 bucket configuration. ml-git repository remote datasets add git@github.com:example/your-mlgit-datasets.git ml-git repository storage add mlgit-datasets --credentials=mlgit After that initialize the metadata repository. ml-git datasets init Why ML-Git uses git? \u00b6 The ML-Git uses git to versioning project's metadata. See below versioned metadata: .spec , is the specification file that contains informations like version number, artefact name, entity type (dataset, label, model), categories (list of labels to categorize an entity). MANIFEST.yaml , is responsible to map artefact's files. The files are mapped by hashes, that are the references used to perform operations in local files, and download/upload operations in storages (S3, MinIO, Azure, GoogleDrive and SFTP). You can find more information about metadata here . All configurations are stored in .ml-git/config.yaml and you can look at configuration state at any time with the following command: ml-git repository config show Output: config: {'batch_size': 20, 'cache_path': '', 'datasets': {'git': 'git@github.com:example/your-mlgit-datasets.git'}, 'index_path': '', 'labels': {'git': ''}, 'metadata_path': '', 'mlgit_conf': 'config.yaml', 'mlgit_path': '.ml-git', 'models': {'git': ''}, 'object_path': '', 'push_threads_count': 10, 'refs_path': '', 'storages': {'s3h': {'mlgit-datasets': {'aws-credentials': {'profile': 'mlgit'}, 'endpoint-url': <minio-endpoint-url>, 'region': 'us-east-1'}}}, 'verbose': 'info'} Uploading a dataset \u00b6 To create and upload a dataset to a storage, you must be in an already initialized project, if necessary read section 1 to initialize and configure a project. ML-Git expects any dataset to be specified under datasets/ directory of your project and it expects a specification file with the name of the dataset. To create this specification file for a new entity you must run the following command: ml-git datasets create imagenet8 --categories=\"computer-vision, images\" --mutability=strict --storage-type=s3h --bucket-name=mlgit-datasets This command will create the dataset directory at the root of the project entity. If you want to create a version of your dataset in a different directory, you can use the --entity-dir parameter to inform the relative directory where the entity is to be created. Example: ml-git datasets create imagenet8 --categories=\"computer-vision, images\" --mutability=strict --storage-type=s3h --bucket-name=mlgit-datasets --entity-dir=folderA/folderB After that a file must have been created in datasets/folderA/folderB/imagenet8/imagenet8.spec and should look like this: dataset: categories: - computer-vision - images manifest: storage: s3h://mlgit-datasets mutability: strict name: imagenet8 version: 1 There are 5 main items in the spec file: name : it's the dataset name version : the version should be a positive integer, incremented each time a new version is pushed into ML-Git. You can use the --bumpversion as an argument to do the automatic increment when you add more files to a dataset. categories : labels to categorize the entity. This information is used by ML-Git to create the tag in the git repository managing the metadata. manifest : describes the data storage in which the data is actually stored. In the above example, a S3 bucket named mlgit-datasets . The AWS credential profile name and AWS region should be found in the ML-Git config file. mutability : describes the mutability option that your project has. The mutability options are \"strict\", \"flexible\" and \"mutable\", after selecting one of these options, you cannot change that. If you want to know more about each type of mutability and how it works, please take a look at mutability documentation . The items listed above are mandatory in the spec. An important point to note is if the user wishes, it is possible to add new items that will be versioned with the spec. The example below presents a spec with the entity's owner information to be versioned. Those information were put under metadata field just for purpose of organization. dataset: categories: - computer-vision - images mutability: strict manifest: storage: s3h://mlgit-datasets name: imagenet8 version: 1 metadata: owner: name: <your-name-here> email: <your-email-here> After creating the dataset spec file, you can create a README.md to create a web page describing your dataset, adding references and any other useful information. Then, you can put the data of that dataset under the directory. Below, you will see the tree of imagenet8 directory and file structure: imagenet8/ \u251c\u2500\u2500 README.md \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 train \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_1 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_2 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_3 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_4 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_5 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_6 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_7 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_8 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_9 \u2502 \u2502 \u2514\u2500\u2500 train_data_batch_10 \u2502 \u2514\u2500\u2500 val \u2502 \u2514\u2500\u2500 val_data \u2514\u2500\u2500 imagenet8.spec You can look at the working tree status with the following command: ml-git datasets status imagenet8 Output: INFO - Repository: dataset: status of ml-git index for [imagenet8] Changes to be committed untracked files imagenet8.spec README.md data\\train\\train_data_batch_1 data\\train\\train_data_batch_2 data\\train\\train_data_batch_3 data\\train\\train_data_batch_4 data\\train\\train_data_batch_5 data\\train\\train_data_batch_6 data\\train\\train_data_batch_7 data\\train\\train_data_batch_8 data\\train\\train_data_batch_9 data\\train\\train_data_batch_10 data\\val\\val_data corrupted files That command allows printing the tracked files and the ones in the index/staging area. Now, you are ready to put that new dataset under ML-Git management. For this, do: ml-git datasets add imagenet8 The command \" ml-git dataset add \" adds the files into a specific dataset, such as imagenet8 in the index/staging area. If you check the working tree status, you can see that now the files appear as tracked but not committed yet: ml-git datasets status imagenet8 Output: INFO - Repository: dataset: status of ml-git index for [imagenet8] Changes to be committed new file: data\\train\\train_data_batch_1 new file: data\\train\\train_data_batch_2 new file: data\\train\\train_data_batch_3 new file: data\\train\\train_data_batch_4 new file: data\\train\\train_data_batch_5 new file: data\\train\\train_data_batch_6 new file: data\\train\\train_data_batch_7 new file: data\\train\\train_data_batch_8 new file: data\\train\\train_data_batch_9 new file: data\\train\\train_data_batch_10 new file: data\\val\\val_data untracked files corrupted files Then, you can commit the metadata to the local repository. For this purpose, type the following command: ml-git datasets commit imagenet8 After that, you can use \" ml-git dataset push \" to update the remote metadata repository just after storing all actual data under management in the specified remote data storage. ml-git datasets push imagenet8 As you can observe, ML-Git follows very similar workflows as git. Uploading a dataset: Adding data to a dataset \u00b6 If you want to add data to a dataset, perform the following steps: In your workspace, copy the new data in under datasets/<your-dataset>/data Modify the version number. To do this step you have two ways: You can put the option --bumpversion on the add command to auto increment the version number, as shown below. Or, you can put the option --version on the commit command to set an specific version number. After that, like in the previous section, you need to execute the following commands to upload the new data: ml-git datasets add <your-dataset> --bumpversion ml-git datasets commit <your-dataset> ml-git datasets push <your-dataset> This will create a new version of your dataset and push the changes to your remote storage (e.g. S3). Adding data to a dataset: Uploading labels associated to a dataset \u00b6 To create and upload labels associated to a dataset, you must be in an already initialized project, if necessary read section 1 to initialize and configure the project. Also, you will need to have a dataset already versioned by ML-Git in your repository, see section 2 . The first step is to configure your metadata and data repository/storage. ml-git repository remote labels add git@github.com:example/your-mlgit-labels.git ml-git repository storage add mlgit-labels --endpoint-url=<minio-endpoint-url> ml-git labels init Even these commands show a different bucket to store the labels data. It would be possible to store both datasets and labels into the same bucket. If you look at your config file, you would see the following information: ml-git repository config show Output: config: {'batch_size': 20, 'cache_path': '', 'datasets': {'git': 'git@github.com:example/your-mlgit-datasets.git'}, 'index_path': '', 'labels': {'git': 'git@github.com:example/your-mlgit-labels.git'}, 'metadata_path': '', 'mlgit_conf': 'config.yaml', 'mlgit_path': '.ml-git', 'models': {'git': ''}, 'object_path': '', 'refs_path': '', 'storages': {'s3h': {'mlgit-datasets': {'aws-credentials': {'profile': 'default'}, 'endpoint-url': <minio-endpoint-url>, 'region': 'us-east-1'}}}, 's3h': {'mlgit-labels': {'aws-credentials': {'profile': 'default'}, 'endpoint-url': <minio-endpoint-url>, 'region': 'us-east-1'}}}, 'verbose': 'info'} Then, you can create your first set of labels. As an example, we will use mscoco. ML-Git expects any set of labels to be specified under the labels/ directory of your project. Also, it expects a specification file with the name of the labels . ml-git labels create mscoco-captions --categories=\"computer-vision, captions\" --mutability=mutable --storage-type=s3h --bucket-name=mlgit-labels --version=1 After create the entity, you can create the README.md describing your set of labels. Below is the tree of caption labels for the mscoco directory and file structure: mscoco-captions/ \u251c\u2500\u2500 README.md \u251c\u2500\u2500 annotations \u2502 \u251c\u2500\u2500 captions_train2014.json \u2502 \u2514\u2500\u2500 captions_val2014.json \u2514\u2500\u2500 mscoco-captions.spec Now, you are ready to put the new set of labels under ML-Git management. We assume there is an existing mscoco dataset. For this, do: ml-git labels add mscoco-captions ml-git labels commit mscoco-captions --dataset=mscoco ml-git labels push mscoco-captions The commands are very similar to dataset operations. However, you can note one particular change in the commit command. There is an option \" --dataset \" which is used to tell ML-Git that the labels should be linked to the specified dataset. Internally, ML-Git will look at the checked out dataset in your workspace for that specified dataset. Then, it will include the git tag and sha into the specification file to be committed into the metadata repository. Once done, anyone will be able to retrieve the exact same version of the dataset that has been used for that specific set of labels. One can look at the specific dataset associated with that set of labels by executing the following command: ml-git labels show mscoco-captions Output: -- labels : mscoco-captions -- categories: - computer-vision - captions dataset: sha: 607fa818da716c3313a6855eb3bbd4587e412816 tag: computer-vision__images__mscoco__1 manifest: files: MANIFEST.yaml storage: s3h://mlgit-datasets name: mscoco-captions version: 1 As you can see, there is a new section \" dataset \" that has been added by ML-Git with the sha & tag fields. It can be used to checkout the exact version of the dataset for that set of labels. Uploading labels related to a dataset: Uploading Models \u00b6 To create and upload your model, you must be in an already initialized project, if necessary read section 1 to initialize and configure a project. The first step is to configure your metadata & data repository/storage. ml-git repository remote models add git@github.com:example/your-mlgit-models.git ml-git repository storage add mlgit-models --endpoint-url=<minio-endpoint-url> ml-git models init To create a model entity, you can run the following command: ml-git models create imagenet-model --categories=\"computer-vision, images\" --storage-type=s3h --mutability=mutable --bucket-name=mlgit-models After creating the model, we add the model file to the data folder. Here below is the directory tree structure: imagenet-model/ \u251c\u2500\u2500 README.md \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 model_file \u2514\u2500\u2500 imagenet-model.spec Now, you're ready to put that new model set under ML-Git management. We assume there is an existing imagenet8 dataset and mscoco-captions labels. For this, do: ml-git models add imagenet-model ml-git models commit imagenet-model --dataset=imagenet8 --labels=mscoco-captions ml-git models push imagenet-model There is not much change compared to dataset and labels operation. You can use the options \" -- dataset \" and \" --labels \", which tells to ml-git that the model should be linked to the specified dataset and labels. Internally, ml-git will look in your workspace for the checked out dataset and labels specified in the options. It then will include the reference to the checked out versions into the model's specification file to be committed into the metadata repository. Once done, anyone will then be able to retrieve the exact same version of the dataset and labels that has been used for that specific model. Persisting model's metrics: We can insert metrics to the model in the add command, metrics can be added with the following parameters: metrics-file : optional metrics file path. It is expected a CSV file containing the metric names in the header and the values in the next line. metric : optional metric keys and values. An example of adding a model passing a metrics file, would be the following command: ml-git models add imagenet-model --metrics-file='/path/to/your/file.csv' An example of adding a model passing metrics through the command line, would be the following command: ml-git models add imagenet-model --metric accuracy 10 --metric precision 20 --metric recall 30 Obs: The parameters used above were chosen for example purposes, you can name your metrics however you want to, you can also pass as many metrics as you want, as long as you use the command correctly. When inserting the metrics, they will be included in the structure of your model's spec file. An example of what it would look like would be the following structure: model: categories: - computer-vision - images manifest: storage: s3h://mlgit-models metrics: accuracy: 10.0 precision: 20.0 recall: 30.0 name: imagenet-model version: 1 You can view metrics for all tags for that entity by running the following command: ml-git models metrics imagenet-model Downloading a dataset \u00b6 We assume there is an existing ML-Git repository with a few ML datasets under its management and you'd like to download one of the available datasets. If you don't have a dataset versioned by the ML-Git, see section 2 on how to do this. To download a dataset, you need to be in an initialized and configured ML-Git project. If you have a repository with your saved settings, you can run the following command to set up your environment: ml-git clone git@github.com:example/your-mlgit-repository.git Then, go to the project directory: cd your-mlgit-repository Inside the configured ML-Git project directory, the following command will update the metadata repository, allowing visibility of what has been shared since the last update (new ML entity, new versions). ml-git datasets update Or update all metadata repository: ml-git repository update To discover which datasets are under ML-Git management, you can execute the following command: ml-git datasets list Output: ML dataset |-- folderA | |-- folderB | | |-- dataset-ex-minio | | |-- imagenet8 | | |-- dataset-ex The ML-Git repository contains 3 different datasets, all falling under the same directories folderA/folderB (These directories were defined when the entity was created and can be modified at any time by the user). In order for ML-Git to manage the different versions of the same dataset. It internally creates a tag based on categories, ML entity name and its version. To show all these tag representing the versions of a dataset, simply type the following: ml-git datasets tag list imagenet8 Output: computer-vision__images__imagenet8__1 computer-vision__images__imagenet8__2 It means there are actually 2 versions under ML-Git management. You can check what version is checked out in the ML-Git workspace with the following command: ml-git datasets branch imagenet8 Output: ('vision-computing__images__imagenet8__2', '48ba1e994a1e39e1b508bff4a3302a5c1bb9063e') The output is a tuple: The tag auto-generated by ML-Git based on the .spec. The sha of the git commit of that version. It is simple to retrieve a specific version locally to start any experiment by executing one of the following commands: ml-git datasets checkout computer-vision__images__imagenet8__1 or ml-git datasets checkout imagenet8 --version=1 If you want to get the latest available version of an entity you can just pass its name in the checkout command, as shown below: ml-git datasets checkout imagenet8 Getting the data will auto-create a directory structure under dataset directory as shown below. That structure folderA/folderB is actually the structure in which the dataset was versioned. folderA \u2514\u2500\u2500 folderB \u2514\u2500\u2500 imagenet8 \u251c\u2500\u2500 README.md \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 train \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_1 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_2 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_3 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_4 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_5 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_6 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_7 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_8 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_9 \u2502 \u2502 \u2514\u2500\u2500 train_data_batch_10 \u2502 \u2514\u2500\u2500 val \u2502 \u2514\u2500\u2500 val_data \u2514\u2500\u2500 imagenet8.spec Downloading a dataset: Checking data integrity \u00b6 If at some point you want to check the integrity of the metadata repository (e.g. computer shuts down during a process), simply type the following command: ml-git datasets fsck INFO - HashFS: starting integrity check on [.\\.ml-git\\dataset\\objects\\hashfs] INFO - HashFS: starting integrity check on [.\\.ml-git\\dataset\\index\\hashfs] [1] corrupted file(s) in Local Repository [0] corrupted file(s) in Index Total of corrupted files: 1 INFO - Repository: For more information about the corrupted files you can run the command with the --verbose option. That command will walk through the internal ML-Git directories (index & local repository) and will check the integrity of all blobs under its management. It will fix and return the list of blobs that are corrupted or missing. Checking data integrity: Changing a Dataset \u00b6 When adding files to an entity ML-Git locks the files for read only. When the entity's mutability type is flexible or mutable, you can change the data of a file and resubmit it without being considered corrupted. In case of a flexible entity you should perform the following command to unlock the file: ml-git datasets unlock imagenet8 data\\train\\train_data_batch_1 After that, the unlocked file is subject to modification. If you modify the file without performing this command, it will be considered corrupted. To upload the data, you can execute the following commands: ml-git datasets add <yourdataset> --bumpversion ml-git datasets commit <yourdataset> ml-git datasets push <yourdataset> This will create a new version of your dataset and push the changes to your remote storage (e.g. S3). Changing a dataset:","title":"Your 1st ML artefacts under ML-Git management #"},{"location":"first_project/#your-1st-ml-artefacts-under-ml-git-management","text":"We will divide this quick howto into 6 main sections: ML-Git repository configuration / intialization This section explains how to initialize and configure a repository for ML-Git, considering the scenarios of the storage be an S3 or a MinIO. Uploading a dataset Having a repository initialized, this section explains how to create and upload a dataset to the storage. Adding data to a dataset This section explains how to add new data to an entity already versioned by ML-Git. Uploading labels associated to a dataset This section describes how to upload a set of labels by associating the dataset to which these labels refer. Uploading models This section explains how to create and upload your models. Downloading a dataset This section describes how to download a versioned data set using ML-Git. Checking data integrity This section explains how to check the integrity of the metadata repository. At the end of each section there is a video to demonstrate the ML-Git usage.","title":"Your 1st ML artefacts under ML-Git management"},{"location":"first_project/#initial-configuration-of-ml-git","text":"Make sure you have created your own git repository (more information) for dataset metadata and a S3 bucket or a MinIO server for the dataset actual data. If you haven't created it yet, you can use the resources initialization script which aims to facilitate the creation of resources (buckets and repositories). After that, create a ML-Git project. To do this, use the following commands (note that 'mlgit-project' is the project name used as example): mkdir mlgit-project && cd mlgit-project (or clone an existing repo from Github or Github Enterprise) ml-git repository init Now, we need to configure our project with the remote configurations. This section is divided into two parts according to the storage: Setting up a ml-git project with S3 and Setting up a ml-git project with MinIO . After configuring the project with the bucket, the remote ones, the credentials that will be used, and the other configurations that were performed in this section, a good practice is to make the version of the .ml-git folder that was generated in a git repository. That way in future projects or if you want to share with someone you can use the command ml-git clone to import the project's settings, without having to configure it for each new project.","title":" Initial configuration of ML-Git"},{"location":"first_project/#setting-up-an-ml-git-project-with-minio","text":"In addition to creating the MinIO server, it is necessary to configure the settings that the ML-Git uses to interact with your bucket, see how to configure MinIO for this. For a basic ML-Git repository, you need to add a remote repository for metadata and the MinIO bucket configuration. ml-git repository remote datasets add git@github.com:example/your-mlgit-datasets.git ml-git repository storage add mlgit-datasets --credentials=mlgit --endpoint-url=<minio-endpoint-url> Last but not least, initialize the metadata repository. ml-git datasets init Setting up an ML-Git project with MinIO:","title":"Setting up an ML-Git project with MinIO "},{"location":"first_project/#setting-up-an-ml-git-project-with-s3","text":"Similar to the MinIO setup, in addition to creating the bucket in S3, it is necessary to configure the settings that the ML-Git uses to interact with your bucket, see how to configure a S3 bucket for more details. For a basic ML-Git repository, you need to add a remote repository for metadata and a S3 bucket configuration. ml-git repository remote datasets add git@github.com:example/your-mlgit-datasets.git ml-git repository storage add mlgit-datasets --credentials=mlgit After that initialize the metadata repository. ml-git datasets init","title":"Setting up an ML-Git project with S3 "},{"location":"first_project/#why-ml-git-uses-git","text":"The ML-Git uses git to versioning project's metadata. See below versioned metadata: .spec , is the specification file that contains informations like version number, artefact name, entity type (dataset, label, model), categories (list of labels to categorize an entity). MANIFEST.yaml , is responsible to map artefact's files. The files are mapped by hashes, that are the references used to perform operations in local files, and download/upload operations in storages (S3, MinIO, Azure, GoogleDrive and SFTP). You can find more information about metadata here . All configurations are stored in .ml-git/config.yaml and you can look at configuration state at any time with the following command: ml-git repository config show Output: config: {'batch_size': 20, 'cache_path': '', 'datasets': {'git': 'git@github.com:example/your-mlgit-datasets.git'}, 'index_path': '', 'labels': {'git': ''}, 'metadata_path': '', 'mlgit_conf': 'config.yaml', 'mlgit_path': '.ml-git', 'models': {'git': ''}, 'object_path': '', 'push_threads_count': 10, 'refs_path': '', 'storages': {'s3h': {'mlgit-datasets': {'aws-credentials': {'profile': 'mlgit'}, 'endpoint-url': <minio-endpoint-url>, 'region': 'us-east-1'}}}, 'verbose': 'info'}","title":"Why ML-Git uses git?"},{"location":"first_project/#uploading-a-dataset","text":"To create and upload a dataset to a storage, you must be in an already initialized project, if necessary read section 1 to initialize and configure a project. ML-Git expects any dataset to be specified under datasets/ directory of your project and it expects a specification file with the name of the dataset. To create this specification file for a new entity you must run the following command: ml-git datasets create imagenet8 --categories=\"computer-vision, images\" --mutability=strict --storage-type=s3h --bucket-name=mlgit-datasets This command will create the dataset directory at the root of the project entity. If you want to create a version of your dataset in a different directory, you can use the --entity-dir parameter to inform the relative directory where the entity is to be created. Example: ml-git datasets create imagenet8 --categories=\"computer-vision, images\" --mutability=strict --storage-type=s3h --bucket-name=mlgit-datasets --entity-dir=folderA/folderB After that a file must have been created in datasets/folderA/folderB/imagenet8/imagenet8.spec and should look like this: dataset: categories: - computer-vision - images manifest: storage: s3h://mlgit-datasets mutability: strict name: imagenet8 version: 1 There are 5 main items in the spec file: name : it's the dataset name version : the version should be a positive integer, incremented each time a new version is pushed into ML-Git. You can use the --bumpversion as an argument to do the automatic increment when you add more files to a dataset. categories : labels to categorize the entity. This information is used by ML-Git to create the tag in the git repository managing the metadata. manifest : describes the data storage in which the data is actually stored. In the above example, a S3 bucket named mlgit-datasets . The AWS credential profile name and AWS region should be found in the ML-Git config file. mutability : describes the mutability option that your project has. The mutability options are \"strict\", \"flexible\" and \"mutable\", after selecting one of these options, you cannot change that. If you want to know more about each type of mutability and how it works, please take a look at mutability documentation . The items listed above are mandatory in the spec. An important point to note is if the user wishes, it is possible to add new items that will be versioned with the spec. The example below presents a spec with the entity's owner information to be versioned. Those information were put under metadata field just for purpose of organization. dataset: categories: - computer-vision - images mutability: strict manifest: storage: s3h://mlgit-datasets name: imagenet8 version: 1 metadata: owner: name: <your-name-here> email: <your-email-here> After creating the dataset spec file, you can create a README.md to create a web page describing your dataset, adding references and any other useful information. Then, you can put the data of that dataset under the directory. Below, you will see the tree of imagenet8 directory and file structure: imagenet8/ \u251c\u2500\u2500 README.md \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 train \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_1 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_2 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_3 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_4 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_5 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_6 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_7 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_8 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_9 \u2502 \u2502 \u2514\u2500\u2500 train_data_batch_10 \u2502 \u2514\u2500\u2500 val \u2502 \u2514\u2500\u2500 val_data \u2514\u2500\u2500 imagenet8.spec You can look at the working tree status with the following command: ml-git datasets status imagenet8 Output: INFO - Repository: dataset: status of ml-git index for [imagenet8] Changes to be committed untracked files imagenet8.spec README.md data\\train\\train_data_batch_1 data\\train\\train_data_batch_2 data\\train\\train_data_batch_3 data\\train\\train_data_batch_4 data\\train\\train_data_batch_5 data\\train\\train_data_batch_6 data\\train\\train_data_batch_7 data\\train\\train_data_batch_8 data\\train\\train_data_batch_9 data\\train\\train_data_batch_10 data\\val\\val_data corrupted files That command allows printing the tracked files and the ones in the index/staging area. Now, you are ready to put that new dataset under ML-Git management. For this, do: ml-git datasets add imagenet8 The command \" ml-git dataset add \" adds the files into a specific dataset, such as imagenet8 in the index/staging area. If you check the working tree status, you can see that now the files appear as tracked but not committed yet: ml-git datasets status imagenet8 Output: INFO - Repository: dataset: status of ml-git index for [imagenet8] Changes to be committed new file: data\\train\\train_data_batch_1 new file: data\\train\\train_data_batch_2 new file: data\\train\\train_data_batch_3 new file: data\\train\\train_data_batch_4 new file: data\\train\\train_data_batch_5 new file: data\\train\\train_data_batch_6 new file: data\\train\\train_data_batch_7 new file: data\\train\\train_data_batch_8 new file: data\\train\\train_data_batch_9 new file: data\\train\\train_data_batch_10 new file: data\\val\\val_data untracked files corrupted files Then, you can commit the metadata to the local repository. For this purpose, type the following command: ml-git datasets commit imagenet8 After that, you can use \" ml-git dataset push \" to update the remote metadata repository just after storing all actual data under management in the specified remote data storage. ml-git datasets push imagenet8 As you can observe, ML-Git follows very similar workflows as git. Uploading a dataset:","title":"Uploading a dataset"},{"location":"first_project/#adding-data-to-a-dataset","text":"If you want to add data to a dataset, perform the following steps: In your workspace, copy the new data in under datasets/<your-dataset>/data Modify the version number. To do this step you have two ways: You can put the option --bumpversion on the add command to auto increment the version number, as shown below. Or, you can put the option --version on the commit command to set an specific version number. After that, like in the previous section, you need to execute the following commands to upload the new data: ml-git datasets add <your-dataset> --bumpversion ml-git datasets commit <your-dataset> ml-git datasets push <your-dataset> This will create a new version of your dataset and push the changes to your remote storage (e.g. S3). Adding data to a dataset:","title":"Adding data to a dataset"},{"location":"first_project/#uploading-labels-associated-to-a-dataset","text":"To create and upload labels associated to a dataset, you must be in an already initialized project, if necessary read section 1 to initialize and configure the project. Also, you will need to have a dataset already versioned by ML-Git in your repository, see section 2 . The first step is to configure your metadata and data repository/storage. ml-git repository remote labels add git@github.com:example/your-mlgit-labels.git ml-git repository storage add mlgit-labels --endpoint-url=<minio-endpoint-url> ml-git labels init Even these commands show a different bucket to store the labels data. It would be possible to store both datasets and labels into the same bucket. If you look at your config file, you would see the following information: ml-git repository config show Output: config: {'batch_size': 20, 'cache_path': '', 'datasets': {'git': 'git@github.com:example/your-mlgit-datasets.git'}, 'index_path': '', 'labels': {'git': 'git@github.com:example/your-mlgit-labels.git'}, 'metadata_path': '', 'mlgit_conf': 'config.yaml', 'mlgit_path': '.ml-git', 'models': {'git': ''}, 'object_path': '', 'refs_path': '', 'storages': {'s3h': {'mlgit-datasets': {'aws-credentials': {'profile': 'default'}, 'endpoint-url': <minio-endpoint-url>, 'region': 'us-east-1'}}}, 's3h': {'mlgit-labels': {'aws-credentials': {'profile': 'default'}, 'endpoint-url': <minio-endpoint-url>, 'region': 'us-east-1'}}}, 'verbose': 'info'} Then, you can create your first set of labels. As an example, we will use mscoco. ML-Git expects any set of labels to be specified under the labels/ directory of your project. Also, it expects a specification file with the name of the labels . ml-git labels create mscoco-captions --categories=\"computer-vision, captions\" --mutability=mutable --storage-type=s3h --bucket-name=mlgit-labels --version=1 After create the entity, you can create the README.md describing your set of labels. Below is the tree of caption labels for the mscoco directory and file structure: mscoco-captions/ \u251c\u2500\u2500 README.md \u251c\u2500\u2500 annotations \u2502 \u251c\u2500\u2500 captions_train2014.json \u2502 \u2514\u2500\u2500 captions_val2014.json \u2514\u2500\u2500 mscoco-captions.spec Now, you are ready to put the new set of labels under ML-Git management. We assume there is an existing mscoco dataset. For this, do: ml-git labels add mscoco-captions ml-git labels commit mscoco-captions --dataset=mscoco ml-git labels push mscoco-captions The commands are very similar to dataset operations. However, you can note one particular change in the commit command. There is an option \" --dataset \" which is used to tell ML-Git that the labels should be linked to the specified dataset. Internally, ML-Git will look at the checked out dataset in your workspace for that specified dataset. Then, it will include the git tag and sha into the specification file to be committed into the metadata repository. Once done, anyone will be able to retrieve the exact same version of the dataset that has been used for that specific set of labels. One can look at the specific dataset associated with that set of labels by executing the following command: ml-git labels show mscoco-captions Output: -- labels : mscoco-captions -- categories: - computer-vision - captions dataset: sha: 607fa818da716c3313a6855eb3bbd4587e412816 tag: computer-vision__images__mscoco__1 manifest: files: MANIFEST.yaml storage: s3h://mlgit-datasets name: mscoco-captions version: 1 As you can see, there is a new section \" dataset \" that has been added by ML-Git with the sha & tag fields. It can be used to checkout the exact version of the dataset for that set of labels. Uploading labels related to a dataset:","title":"Uploading labels associated to a dataset"},{"location":"first_project/#uploading-models","text":"To create and upload your model, you must be in an already initialized project, if necessary read section 1 to initialize and configure a project. The first step is to configure your metadata & data repository/storage. ml-git repository remote models add git@github.com:example/your-mlgit-models.git ml-git repository storage add mlgit-models --endpoint-url=<minio-endpoint-url> ml-git models init To create a model entity, you can run the following command: ml-git models create imagenet-model --categories=\"computer-vision, images\" --storage-type=s3h --mutability=mutable --bucket-name=mlgit-models After creating the model, we add the model file to the data folder. Here below is the directory tree structure: imagenet-model/ \u251c\u2500\u2500 README.md \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 model_file \u2514\u2500\u2500 imagenet-model.spec Now, you're ready to put that new model set under ML-Git management. We assume there is an existing imagenet8 dataset and mscoco-captions labels. For this, do: ml-git models add imagenet-model ml-git models commit imagenet-model --dataset=imagenet8 --labels=mscoco-captions ml-git models push imagenet-model There is not much change compared to dataset and labels operation. You can use the options \" -- dataset \" and \" --labels \", which tells to ml-git that the model should be linked to the specified dataset and labels. Internally, ml-git will look in your workspace for the checked out dataset and labels specified in the options. It then will include the reference to the checked out versions into the model's specification file to be committed into the metadata repository. Once done, anyone will then be able to retrieve the exact same version of the dataset and labels that has been used for that specific model. Persisting model's metrics: We can insert metrics to the model in the add command, metrics can be added with the following parameters: metrics-file : optional metrics file path. It is expected a CSV file containing the metric names in the header and the values in the next line. metric : optional metric keys and values. An example of adding a model passing a metrics file, would be the following command: ml-git models add imagenet-model --metrics-file='/path/to/your/file.csv' An example of adding a model passing metrics through the command line, would be the following command: ml-git models add imagenet-model --metric accuracy 10 --metric precision 20 --metric recall 30 Obs: The parameters used above were chosen for example purposes, you can name your metrics however you want to, you can also pass as many metrics as you want, as long as you use the command correctly. When inserting the metrics, they will be included in the structure of your model's spec file. An example of what it would look like would be the following structure: model: categories: - computer-vision - images manifest: storage: s3h://mlgit-models metrics: accuracy: 10.0 precision: 20.0 recall: 30.0 name: imagenet-model version: 1 You can view metrics for all tags for that entity by running the following command: ml-git models metrics imagenet-model","title":"Uploading Models"},{"location":"first_project/#downloading-a-dataset","text":"We assume there is an existing ML-Git repository with a few ML datasets under its management and you'd like to download one of the available datasets. If you don't have a dataset versioned by the ML-Git, see section 2 on how to do this. To download a dataset, you need to be in an initialized and configured ML-Git project. If you have a repository with your saved settings, you can run the following command to set up your environment: ml-git clone git@github.com:example/your-mlgit-repository.git Then, go to the project directory: cd your-mlgit-repository Inside the configured ML-Git project directory, the following command will update the metadata repository, allowing visibility of what has been shared since the last update (new ML entity, new versions). ml-git datasets update Or update all metadata repository: ml-git repository update To discover which datasets are under ML-Git management, you can execute the following command: ml-git datasets list Output: ML dataset |-- folderA | |-- folderB | | |-- dataset-ex-minio | | |-- imagenet8 | | |-- dataset-ex The ML-Git repository contains 3 different datasets, all falling under the same directories folderA/folderB (These directories were defined when the entity was created and can be modified at any time by the user). In order for ML-Git to manage the different versions of the same dataset. It internally creates a tag based on categories, ML entity name and its version. To show all these tag representing the versions of a dataset, simply type the following: ml-git datasets tag list imagenet8 Output: computer-vision__images__imagenet8__1 computer-vision__images__imagenet8__2 It means there are actually 2 versions under ML-Git management. You can check what version is checked out in the ML-Git workspace with the following command: ml-git datasets branch imagenet8 Output: ('vision-computing__images__imagenet8__2', '48ba1e994a1e39e1b508bff4a3302a5c1bb9063e') The output is a tuple: The tag auto-generated by ML-Git based on the .spec. The sha of the git commit of that version. It is simple to retrieve a specific version locally to start any experiment by executing one of the following commands: ml-git datasets checkout computer-vision__images__imagenet8__1 or ml-git datasets checkout imagenet8 --version=1 If you want to get the latest available version of an entity you can just pass its name in the checkout command, as shown below: ml-git datasets checkout imagenet8 Getting the data will auto-create a directory structure under dataset directory as shown below. That structure folderA/folderB is actually the structure in which the dataset was versioned. folderA \u2514\u2500\u2500 folderB \u2514\u2500\u2500 imagenet8 \u251c\u2500\u2500 README.md \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 train \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_1 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_2 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_3 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_4 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_5 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_6 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_7 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_8 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_9 \u2502 \u2502 \u2514\u2500\u2500 train_data_batch_10 \u2502 \u2514\u2500\u2500 val \u2502 \u2514\u2500\u2500 val_data \u2514\u2500\u2500 imagenet8.spec Downloading a dataset:","title":"Downloading a dataset"},{"location":"first_project/#checking-data-integrity","text":"If at some point you want to check the integrity of the metadata repository (e.g. computer shuts down during a process), simply type the following command: ml-git datasets fsck INFO - HashFS: starting integrity check on [.\\.ml-git\\dataset\\objects\\hashfs] INFO - HashFS: starting integrity check on [.\\.ml-git\\dataset\\index\\hashfs] [1] corrupted file(s) in Local Repository [0] corrupted file(s) in Index Total of corrupted files: 1 INFO - Repository: For more information about the corrupted files you can run the command with the --verbose option. That command will walk through the internal ML-Git directories (index & local repository) and will check the integrity of all blobs under its management. It will fix and return the list of blobs that are corrupted or missing. Checking data integrity:","title":"Checking data integrity"},{"location":"first_project/#changing-a-dataset","text":"When adding files to an entity ML-Git locks the files for read only. When the entity's mutability type is flexible or mutable, you can change the data of a file and resubmit it without being considered corrupted. In case of a flexible entity you should perform the following command to unlock the file: ml-git datasets unlock imagenet8 data\\train\\train_data_batch_1 After that, the unlocked file is subject to modification. If you modify the file without performing this command, it will be considered corrupted. To upload the data, you can execute the following commands: ml-git datasets add <yourdataset> --bumpversion ml-git datasets commit <yourdataset> ml-git datasets push <yourdataset> This will create a new version of your dataset and push the changes to your remote storage (e.g. S3). Changing a dataset:","title":"Changing a Dataset"},{"location":"first_steps/","text":"Getting Started with ML-Git \u00b6 First steps is a chapter to explore how a user should do a basic setup and first project Installation \u00b6 To install ML-Git, run the following command from the command line: pip install git+git://github.com/HPInc/ml-git.git For more details, see the Installation Guide . Setting up \u00b6 As ML-Git leverages git to manage ML entities metadata, it is required that you configure git user and email address: git config --global user.name \"Your User\" git config --global user.email \"your_email@example.com\" To fully configure and use ML-Git, you will also need to create a git repository and the storage type you want to use. You can create manually, the git repository and storage, or use a script provided in our advanced scenarios . We recommend that for your first project you stick with this tutorial, so you can exercise many ML-Git commands. ML-Git First Project \u00b6 After completing the previous steps, you can create your first project. Configuring Git Repository and Storage \u00b6 First, create a folder for your ML-Git project (We will use as an example the folder named \"mlgit-project\"): mkdir mlgit-project && cd mlgit-project Then, we will initialize this folder as an ML-Git repository: ml-git repository init The next step is configure the remote repositories and buckets that your project will use to store data. To configure the git repository: ml-git repository remote datasets add git@github.com:user/user-mlgit-project To configure the storage: ml-git repository storage add mlgit-datasets --endpoint-url=<minio-endpoint-url> Adding Your First Dataset \u00b6 Now, you have repositories, and storage configurated for your project. To create and upload your first dataset to a storage, first, run the below command: ml-git datasets init Then, you can run the below command to create your dataset ml-git datasets create imagenet8 --categories=computer-vision --mutability=strict --bucket-name=mlgit-datasets It will generate an output saying that the project was created. Also, it will create a series of folders and files with the specifications of the dataset. You can see the generated files looking into the root folder. After you add your dataset files inside the folder, you can run the following command to see the dataset status: ml-git datasets status imagenet8 Below, you can see a possible output: INFO - Repository: datasets: status of ml-git index for [imagenet8] Changes to be committed: Untracked files: README.md imagenet8.spec data/ -> 3 FILES Corrupted files: Above, the output shows some untracked files. To commit these files, similarly to git, we can run the following sequence of commands: The following command will add all untracked files: ml-git datasets add imagenet8 The following command will commit the metadata to the local repository: ml-git datasets commit imagenet8 The following command will update the remote metadata repository: ml-git datasets push imagenet8 Downloading a Dataset \u00b6 If you already have access to an existing ML-Git project. You can clone the repository and use ML-Git to bring a dataset to your workspace. To clone a repository use the command: ml-git clone git@github.com:example/your-mlgit-repository.git Then, go to the project directory: cd your-mlgit-repository Now you can discover which datasets are under ML-Git management by executing the command: ml-git datasets list It will generate a similar output as you can see below: ML dataset |-- folderA | |-- folderB | | |-- dataset-ex-minio | | |-- imagenet8 | | |-- dataset-ex The example above represets a ML-Git repository containing 3 different datasets, all falling under the same directory folderA/folderB (This hierarchy was defined when the entity was created and can be modified at any time by the user). In order for ML-Git to manage the different versions of the same dataset. It internally creates a tag based on categories, ML entity name and its version. To show all these tag representing the versions of a dataset, simply type the following: ml-git datasets tag list imagenet8 A possible output: computer-vision__images__imagenet8__1 computer-vision__images__imagenet8__2 It means there are 2 versions under ML-Git management. You can check what version is checked out in the ML-Git workspace with the following command: ml-git datasets branch imagenet8 It is simple to retrieve a specific version locally to start any experiment by executing one of the following commands: ml-git datasets checkout computer-vision__images__imagenet8__1 or ml-git datasets checkout imagenet8 --version=1 If you want to get the latest available version of an entity you can just pass its name in the checkout command, as shown below: ml-git datasets checkout imagenet8 Downloading a Dataset:","title":"Getting Started"},{"location":"first_steps/#getting-started-with-ml-git","text":"First steps is a chapter to explore how a user should do a basic setup and first project","title":"Getting Started with ML-Git"},{"location":"first_steps/#installation","text":"To install ML-Git, run the following command from the command line: pip install git+git://github.com/HPInc/ml-git.git For more details, see the Installation Guide .","title":"Installation"},{"location":"first_steps/#setting-up","text":"As ML-Git leverages git to manage ML entities metadata, it is required that you configure git user and email address: git config --global user.name \"Your User\" git config --global user.email \"your_email@example.com\" To fully configure and use ML-Git, you will also need to create a git repository and the storage type you want to use. You can create manually, the git repository and storage, or use a script provided in our advanced scenarios . We recommend that for your first project you stick with this tutorial, so you can exercise many ML-Git commands.","title":"Setting up"},{"location":"first_steps/#ml-git-first-project","text":"After completing the previous steps, you can create your first project.","title":"ML-Git First Project"},{"location":"first_steps/#configuring-git-repository-and-storage","text":"First, create a folder for your ML-Git project (We will use as an example the folder named \"mlgit-project\"): mkdir mlgit-project && cd mlgit-project Then, we will initialize this folder as an ML-Git repository: ml-git repository init The next step is configure the remote repositories and buckets that your project will use to store data. To configure the git repository: ml-git repository remote datasets add git@github.com:user/user-mlgit-project To configure the storage: ml-git repository storage add mlgit-datasets --endpoint-url=<minio-endpoint-url>","title":"Configuring Git Repository and Storage"},{"location":"first_steps/#adding-your-first-dataset","text":"Now, you have repositories, and storage configurated for your project. To create and upload your first dataset to a storage, first, run the below command: ml-git datasets init Then, you can run the below command to create your dataset ml-git datasets create imagenet8 --categories=computer-vision --mutability=strict --bucket-name=mlgit-datasets It will generate an output saying that the project was created. Also, it will create a series of folders and files with the specifications of the dataset. You can see the generated files looking into the root folder. After you add your dataset files inside the folder, you can run the following command to see the dataset status: ml-git datasets status imagenet8 Below, you can see a possible output: INFO - Repository: datasets: status of ml-git index for [imagenet8] Changes to be committed: Untracked files: README.md imagenet8.spec data/ -> 3 FILES Corrupted files: Above, the output shows some untracked files. To commit these files, similarly to git, we can run the following sequence of commands: The following command will add all untracked files: ml-git datasets add imagenet8 The following command will commit the metadata to the local repository: ml-git datasets commit imagenet8 The following command will update the remote metadata repository: ml-git datasets push imagenet8","title":"Adding Your First Dataset"},{"location":"first_steps/#downloading-a-dataset","text":"If you already have access to an existing ML-Git project. You can clone the repository and use ML-Git to bring a dataset to your workspace. To clone a repository use the command: ml-git clone git@github.com:example/your-mlgit-repository.git Then, go to the project directory: cd your-mlgit-repository Now you can discover which datasets are under ML-Git management by executing the command: ml-git datasets list It will generate a similar output as you can see below: ML dataset |-- folderA | |-- folderB | | |-- dataset-ex-minio | | |-- imagenet8 | | |-- dataset-ex The example above represets a ML-Git repository containing 3 different datasets, all falling under the same directory folderA/folderB (This hierarchy was defined when the entity was created and can be modified at any time by the user). In order for ML-Git to manage the different versions of the same dataset. It internally creates a tag based on categories, ML entity name and its version. To show all these tag representing the versions of a dataset, simply type the following: ml-git datasets tag list imagenet8 A possible output: computer-vision__images__imagenet8__1 computer-vision__images__imagenet8__2 It means there are 2 versions under ML-Git management. You can check what version is checked out in the ML-Git workspace with the following command: ml-git datasets branch imagenet8 It is simple to retrieve a specific version locally to start any experiment by executing one of the following commands: ml-git datasets checkout computer-vision__images__imagenet8__1 or ml-git datasets checkout imagenet8 --version=1 If you want to get the latest available version of an entity you can just pass its name in the checkout command, as shown below: ml-git datasets checkout imagenet8 Downloading a Dataset:","title":"Downloading a Dataset"},{"location":"gdrive_configurations/","text":"Google Drive API configuration \u00b6 This section aims to explain how to enable Google Drive API and configure OAuth 2.0 credentials to use with ML-Git. Enabling Drive API \u00b6 You need to create a project in Google developer console to activate Drive API, follow instructions bellow: 1. Access console developer and click on create project: 2. Then type name of your preference and click on \"CREATE\" button: 3. Go back to dashboard and enable Drive API: 4. Search for Drive API on search bar: Creating credentials \u00b6 When you finish Enabling API step, you need to create your credentials and configure authentication consent screen. 1. Click on create credentials: 2. Select user type of your consent: 3. Add application name to authentication consent screen: 4. Change application's scope if you prefer and save: 5. Go back to dashboard and click on create credentials and generate your API KEY: 6. Generate OAuth client id: 7. Add client name and select application type: 8. Finally you can download your credentials file: Setting up a ML-Git project with Google Drive \u00b6 Create directory with name of your preference and copy your credentials file with name credentials.json inside the directory. Add storage configurations example: ml-git repository storage add path-in-your-drive --type=gdriveh --credentials=/home/profile/.gdrive After that initialize the metadata repository: ml-git datasets init We strongly recommend that you add push_threads_count: 10 option in your . ml-git/config.yaml , because of Google Drive API request limit of 10/s. This option change the number of workers used in multithreading push process, by default the number of workers is cpu numbers multiplied by 5. The push command was tested with 10 workers and the request limit was not exceeded. Configuration example: batch_size: 20 push_threads_count: 10 datasets: git: '' labels: git: '' models: git: '' storages: gdriveh: dataset-gdrive: credentials-path: /home/profile/.gdrive","title":"Google Drive"},{"location":"gdrive_configurations/#google-drive-api-configuration","text":"This section aims to explain how to enable Google Drive API and configure OAuth 2.0 credentials to use with ML-Git.","title":"Google Drive API configuration"},{"location":"gdrive_configurations/#enabling-drive-api","text":"You need to create a project in Google developer console to activate Drive API, follow instructions bellow: 1. Access console developer and click on create project: 2. Then type name of your preference and click on \"CREATE\" button: 3. Go back to dashboard and enable Drive API: 4. Search for Drive API on search bar:","title":"Enabling Drive API"},{"location":"gdrive_configurations/#creating-credentials","text":"When you finish Enabling API step, you need to create your credentials and configure authentication consent screen. 1. Click on create credentials: 2. Select user type of your consent: 3. Add application name to authentication consent screen: 4. Change application's scope if you prefer and save: 5. Go back to dashboard and click on create credentials and generate your API KEY: 6. Generate OAuth client id: 7. Add client name and select application type: 8. Finally you can download your credentials file:","title":"Creating credentials"},{"location":"gdrive_configurations/#setting-up-a-ml-git-project-with-google-drive","text":"Create directory with name of your preference and copy your credentials file with name credentials.json inside the directory. Add storage configurations example: ml-git repository storage add path-in-your-drive --type=gdriveh --credentials=/home/profile/.gdrive After that initialize the metadata repository: ml-git datasets init We strongly recommend that you add push_threads_count: 10 option in your . ml-git/config.yaml , because of Google Drive API request limit of 10/s. This option change the number of workers used in multithreading push process, by default the number of workers is cpu numbers multiplied by 5. The push command was tested with 10 workers and the request limit was not exceeded. Configuration example: batch_size: 20 push_threads_count: 10 datasets: git: '' labels: git: '' models: git: '' storages: gdriveh: dataset-gdrive: credentials-path: /home/profile/.gdrive","title":"Setting up a ML-Git project with Google Drive"},{"location":"installation_guide/","text":"ML-Git Installation - A detailed guide. \u00b6 Requirements \u00b6 ML-Git requires a Python version 3.7 or superior, and the Python package manager, pip, to be installed on your system. Also, git is required as ML-Git uses git to manage ML entities metadata. You can check if you already have these installed from the command line: python --version && pip --version && git --version Output: Pyhon 3.8.2 pip 20.0.2 from /usr/lib/python3/dist-packages/pip (python 3.8) git version 2.25.1 If you already have those packages installed, you may skip down to Installing ML-Git . Installing Python \u00b6 Install Python using your package manager of choice, or by downloading an installer appropriate for your system from python.org and running it. Installing Pip \u00b6 If you're using a recent version of Python, the Python package manager, pip , is most likely installed by default. However, you may need to upgrade pip to the lasted version: pip install --upgrade pip If you need to install pip for the first time, download get-pip.py . Then run the following command to install it: python get-pip.py Installing git \u00b6 You can install git using the following command: sudo apt install git Installing ML-Git \u00b6 Install the ML-Git package using pip: pip install git+git://github.com/HPInc/ml-git.git You should now have the ML-Git installed on your system. Run ml-git --version to check that everything worked okay. ml-git --version ml-git 2.0.1 Install the ML-Git package using the Source Code: Also, you can download ML-Git from the repository and execute commands below: Windows: cd ml-git/ python3.7 setup.py install Linux: cd ml-git/ sudo python3.7 setup.py install The output should be the same as using pip.","title":"ML-Git Installation - A detailed guide."},{"location":"installation_guide/#ml-git-installation-a-detailed-guide","text":"","title":"ML-Git Installation - A detailed guide."},{"location":"installation_guide/#requirements","text":"ML-Git requires a Python version 3.7 or superior, and the Python package manager, pip, to be installed on your system. Also, git is required as ML-Git uses git to manage ML entities metadata. You can check if you already have these installed from the command line: python --version && pip --version && git --version Output: Pyhon 3.8.2 pip 20.0.2 from /usr/lib/python3/dist-packages/pip (python 3.8) git version 2.25.1 If you already have those packages installed, you may skip down to Installing ML-Git .","title":"Requirements"},{"location":"installation_guide/#installing-python","text":"Install Python using your package manager of choice, or by downloading an installer appropriate for your system from python.org and running it.","title":"Installing Python"},{"location":"installation_guide/#installing-pip","text":"If you're using a recent version of Python, the Python package manager, pip , is most likely installed by default. However, you may need to upgrade pip to the lasted version: pip install --upgrade pip If you need to install pip for the first time, download get-pip.py . Then run the following command to install it: python get-pip.py","title":"Installing Pip"},{"location":"installation_guide/#installing-git","text":"You can install git using the following command: sudo apt install git","title":"Installing git"},{"location":"installation_guide/#installing-ml-git","text":"Install the ML-Git package using pip: pip install git+git://github.com/HPInc/ml-git.git You should now have the ML-Git installed on your system. Run ml-git --version to check that everything worked okay. ml-git --version ml-git 2.0.1 Install the ML-Git package using the Source Code: Also, you can download ML-Git from the repository and execute commands below: Windows: cd ml-git/ python3.7 setup.py install Linux: cd ml-git/ sudo python3.7 setup.py install The output should be the same as using pip.","title":" Installing ML-Git"},{"location":"minio_s3_configuration/","text":"MinIO \u00b6 MinIO is a cloud storage server compatible with Amazon S3. The following sections will explain how to properly set up so ML-Git can work by using the Access Key and Secret Access Key of your MinIO user. Running MinIO locally \u00b6 In case you want to run MinIO locally for testing purposes, it's possible to run a docker container using the following command: docker run -v /path/to/your/dir:/data --name=minio --network=host minio/minio server --console-address \":9001\" /data The command will start the MinIO API and Console servers in ports 9000 and 9001, respectively. Once you have successfully started the container, you can access the Console URL (usually http://127.0.0.1:9001) using the default user (username: minioadmin, password: minioadmin) to create a new user (setting up its Access Key and Secret Access Key) or create new buckets. After you finish creating a new user, remember that you'll need the proper local URL for the API to be used in the --endpoint-url parameter of the storage creation command, it'll usually be http://127.0.0.1:9000 . Note: In case you decide to work with a deployed MinIO server instead, just remember to use the proper URL when creating new storage and to have your MinIO user's Access Key and Secret Access Key in hand for the following setup. Credentials configuration \u00b6 This section explains how to configure the settings that the ML-Git uses to interact with your bucket. This requires that you have the following data: Profile Name Access Key ID Secret Access Key Output Format The Access Key ID and Secret Access Key are the credentials for your MinIO user. The Output Format specifies how the results are formatted. Internally ML-Git uses Boto3 to communicate with the MinIO API. Even though Boto3 is the Amazon Web Services (AWS) SDK for Python, we can still use it to communicate with the MinIO services. Boto3 looks at various configuration locations until it finds configuration values. The following lookup order is used searching through sources for configuration values: Environment variables The ~/.aws/config file Note: If, when creating a storage, you define a specific profile to be used, Boto3 will only search for that profile in the ~/.aws/config file. You can configure the credentials in three ways (environment variables, through the console or with the AWS Command Line Interface ). These are described in the following sections. 1 - Environment Variables Linux or macOS : export AWS_ACCESS_KEY_ID=your-access-key export AWS_SECRET_ACCESS_KEY=your-secret-access-key Windows : C:\\> setx AWS_ACCESS_KEY_ID your-access-key C:\\> setx AWS_SECRET_ACCESS_KEY your-secret-access-key 2 - Console From the home directory (UserProfile) execute: mkdir .aws You need to create two files to store the sensitive credential information (\\~/.aws/credentials) separated from the less sensitive configuration options (\\~/.aws/config). To create these two files type the following commands: For config file: echo \" [your-profile-name] output=json \" > .aws/config For credentials file: echo \" [your-profile-name] aws_access_key_id = your-access-key aws_secret_access_key = your-secret-access-key \" > .aws/credentials 3 - AWS CLI For general use, the aws configure command is the fastest way to set up but requires the AWS CLI installed. To install and configure type the following commands: pip install awscli aws configure AWS Access Key ID [None]: your-access-key AWS Secret Access Key [None]: your-secret-access-key Default region name [None]: Default output format [None]: json These commands will create the files ~/.aws/credentials and ~/.aws/config. Demonstrating AWS Configure","title":"MinIO"},{"location":"minio_s3_configuration/#minio","text":"MinIO is a cloud storage server compatible with Amazon S3. The following sections will explain how to properly set up so ML-Git can work by using the Access Key and Secret Access Key of your MinIO user.","title":"MinIO"},{"location":"minio_s3_configuration/#running-minio-locally","text":"In case you want to run MinIO locally for testing purposes, it's possible to run a docker container using the following command: docker run -v /path/to/your/dir:/data --name=minio --network=host minio/minio server --console-address \":9001\" /data The command will start the MinIO API and Console servers in ports 9000 and 9001, respectively. Once you have successfully started the container, you can access the Console URL (usually http://127.0.0.1:9001) using the default user (username: minioadmin, password: minioadmin) to create a new user (setting up its Access Key and Secret Access Key) or create new buckets. After you finish creating a new user, remember that you'll need the proper local URL for the API to be used in the --endpoint-url parameter of the storage creation command, it'll usually be http://127.0.0.1:9000 . Note: In case you decide to work with a deployed MinIO server instead, just remember to use the proper URL when creating new storage and to have your MinIO user's Access Key and Secret Access Key in hand for the following setup.","title":"Running MinIO locally"},{"location":"minio_s3_configuration/#credentials-configuration","text":"This section explains how to configure the settings that the ML-Git uses to interact with your bucket. This requires that you have the following data: Profile Name Access Key ID Secret Access Key Output Format The Access Key ID and Secret Access Key are the credentials for your MinIO user. The Output Format specifies how the results are formatted. Internally ML-Git uses Boto3 to communicate with the MinIO API. Even though Boto3 is the Amazon Web Services (AWS) SDK for Python, we can still use it to communicate with the MinIO services. Boto3 looks at various configuration locations until it finds configuration values. The following lookup order is used searching through sources for configuration values: Environment variables The ~/.aws/config file Note: If, when creating a storage, you define a specific profile to be used, Boto3 will only search for that profile in the ~/.aws/config file. You can configure the credentials in three ways (environment variables, through the console or with the AWS Command Line Interface ). These are described in the following sections. 1 - Environment Variables Linux or macOS : export AWS_ACCESS_KEY_ID=your-access-key export AWS_SECRET_ACCESS_KEY=your-secret-access-key Windows : C:\\> setx AWS_ACCESS_KEY_ID your-access-key C:\\> setx AWS_SECRET_ACCESS_KEY your-secret-access-key 2 - Console From the home directory (UserProfile) execute: mkdir .aws You need to create two files to store the sensitive credential information (\\~/.aws/credentials) separated from the less sensitive configuration options (\\~/.aws/config). To create these two files type the following commands: For config file: echo \" [your-profile-name] output=json \" > .aws/config For credentials file: echo \" [your-profile-name] aws_access_key_id = your-access-key aws_secret_access_key = your-secret-access-key \" > .aws/credentials 3 - AWS CLI For general use, the aws configure command is the fastest way to set up but requires the AWS CLI installed. To install and configure type the following commands: pip install awscli aws configure AWS Access Key ID [None]: your-access-key AWS Secret Access Key [None]: your-secret-access-key Default region name [None]: Default output format [None]: json These commands will create the files ~/.aws/credentials and ~/.aws/config. Demonstrating AWS Configure","title":"Credentials configuration"},{"location":"mlgit_commands/","text":"ML-Git commands \u00b6 ml-git --help Usage: ml-git [OPTIONS] COMMAND [ARGS]... Options: --version Show the version and exit. Commands: clone Clone an ml-git repository ML_GIT_REPOSITORY_URL datasets Management of datasets within this ml-git repository. labels Management of labels sets within this ml-git repository. models Management of models within this ml-git repository. repository Management of this ml-git repository. Example: ml-git --help ml-git --version Displays the installed version of ML-Git. ml-git <ml-entity> add Usage: ml-git datasets add [OPTIONS] ML_ENTITY_NAME [FILE_PATH]... Add datasets change set ML_ENTITY_NAME to the local ml-git staging area. Options: --bumpversion Increment the version number when adding more files. --fsck Run fsck after command execution. --metric <TEXT FLOAT>... Metric key and value. --metrics-file Metrics file path. --wizard Enable the wizard to request information when needed. --verbose Debug mode Dataset example: ml-git datasets add dataset-ex --bumpversion ml-git expects datasets to be managed under dataset directory. \\<ml-entity-name> is also expected to be a repository under the tree structure and ml-git will search for it in the tree. Under that repository, it is also expected to have a \\<ml-entity-name>.spec file, defining the ML entity to be added. Optionally, one can add a README.md which will describe the dataset and be what will be shown in the github repository for that specific dataset. Internally, the ml-git add will add all the files under the \\<ml-entity> directory into the ml-git index / staging area. Model example: ml-git models add model-ex --metrics-file='/path/to/your/file.csv' ml-git allows you to enter a metrics file or the metrics themselves on the command line when adding a model. ml-git <ml-entity> branch Usage: ml-git datasets branch [OPTIONS] ML_ENTITY_NAME This command allows to check which tag is checked out in the ml-git workspace. Options: --verbose Debug mode Example: ml-git datasets branch imagenet8 Output: ('vision-computing__images__imagenet8__1', '48ba1e994a1e39e1b508bff4a3302a5c1bb9063e') That information is equal to the HEAD reference from a git concept. ml-git keeps that information on a per \\<ml-entity-name> basis. which enables independent checkout of each of these \\<ml-entity-name>. The output is a tuple: 1) the tag auto-generated by ml-git based on the \\<ml-entity-name>.spec (composite with categories, \\<ml-entity-name>, version) 2) the sha of the git commit of that \\<ml-entity> version Both are the same representation. One is human-readable and is also used internally by ml-git to find out the path to the referenced \\<ml-entity-name>. ml-git <ml-entity> checkout Usage: ml-git models checkout [OPTIONS] ML_ENTITY_TAG|ML_ENTITY Checkout the ML_ENTITY_TAG|ML_ENTITY of a model set into user workspace. Options: -l, --with-labels The checkout associated labels in user workspace as well. -d, --with-dataset The checkout associated dataset in user workspace as well. --retry INTEGER RANGE Number of retries to download the files from the storage. This number must be in the range 0-999999999 [default: 2]. --force Force checkout command to delete untracked/uncommitted files from local repository. --bare Ability to add/commit/push without having the ml-entity checked out. --version INTEGER RANGE Number of artifact version to be downloaded. This number must be in the range 0-999999999 [default: latest]. --fail-limit INTEGER RANGE Number of failures before aborting the command. This number must be in the range 0-999999999 [default: no limit]. --full Show all contents for each directory. --wizard Enable the wizard to request information when needed. --verbose Debug mode Examples: ml-git datasets checkout computer-vision__images__faces__fddb__1 or you can use the name of the entity directly and download the latest available tag ml-git datasets checkout fddb Note: --d: It can only be used in checkout of labels and models to get the entities that are associated with the entity. --l: It can only be used in checkout of models to get the label entity that are associated with the entity. --sample-type, --sampling, --seed: These options are available only for dataset. If you use this option ml-git will not allow you to make changes to the entity and create a new tag. ml-git <ml-entity> commit Usage: ml-git models commit [OPTIONS] ML_ENTITY_NAME Commit model change set of ML_ENTITY_NAME locally to this ml-git repository. Options: --dataset NOT EMPTY STRING Link a dataset entity name to this model set version --labels NOT EMPTY STRING Link a labels entity name to this model set version --version INTEGER RANGE Set the version number of the artifact. This number must be in the range 0-999999999. -m, --message TEXT Use the provided <msg> as the commit message. --fsck Run fsck after command execution. --wizard Enable the wizard to request information when needed. --verbose Debug mode Example: ml-git models commit model-ex --dataset=dataset-ex This command commits the index / staging area to the local repository. It is a 2-step operation in which 1) the actual data (blobs) is copied to the local repository, 2) committing the metadata to the git repository managing the metadata. Internally, ml-git keeps track of files that have been added to the data storage and is storing that information to the metadata management layer to be able to restore any version of each \\<ml-entity-name>. Another important feature of ml-git is the ability to keep track of the relationship between the ML entities. So when committing a label set, one can (should) provide the option --dataset=<dataset-name> . Internally, ml-git will inspect the HEAD / ref of the specified \\<dataset-name> checked out in the ml-git repository and will add that information to the specificatino file that is committed to the metadata repository. With that relationship kept into the metadata repository, it is now possible for anyone to checkout exactly the same versions of labels and dataset. Same for ML model, one can specify which dataset and label set that have been used to generate that model through --dataset=<dataset-name> and --labels=<labels-name> ml-git <ml-entity> create Usage: ml-git datasets create [OPTIONS] ARTIFACT_NAME This command will create the workspace structure with data and spec file for an entity and set the git and storage configurations. [This command has a wizard that will request the necessary information if it is not informed] Options: --categories TEXT Artifact's categories names. The categories names must be separated by comma. E.g: \"category1,category2,category3\". [required] --mutability [strict|flexible|mutable] Mutability type. [required] --storage-type [s3h|azureblobh|gdriveh|sftph] Storage type (s3h, azureblobh, gdriveh, sftph) [default: s3h] --version INTEGER RANGE Set the version number of the artifact. This number must be in the range 0-999999999. --import NOT EMPTY STRING Path to be imported to the project. NOTE: Mutually exclusive with argument: credentials_path, import_url. --wizard-config If specified, ask interactive questions at console for git & storage configurations. [DEPRECATED: This option should no longer be used.] --bucket-name NOT EMPTY STRING Bucket name --import-url NOT EMPTY STRING Import data from a google drive url. NOTE: Mutually exclusive with argument: import. --credentials-path NOT EMPTY STRING Directory of credentials.json. NOTE: This option is required if --import-url is used. --unzip Unzip imported zipped files. Only available if --import-url is used. --entity-dir NOT EMPTY STRING The relative path where the entity will be created inside the ml entity directory. --wizard Enable the wizard to request information when needed. --verbose Debug mode Examples: - To create an entity with s3 as storage and importing files from a path of your computer: ml-git datasets create imagenet8 --storage-type=s3h --categories=\"computer-vision, images\" --version=0 --import='/path/to/dataset' --mutability=strict To create an entity with s3 as storage and importing files from a google drive URL: ml-git datasets create imagenet8 --storage-type=s3h --categories=computer-vision,images --import-url='gdrive.url' --credentials-path='/path/to/gdrive/credentials' --mutability=strict --unzip ml-git <ml-entity> diff Usage: ml-git datasets diff [OPTIONS] ML_ENTITY_NAME FIRST_TAG SECOND_TAG Print the difference between two entity tag versions. The command will show added, updated and deleted files. Options: --full Show all contents for each directory. --verbose Debug mode Examples: - To check the difference between entity tag versions: ml-git datasets diff dataset-ex computer-vision__images__dataset-ex__1 computer-vision__images__dataset-ex__4 Output: Added files: data/ -> 4 FILES tabular.csv Updated files: data/dataset_test.csv Deleted files: data/dataset_old.csv To check the difference between entity tag versions showing all contents for each directory: ml-git datasets diff --full dataset-ex computer-vision__images__dataset-ex__1 computer-vision__images__dataset-ex__4 Output: Added files: data/dataset_1.csv data/dataset_2.csv data/dataset_3.csv data/dataset_4.csv tabular.csv Updated files: data/dataset_test.csv Deleted files: data/dataset_old.csv ml-git <ml-entity> export Usage: ml-git datasets export [OPTIONS] ML_ENTITY_TAG BUCKET_NAME This command allows you to export files from one storage (S3|MinIO) to another (S3|MinIO). Options: --credentials TEXT Profile of AWS credentials [default: default]. --endpoint TEXT Storage endpoint url. --region TEXT AWS region name [default: us-east-1]. --retry INTEGER RANGE Number of retries to download the files from the storage. This number must be in the range 0-999999999 [default: 2]. --verbose Debug mode Example: ml-git datasets export computer-vision__images__faces__fddb__1 minio ml-git <ml-entity> fetch Usage: ml-git datasets fetch [OPTIONS] ML_ENTITY_TAG Allows you to download just the metadata files of an entity. Options: --sample-type [group|range|random] --sampling TEXT The group: <amount>:<group> The group sample option consists of amount and group used to download a sample. range: <start:stop:step> The range sample option consists of start, stop and step used to download a sample. The start parameter can be equal or greater than zero.The stop parameter can be 'all', -1 or any integer above zero. random: <amount:frequency> The random sample option consists of amount and frequency used to download a sample. --seed TEXT Seed to be used in random-based samplers. --retry INTEGER RANGE Number of retries to download the files from the storage. This number must be in the range 0-999999999 [default: 2]. --verbose Debug mode Example: ml-git datasets fetch computer-vision__images__faces__fddb__1 ml-git <ml-entity> fsck Usage: ml-git datasets fsck [OPTIONS] Options: --fix-workspace Use this option to repair files identified as corrupted in the entity workspace. --full Show the list of corrupted files. --verbose Debug mode Example: ml-git datasets fsck This command will walk through the internal ml-git directories (index & local repository) and check the presence and integrity of all file blobs under its management. This command will basically try to: Detect any chunk/blob that is corrupted or missing in the internal ml-git directory (.ml-git/{entity-type}/objects) Fetch files detected as corrupted or missing from storage Check the integrity of files mounted in the entities workspace In fix-workspace mode, repair corrupted files found in the entities workspace. A file in the entities workspace is considered 'corrupted' based on the business rule defined by the mutability of the entity. If you want to know more about each type of mutability and how it works, please take a look at Mutability documentation . It will return the list of blobs that are corrupted/missing if the user passes the --full option. ml-git <ml-entity> import Usage: ml-git datasets import [OPTIONS] BUCKET_NAME ENTITY_DIR This command allows you to download a file or directory from the S3 or Gdrive to ENTITY_DIR. Options: --credentials TEXT Input your profile to an s3 storage or your credentials path to a gdrive storage.(eg, --credentials=path/to/.credentials --region TEXT AWS region name [default: us-east-1]. --retry INTEGER RANGE Number of retries to download the files from the storage. This number must be in the range 0-999999999 [default: 2]. --path TEXT Storage folder path. --object TEXT Filename in storage. --storage-type [s3|gdrive] Storage type (s3, gdrive) [default: s3] --endpoint-url TEXT Storage endpoint url. --verbose Debug mode Example: ml-git datasets import bucket-name dataset/computer-vision/imagenet8/data For google drive storage: ml-git datasets import gdrive-folder --storage-type=gdrive --object=file_to_download --credentials=credentials-path dataset/ ml-git <ml-entity> init Usage: ml-git datasets init [OPTIONS] Init a ml-git datasets repository. Options: --verbose Debug mode Example: ml-git datasets init This command is mandatory to be executed just after the addition of a remote metadata repository ( ml-git \\<ml-entity> remote add ). It initializes the metadata by pulling all metadata to the local repository. ml-git <ml-entity> metrics Usage: ml-git models metrics [OPTIONS] ML_ENTITY_NAME Shows metrics information for each tag of the entity. Options: --export-path TEXT Set the path to export metrics to a file. NOTE: This option is required if --export-type is used. --export-type [csv|json] Choose the format of the file that will be generated with the metrics [default: json]. --verbose Debug mode Example: ml-git models metrics model-ex Note: This command is only available for model entities. ml-git <ml-entity> list Usage: ml-git datasets list [OPTIONS] List datasets managed under this ml-git repository. Options: --verbose Debug mode Example: ml-git datasets list Output: ML dataset |-- computer-vision | |-- images | | |-- dataset-ex-minio | | |-- imagenet8 | | |-- dataset-ex ml-git <ml-entity> log Usage: ml-git datasets log [OPTIONS] ML_ENTITY_NAME This command shows ml-entity-name's commit information like author, date, commit message. Options: --stat Show amount of files and size of an ml-entity. --fullstat Show added and deleted files. --verbose Debug mode Example: ml-git datasets log dataset-ex ml-git <ml-entity> push Usage: ml-git datasets push [OPTIONS] ML_ENTITY_NAME Push local commits from ML_ENTITY_NAME to remote ml-git repository & storage. Options: --retry INTEGER RANGE Number of retries to download the files from the storage. This number must be in the range 0-999999999 [default: 2]. --clearonfail Remove the files from the storage in case of failure during the push operation. --fail-limit INTEGER RANGE Number of failures before aborting the command. This number must be in the range 0-999999999 [default: no limit]. --verbose Debug mode Example: ml-git datasets push dataset-ex This command will perform a 2-step operations: 1. push all blobs to the configured data storage. 2. push all metadata related to the commits to the remote metadata repository. ml-git <ml-entity> remote-fsck Usage: ml-git datasets remote-fsck [OPTIONS] ML_ENTITY_NAME This command will check and repair the remote, by default it will only repair by uploading lacking chunks/blobs. Options bring more specialized repairs. Options: --thorough Try to download the IPLD if it is not present in the local repository to verify the existence of all contained IPLD links associated. --paranoid Adds an additional step that will download all IPLD and its associated IPLD links to verify the content by computing the multihash of all these. --retry INTEGER RANGE Number of retries to download the files from the storage. This number must be in the range 0-999999999 [default: 2]. --full Show the list of fixed and unfixed blobs and IPLDs. --wizard Enable the wizard to request information when needed. --verbose Debug mode Example: ml-git datasets remote-fsck dataset-ex This ml-git command will basically try to: Detects any chunk/blob lacking in a remote storage for a specific ML artefact version Repair - if possible - by uploading lacking chunks/blobs In paranoid mode, verifies the content of all the blobs ml-git <ml-entity> reset Usage: ml-git datasets reset [OPTIONS] ML_ENTITY_NAME Reset ml-git state(s) of an ML_ENTITY_NAME Options: --hard Remove untracked files from workspace, files to be committed from staging area as well as committed files upto <reference>. --mixed Revert the committed files and the staged files to 'Untracked Files'. This is the default action. --soft Revert the committed files to 'Changes to be committed'. --reference [head|head~1] head:Will keep the metadata in the current commit. head~1:Will move the metadata to the last commit. --verbose Debug mode Examples: ml-git datasets reset dataset-ex --hard Undo the committed changes. Undo the added/tracked files. Reset the workspace to fit with the current HEAD state. ml-git datasets reset dataset-ex --mixed if HEAD: * nothing happens. else: * Undo the committed changes. * Undo the added/tracked files. ml-git datasets reset dataset-ex --soft if HEAD: * nothing happens. else: * Undo the committed changes. ml-git <ml-entity> show Usage: ml-git datasets show [OPTIONS] ML_ENTITY_NAME Print the specification file of the entity. Options: --verbose Debug mode Example: ml-git datasets show dataset-ex Output: -- dataset : imagenet8 -- categories: - vision-computing - images manifest: files: MANIFEST.yaml storage: s3h://mlgit-datasets name: imagenet8 version: 1 ml-git <ml-entity> status Usage: ml-git datasets status [OPTIONS] ML_ENTITY_NAME [STATUS_DIRECTORY] Print the files that are tracked or not and the ones that are in the index/staging area. Options: --full Show all contents for each directory. --verbose Debug mode Example: ml-git datasets status dataset-ex ml-git <ml-entity> tag add Usage: ml-git datasets tag add [OPTIONS] ML_ENTITY_NAME TAG Use this command to associate a tag to a commit. Options: --verbose Debug mode Example: ml-git datasets tag add dataset-ex my_tag ml-git <ml-entity> tag list Usage: ml-git datasets tag list [OPTIONS] ML_ENTITY_NAME List tags of ML_ENTITY_NAME from this ml-git repository. Options: --verbose Debug mode Example: ml-git datasets tag list dataset-ex ml-git <ml-entity> update Usage: ml-git datasets update [OPTIONS] This command will update the metadata repository. Options: --verbose Debug mode Example: ml-git datasets update This command enables one to have the visibility of what has been shared since the last update (new ML entity, new versions). ml-git <ml-entity> unlock Usage: ml-git datasets unlock [OPTIONS] ML_ENTITY_NAME FILE This command add read and write permissions to file or directory. Note: You should only use this command for the flexible mutability option. Options: --verbose Debug mode Example: ml-git datasets unlock dataset-ex data/file1.txt Note: You should only use this command for the flexible mutability option. ml-git clone <repository-url> Usage: ml-git clone [OPTIONS] REPOSITORY_URL [DIRECTORY] Clone an ml-git repository ML_GIT_REPOSITORY_URL Options: --untracked Does not preserve git repository tracking. --verbose Debug mode Example: ml-git clone https://git@github.com/mlgit-repository ml-git login Usage: ml-git login [OPTIONS] login command generates new Aws credential. Options: --credentials TEXT profile name for store credentials [default: default]. --insecure use this option when operating in a insecure location. This option prevents storage of a cookie in the folder. Never execute this program without --insecure option in a compute device you do not trust. --rolearn TEXT directly STS to this AWS Role ARN instead of the selecting the option during runtime. --help Show this message and exit. Example: ml-git login ml-git repository config Usage: ml-git repository config [OPTIONS] COMMAND [ARGS]... Management of the ML-Git config file. Options: --set-wizard [enabled|disabled] Enable or disable the wizard for all supported commands. --help Show this message and exit. Commands: push Create a new version of the ML-Git configuration file. show Configuration of this ML-Git repository Example: ml-git repository config --set-wizard=enabled ml-git repository config push Usage: ml-git repository config push [OPTIONS] Create a new version of the ML-Git configuration file. This command internally runs git's add, commit and push commands. Options: -m, --message TEXT Use the provided <msg> as the commit message. --verbose Debug mode Example: ml-git repository config push -m \"My commit message\" ml-git repository config show Usage: ml-git repository config show [OPTIONS] Configuration of this ml-git repository Options: -l, --local Local configurations -g, --global Global configurations --verbose Debug mode Example: ml-git repository config show Output: config: {'datasets': {'git': 'git@github.com:example/your-mlgit-datasets'}, 'storages': {'s3': {'mlgit-datasets': {'aws-credentials': {'profile': 'mlgit'}, 'region': 'us-east-1'}}}, 'verbose': 'info'} Use this command if you want to check what configuration ml-git is running with. It is highly likely one will need to change the default configuration to adapt for her needs. ml-git repository gc Usage: ml-git repository gc [OPTIONS] Cleanup unnecessary files and optimize the use of the disk space. Options: --verbose Debug mode This command will remove unnecessary files contained in the cache and objects directories of the ml-git metadata (.ml-git). ml-git repository graph Usage: ml-git repository graph [OPTIONS] Creates a graph of all entity relations as an HTML file and automatically displays it in the default system application. Options: --verbose Debug mode --dot Instead of creating an HTML file, it displays the graph on the command line as a DOT language. --export-path TEXT Set the directory path to export the generated graph file. Example: ml-git repository graph Output: digraph \"Entities Graph\" { \"models-ex (1)\" [color=\"#d63638\"]; \"dataset-ex (1)\" [color=\"#2271b1\"]; \"models-ex (1)\" -> \"dataset-ex (1)\"; \"models-ex (1)\" [color=\"#d63638\"]; \"labels-ex (1)\" [color=\"#996800\"]; \"models-ex (1)\" -> \"labels-ex (1)\"; } This command will iterate through the tags of all ML-Git entities and create the relationships between them. Note: To successfully execute the command it is necessary that it is in an ML-Git project initialized, and with the URLs of the remote repositories properly configured. ml-git repository init Usage: ml-git repository init [OPTIONS] Initialization of this ML-Git repository Options: --help Show this message and exit. Example: ml-git repository init This is the first command you need to run to initialize a ml-git project. It will bascially create a default .ml-git/config.yaml ml-git repository remote <ml-entity> add Usage: ml-git repository remote datasets add [OPTIONS] REMOTE_URL Add remote dataset metadata REMOTE_URL to this ml-git repository. Options: --help Show this message and exit. Example: ml-git repository remote datasets add https://git@github.com/mlgit-datasets ml-git repository remote <ml-entity> del Usage: ml-git repository remote datasets del Remove remote datasets metadata REMOTE_URL from this ml-git repository Options: --help Show this message and exit. Example: ml-git repository remote datasets del ml-git repository remote config add Usage: ml-git repository remote config add [OPTIONS] REMOTE_URL Starts a git at the root of the project and configure the remote. Options: --verbose Debug mode Example: ml-git repository remote config add https://git@github.com/mlgit-config ml-git repository storage add Usage: ml-git repository storage add [OPTIONS] BUCKET_NAME Add a storage BUCKET_NAME to ml-git [This command has a wizard that will request the necessary information if it is not informed] Options: --credentials TEXT Profile name for storage credentials --type [s3h|azureblobh|gdriveh|sftph] Storage type (s3h, azureblobh, gdriveh, sftph) [default: s3h] --region TEXT AWS region name for S3 bucket --endpoint-url TEXT Storage endpoint url. --username TEXT The username for the sftp login. --private-key TEXT Full path for the private key file. --port INTEGER SFTP port [default: 22]. -g, --global Use this option to set configuration at global level --wizard Enable the wizard to request information when needed. --verbose Debug mode Example: ml-git repository storage add minio --endpoint-url=<minio-endpoint-url> Use this command to add a data storage to a ml-git project. ml-git repository storage del Usage: ml-git repository storage del [OPTIONS] BUCKET_NAME Delete a storage BUCKET_NAME from ml-git Options: --type [s3h|azureblobh|gdriveh|sftph] Storage type (s3h, azureblobh, gdriveh, sftph) [default: s3h] -g, --global Use this option to set configuration at global level --wizard Enable the wizard to request information when needed. --verbose Debug mode Example: ml-git repository storage del minio ml-git repository update Usage: ml-git repository update This command updates the metadata for all entities. Example: ml-git repository update","title":"Using ML-Git CLI"},{"location":"mlgit_commands/#ml-git-commands","text":"ml-git --help Usage: ml-git [OPTIONS] COMMAND [ARGS]... Options: --version Show the version and exit. Commands: clone Clone an ml-git repository ML_GIT_REPOSITORY_URL datasets Management of datasets within this ml-git repository. labels Management of labels sets within this ml-git repository. models Management of models within this ml-git repository. repository Management of this ml-git repository. Example: ml-git --help ml-git --version Displays the installed version of ML-Git. ml-git <ml-entity> add Usage: ml-git datasets add [OPTIONS] ML_ENTITY_NAME [FILE_PATH]... Add datasets change set ML_ENTITY_NAME to the local ml-git staging area. Options: --bumpversion Increment the version number when adding more files. --fsck Run fsck after command execution. --metric <TEXT FLOAT>... Metric key and value. --metrics-file Metrics file path. --wizard Enable the wizard to request information when needed. --verbose Debug mode Dataset example: ml-git datasets add dataset-ex --bumpversion ml-git expects datasets to be managed under dataset directory. \\<ml-entity-name> is also expected to be a repository under the tree structure and ml-git will search for it in the tree. Under that repository, it is also expected to have a \\<ml-entity-name>.spec file, defining the ML entity to be added. Optionally, one can add a README.md which will describe the dataset and be what will be shown in the github repository for that specific dataset. Internally, the ml-git add will add all the files under the \\<ml-entity> directory into the ml-git index / staging area. Model example: ml-git models add model-ex --metrics-file='/path/to/your/file.csv' ml-git allows you to enter a metrics file or the metrics themselves on the command line when adding a model. ml-git <ml-entity> branch Usage: ml-git datasets branch [OPTIONS] ML_ENTITY_NAME This command allows to check which tag is checked out in the ml-git workspace. Options: --verbose Debug mode Example: ml-git datasets branch imagenet8 Output: ('vision-computing__images__imagenet8__1', '48ba1e994a1e39e1b508bff4a3302a5c1bb9063e') That information is equal to the HEAD reference from a git concept. ml-git keeps that information on a per \\<ml-entity-name> basis. which enables independent checkout of each of these \\<ml-entity-name>. The output is a tuple: 1) the tag auto-generated by ml-git based on the \\<ml-entity-name>.spec (composite with categories, \\<ml-entity-name>, version) 2) the sha of the git commit of that \\<ml-entity> version Both are the same representation. One is human-readable and is also used internally by ml-git to find out the path to the referenced \\<ml-entity-name>. ml-git <ml-entity> checkout Usage: ml-git models checkout [OPTIONS] ML_ENTITY_TAG|ML_ENTITY Checkout the ML_ENTITY_TAG|ML_ENTITY of a model set into user workspace. Options: -l, --with-labels The checkout associated labels in user workspace as well. -d, --with-dataset The checkout associated dataset in user workspace as well. --retry INTEGER RANGE Number of retries to download the files from the storage. This number must be in the range 0-999999999 [default: 2]. --force Force checkout command to delete untracked/uncommitted files from local repository. --bare Ability to add/commit/push without having the ml-entity checked out. --version INTEGER RANGE Number of artifact version to be downloaded. This number must be in the range 0-999999999 [default: latest]. --fail-limit INTEGER RANGE Number of failures before aborting the command. This number must be in the range 0-999999999 [default: no limit]. --full Show all contents for each directory. --wizard Enable the wizard to request information when needed. --verbose Debug mode Examples: ml-git datasets checkout computer-vision__images__faces__fddb__1 or you can use the name of the entity directly and download the latest available tag ml-git datasets checkout fddb Note: --d: It can only be used in checkout of labels and models to get the entities that are associated with the entity. --l: It can only be used in checkout of models to get the label entity that are associated with the entity. --sample-type, --sampling, --seed: These options are available only for dataset. If you use this option ml-git will not allow you to make changes to the entity and create a new tag. ml-git <ml-entity> commit Usage: ml-git models commit [OPTIONS] ML_ENTITY_NAME Commit model change set of ML_ENTITY_NAME locally to this ml-git repository. Options: --dataset NOT EMPTY STRING Link a dataset entity name to this model set version --labels NOT EMPTY STRING Link a labels entity name to this model set version --version INTEGER RANGE Set the version number of the artifact. This number must be in the range 0-999999999. -m, --message TEXT Use the provided <msg> as the commit message. --fsck Run fsck after command execution. --wizard Enable the wizard to request information when needed. --verbose Debug mode Example: ml-git models commit model-ex --dataset=dataset-ex This command commits the index / staging area to the local repository. It is a 2-step operation in which 1) the actual data (blobs) is copied to the local repository, 2) committing the metadata to the git repository managing the metadata. Internally, ml-git keeps track of files that have been added to the data storage and is storing that information to the metadata management layer to be able to restore any version of each \\<ml-entity-name>. Another important feature of ml-git is the ability to keep track of the relationship between the ML entities. So when committing a label set, one can (should) provide the option --dataset=<dataset-name> . Internally, ml-git will inspect the HEAD / ref of the specified \\<dataset-name> checked out in the ml-git repository and will add that information to the specificatino file that is committed to the metadata repository. With that relationship kept into the metadata repository, it is now possible for anyone to checkout exactly the same versions of labels and dataset. Same for ML model, one can specify which dataset and label set that have been used to generate that model through --dataset=<dataset-name> and --labels=<labels-name> ml-git <ml-entity> create Usage: ml-git datasets create [OPTIONS] ARTIFACT_NAME This command will create the workspace structure with data and spec file for an entity and set the git and storage configurations. [This command has a wizard that will request the necessary information if it is not informed] Options: --categories TEXT Artifact's categories names. The categories names must be separated by comma. E.g: \"category1,category2,category3\". [required] --mutability [strict|flexible|mutable] Mutability type. [required] --storage-type [s3h|azureblobh|gdriveh|sftph] Storage type (s3h, azureblobh, gdriveh, sftph) [default: s3h] --version INTEGER RANGE Set the version number of the artifact. This number must be in the range 0-999999999. --import NOT EMPTY STRING Path to be imported to the project. NOTE: Mutually exclusive with argument: credentials_path, import_url. --wizard-config If specified, ask interactive questions at console for git & storage configurations. [DEPRECATED: This option should no longer be used.] --bucket-name NOT EMPTY STRING Bucket name --import-url NOT EMPTY STRING Import data from a google drive url. NOTE: Mutually exclusive with argument: import. --credentials-path NOT EMPTY STRING Directory of credentials.json. NOTE: This option is required if --import-url is used. --unzip Unzip imported zipped files. Only available if --import-url is used. --entity-dir NOT EMPTY STRING The relative path where the entity will be created inside the ml entity directory. --wizard Enable the wizard to request information when needed. --verbose Debug mode Examples: - To create an entity with s3 as storage and importing files from a path of your computer: ml-git datasets create imagenet8 --storage-type=s3h --categories=\"computer-vision, images\" --version=0 --import='/path/to/dataset' --mutability=strict To create an entity with s3 as storage and importing files from a google drive URL: ml-git datasets create imagenet8 --storage-type=s3h --categories=computer-vision,images --import-url='gdrive.url' --credentials-path='/path/to/gdrive/credentials' --mutability=strict --unzip ml-git <ml-entity> diff Usage: ml-git datasets diff [OPTIONS] ML_ENTITY_NAME FIRST_TAG SECOND_TAG Print the difference between two entity tag versions. The command will show added, updated and deleted files. Options: --full Show all contents for each directory. --verbose Debug mode Examples: - To check the difference between entity tag versions: ml-git datasets diff dataset-ex computer-vision__images__dataset-ex__1 computer-vision__images__dataset-ex__4 Output: Added files: data/ -> 4 FILES tabular.csv Updated files: data/dataset_test.csv Deleted files: data/dataset_old.csv To check the difference between entity tag versions showing all contents for each directory: ml-git datasets diff --full dataset-ex computer-vision__images__dataset-ex__1 computer-vision__images__dataset-ex__4 Output: Added files: data/dataset_1.csv data/dataset_2.csv data/dataset_3.csv data/dataset_4.csv tabular.csv Updated files: data/dataset_test.csv Deleted files: data/dataset_old.csv ml-git <ml-entity> export Usage: ml-git datasets export [OPTIONS] ML_ENTITY_TAG BUCKET_NAME This command allows you to export files from one storage (S3|MinIO) to another (S3|MinIO). Options: --credentials TEXT Profile of AWS credentials [default: default]. --endpoint TEXT Storage endpoint url. --region TEXT AWS region name [default: us-east-1]. --retry INTEGER RANGE Number of retries to download the files from the storage. This number must be in the range 0-999999999 [default: 2]. --verbose Debug mode Example: ml-git datasets export computer-vision__images__faces__fddb__1 minio ml-git <ml-entity> fetch Usage: ml-git datasets fetch [OPTIONS] ML_ENTITY_TAG Allows you to download just the metadata files of an entity. Options: --sample-type [group|range|random] --sampling TEXT The group: <amount>:<group> The group sample option consists of amount and group used to download a sample. range: <start:stop:step> The range sample option consists of start, stop and step used to download a sample. The start parameter can be equal or greater than zero.The stop parameter can be 'all', -1 or any integer above zero. random: <amount:frequency> The random sample option consists of amount and frequency used to download a sample. --seed TEXT Seed to be used in random-based samplers. --retry INTEGER RANGE Number of retries to download the files from the storage. This number must be in the range 0-999999999 [default: 2]. --verbose Debug mode Example: ml-git datasets fetch computer-vision__images__faces__fddb__1 ml-git <ml-entity> fsck Usage: ml-git datasets fsck [OPTIONS] Options: --fix-workspace Use this option to repair files identified as corrupted in the entity workspace. --full Show the list of corrupted files. --verbose Debug mode Example: ml-git datasets fsck This command will walk through the internal ml-git directories (index & local repository) and check the presence and integrity of all file blobs under its management. This command will basically try to: Detect any chunk/blob that is corrupted or missing in the internal ml-git directory (.ml-git/{entity-type}/objects) Fetch files detected as corrupted or missing from storage Check the integrity of files mounted in the entities workspace In fix-workspace mode, repair corrupted files found in the entities workspace. A file in the entities workspace is considered 'corrupted' based on the business rule defined by the mutability of the entity. If you want to know more about each type of mutability and how it works, please take a look at Mutability documentation . It will return the list of blobs that are corrupted/missing if the user passes the --full option. ml-git <ml-entity> import Usage: ml-git datasets import [OPTIONS] BUCKET_NAME ENTITY_DIR This command allows you to download a file or directory from the S3 or Gdrive to ENTITY_DIR. Options: --credentials TEXT Input your profile to an s3 storage or your credentials path to a gdrive storage.(eg, --credentials=path/to/.credentials --region TEXT AWS region name [default: us-east-1]. --retry INTEGER RANGE Number of retries to download the files from the storage. This number must be in the range 0-999999999 [default: 2]. --path TEXT Storage folder path. --object TEXT Filename in storage. --storage-type [s3|gdrive] Storage type (s3, gdrive) [default: s3] --endpoint-url TEXT Storage endpoint url. --verbose Debug mode Example: ml-git datasets import bucket-name dataset/computer-vision/imagenet8/data For google drive storage: ml-git datasets import gdrive-folder --storage-type=gdrive --object=file_to_download --credentials=credentials-path dataset/ ml-git <ml-entity> init Usage: ml-git datasets init [OPTIONS] Init a ml-git datasets repository. Options: --verbose Debug mode Example: ml-git datasets init This command is mandatory to be executed just after the addition of a remote metadata repository ( ml-git \\<ml-entity> remote add ). It initializes the metadata by pulling all metadata to the local repository. ml-git <ml-entity> metrics Usage: ml-git models metrics [OPTIONS] ML_ENTITY_NAME Shows metrics information for each tag of the entity. Options: --export-path TEXT Set the path to export metrics to a file. NOTE: This option is required if --export-type is used. --export-type [csv|json] Choose the format of the file that will be generated with the metrics [default: json]. --verbose Debug mode Example: ml-git models metrics model-ex Note: This command is only available for model entities. ml-git <ml-entity> list Usage: ml-git datasets list [OPTIONS] List datasets managed under this ml-git repository. Options: --verbose Debug mode Example: ml-git datasets list Output: ML dataset |-- computer-vision | |-- images | | |-- dataset-ex-minio | | |-- imagenet8 | | |-- dataset-ex ml-git <ml-entity> log Usage: ml-git datasets log [OPTIONS] ML_ENTITY_NAME This command shows ml-entity-name's commit information like author, date, commit message. Options: --stat Show amount of files and size of an ml-entity. --fullstat Show added and deleted files. --verbose Debug mode Example: ml-git datasets log dataset-ex ml-git <ml-entity> push Usage: ml-git datasets push [OPTIONS] ML_ENTITY_NAME Push local commits from ML_ENTITY_NAME to remote ml-git repository & storage. Options: --retry INTEGER RANGE Number of retries to download the files from the storage. This number must be in the range 0-999999999 [default: 2]. --clearonfail Remove the files from the storage in case of failure during the push operation. --fail-limit INTEGER RANGE Number of failures before aborting the command. This number must be in the range 0-999999999 [default: no limit]. --verbose Debug mode Example: ml-git datasets push dataset-ex This command will perform a 2-step operations: 1. push all blobs to the configured data storage. 2. push all metadata related to the commits to the remote metadata repository. ml-git <ml-entity> remote-fsck Usage: ml-git datasets remote-fsck [OPTIONS] ML_ENTITY_NAME This command will check and repair the remote, by default it will only repair by uploading lacking chunks/blobs. Options bring more specialized repairs. Options: --thorough Try to download the IPLD if it is not present in the local repository to verify the existence of all contained IPLD links associated. --paranoid Adds an additional step that will download all IPLD and its associated IPLD links to verify the content by computing the multihash of all these. --retry INTEGER RANGE Number of retries to download the files from the storage. This number must be in the range 0-999999999 [default: 2]. --full Show the list of fixed and unfixed blobs and IPLDs. --wizard Enable the wizard to request information when needed. --verbose Debug mode Example: ml-git datasets remote-fsck dataset-ex This ml-git command will basically try to: Detects any chunk/blob lacking in a remote storage for a specific ML artefact version Repair - if possible - by uploading lacking chunks/blobs In paranoid mode, verifies the content of all the blobs ml-git <ml-entity> reset Usage: ml-git datasets reset [OPTIONS] ML_ENTITY_NAME Reset ml-git state(s) of an ML_ENTITY_NAME Options: --hard Remove untracked files from workspace, files to be committed from staging area as well as committed files upto <reference>. --mixed Revert the committed files and the staged files to 'Untracked Files'. This is the default action. --soft Revert the committed files to 'Changes to be committed'. --reference [head|head~1] head:Will keep the metadata in the current commit. head~1:Will move the metadata to the last commit. --verbose Debug mode Examples: ml-git datasets reset dataset-ex --hard Undo the committed changes. Undo the added/tracked files. Reset the workspace to fit with the current HEAD state. ml-git datasets reset dataset-ex --mixed if HEAD: * nothing happens. else: * Undo the committed changes. * Undo the added/tracked files. ml-git datasets reset dataset-ex --soft if HEAD: * nothing happens. else: * Undo the committed changes. ml-git <ml-entity> show Usage: ml-git datasets show [OPTIONS] ML_ENTITY_NAME Print the specification file of the entity. Options: --verbose Debug mode Example: ml-git datasets show dataset-ex Output: -- dataset : imagenet8 -- categories: - vision-computing - images manifest: files: MANIFEST.yaml storage: s3h://mlgit-datasets name: imagenet8 version: 1 ml-git <ml-entity> status Usage: ml-git datasets status [OPTIONS] ML_ENTITY_NAME [STATUS_DIRECTORY] Print the files that are tracked or not and the ones that are in the index/staging area. Options: --full Show all contents for each directory. --verbose Debug mode Example: ml-git datasets status dataset-ex ml-git <ml-entity> tag add Usage: ml-git datasets tag add [OPTIONS] ML_ENTITY_NAME TAG Use this command to associate a tag to a commit. Options: --verbose Debug mode Example: ml-git datasets tag add dataset-ex my_tag ml-git <ml-entity> tag list Usage: ml-git datasets tag list [OPTIONS] ML_ENTITY_NAME List tags of ML_ENTITY_NAME from this ml-git repository. Options: --verbose Debug mode Example: ml-git datasets tag list dataset-ex ml-git <ml-entity> update Usage: ml-git datasets update [OPTIONS] This command will update the metadata repository. Options: --verbose Debug mode Example: ml-git datasets update This command enables one to have the visibility of what has been shared since the last update (new ML entity, new versions). ml-git <ml-entity> unlock Usage: ml-git datasets unlock [OPTIONS] ML_ENTITY_NAME FILE This command add read and write permissions to file or directory. Note: You should only use this command for the flexible mutability option. Options: --verbose Debug mode Example: ml-git datasets unlock dataset-ex data/file1.txt Note: You should only use this command for the flexible mutability option. ml-git clone <repository-url> Usage: ml-git clone [OPTIONS] REPOSITORY_URL [DIRECTORY] Clone an ml-git repository ML_GIT_REPOSITORY_URL Options: --untracked Does not preserve git repository tracking. --verbose Debug mode Example: ml-git clone https://git@github.com/mlgit-repository ml-git login Usage: ml-git login [OPTIONS] login command generates new Aws credential. Options: --credentials TEXT profile name for store credentials [default: default]. --insecure use this option when operating in a insecure location. This option prevents storage of a cookie in the folder. Never execute this program without --insecure option in a compute device you do not trust. --rolearn TEXT directly STS to this AWS Role ARN instead of the selecting the option during runtime. --help Show this message and exit. Example: ml-git login ml-git repository config Usage: ml-git repository config [OPTIONS] COMMAND [ARGS]... Management of the ML-Git config file. Options: --set-wizard [enabled|disabled] Enable or disable the wizard for all supported commands. --help Show this message and exit. Commands: push Create a new version of the ML-Git configuration file. show Configuration of this ML-Git repository Example: ml-git repository config --set-wizard=enabled ml-git repository config push Usage: ml-git repository config push [OPTIONS] Create a new version of the ML-Git configuration file. This command internally runs git's add, commit and push commands. Options: -m, --message TEXT Use the provided <msg> as the commit message. --verbose Debug mode Example: ml-git repository config push -m \"My commit message\" ml-git repository config show Usage: ml-git repository config show [OPTIONS] Configuration of this ml-git repository Options: -l, --local Local configurations -g, --global Global configurations --verbose Debug mode Example: ml-git repository config show Output: config: {'datasets': {'git': 'git@github.com:example/your-mlgit-datasets'}, 'storages': {'s3': {'mlgit-datasets': {'aws-credentials': {'profile': 'mlgit'}, 'region': 'us-east-1'}}}, 'verbose': 'info'} Use this command if you want to check what configuration ml-git is running with. It is highly likely one will need to change the default configuration to adapt for her needs. ml-git repository gc Usage: ml-git repository gc [OPTIONS] Cleanup unnecessary files and optimize the use of the disk space. Options: --verbose Debug mode This command will remove unnecessary files contained in the cache and objects directories of the ml-git metadata (.ml-git). ml-git repository graph Usage: ml-git repository graph [OPTIONS] Creates a graph of all entity relations as an HTML file and automatically displays it in the default system application. Options: --verbose Debug mode --dot Instead of creating an HTML file, it displays the graph on the command line as a DOT language. --export-path TEXT Set the directory path to export the generated graph file. Example: ml-git repository graph Output: digraph \"Entities Graph\" { \"models-ex (1)\" [color=\"#d63638\"]; \"dataset-ex (1)\" [color=\"#2271b1\"]; \"models-ex (1)\" -> \"dataset-ex (1)\"; \"models-ex (1)\" [color=\"#d63638\"]; \"labels-ex (1)\" [color=\"#996800\"]; \"models-ex (1)\" -> \"labels-ex (1)\"; } This command will iterate through the tags of all ML-Git entities and create the relationships between them. Note: To successfully execute the command it is necessary that it is in an ML-Git project initialized, and with the URLs of the remote repositories properly configured. ml-git repository init Usage: ml-git repository init [OPTIONS] Initialization of this ML-Git repository Options: --help Show this message and exit. Example: ml-git repository init This is the first command you need to run to initialize a ml-git project. It will bascially create a default .ml-git/config.yaml ml-git repository remote <ml-entity> add Usage: ml-git repository remote datasets add [OPTIONS] REMOTE_URL Add remote dataset metadata REMOTE_URL to this ml-git repository. Options: --help Show this message and exit. Example: ml-git repository remote datasets add https://git@github.com/mlgit-datasets ml-git repository remote <ml-entity> del Usage: ml-git repository remote datasets del Remove remote datasets metadata REMOTE_URL from this ml-git repository Options: --help Show this message and exit. Example: ml-git repository remote datasets del ml-git repository remote config add Usage: ml-git repository remote config add [OPTIONS] REMOTE_URL Starts a git at the root of the project and configure the remote. Options: --verbose Debug mode Example: ml-git repository remote config add https://git@github.com/mlgit-config ml-git repository storage add Usage: ml-git repository storage add [OPTIONS] BUCKET_NAME Add a storage BUCKET_NAME to ml-git [This command has a wizard that will request the necessary information if it is not informed] Options: --credentials TEXT Profile name for storage credentials --type [s3h|azureblobh|gdriveh|sftph] Storage type (s3h, azureblobh, gdriveh, sftph) [default: s3h] --region TEXT AWS region name for S3 bucket --endpoint-url TEXT Storage endpoint url. --username TEXT The username for the sftp login. --private-key TEXT Full path for the private key file. --port INTEGER SFTP port [default: 22]. -g, --global Use this option to set configuration at global level --wizard Enable the wizard to request information when needed. --verbose Debug mode Example: ml-git repository storage add minio --endpoint-url=<minio-endpoint-url> Use this command to add a data storage to a ml-git project. ml-git repository storage del Usage: ml-git repository storage del [OPTIONS] BUCKET_NAME Delete a storage BUCKET_NAME from ml-git Options: --type [s3h|azureblobh|gdriveh|sftph] Storage type (s3h, azureblobh, gdriveh, sftph) [default: s3h] -g, --global Use this option to set configuration at global level --wizard Enable the wizard to request information when needed. --verbose Debug mode Example: ml-git repository storage del minio ml-git repository update Usage: ml-git repository update This command updates the metadata for all entities. Example: ml-git repository update","title":"ML-Git commands"},{"location":"mlgit_internals/","text":"ML-Git: architecture and internals \u00b6 Metadata & data decoupling \u00b6 ML-Git's first design concept is to decouple the ML entities' metadata management from the actual data. So, the tool has two main layers: The metadata management, is responsible for organize the ML entities (Models, Datasets, and Labels) through specification files. Then, these files are managed by a git repository to store and retrieve versions of the ML entities. The data storage, is responsible to keep the files of the ML entities. Figure 1. Decoupling Metadata & Data Management Layers CAS for ML-Git \u00b6 ML-Git has been implemented as a Content Addressable Storage (CAS), meaning that we can retrieve the information based on the content and not based on the information's location. Figure 2. Self-Describing Content-Addressed ID Figure 2 shows the basic principle of multihash to obtain a Content Identifier (CID) which is used under the hood by ML-Git to implement its CAS layer. In a nutshell, CID is a self-describing content-addressed identifier that enables natural evolution and customization over simple and fixed cryptographic hashing schemes. An argument why multihash is a valuable feature is that any cryptographic function ultimately ends being weak. It's been a challenge for many software to use another cryptographic hash (including git). For example, when collisions have been proven with SHA-1. Summarizing, a CID is: A unique identifier/hash of \u201cmultihash\u201d content. Encoding the digest of the original content enabling anyone to retrieve thatcontent wherever it lies (through some routing). Enabling the integrity check of the retrieved content (thx to multihash and the encoded digest). Figure 3. IPLD - CID for a file There are a few steps to chunk a file to get an IPLD - CID format: Slide the file in piece of, say, 256KB. For each slice, compute its digest (currently, ml-git uses sha2-256). Obtain the CID for all these digests. These slice of files will be saved in a data storage with the computed CID as their filename. Build a json describing all the chunks of the file. Obtain the CID of that json. That json will also be saved in the data storage with the computed CID as its filename. Note that this last CID is the only piece of information you need to keep to retrieve the whole image.jpg file. And last but not least, one can ensure the integrity of the file while downloading by computing the digests of all downloaded chunks and checking against the digest encoded in their CID. Below, you can find useful links for more information on: Multihash CID IPLD Why slicing files in chunks? \u00b6 IPFS uses small chunk size of 256KB \u2026 Why? security - easy to DOS nodes without forcing small chunks deduplication - small chunks can dedup. big ones effectively dont. latency - can externalize small pieces already (think a stream) bandwidth - optimize the use of bandwidth across many peers performance - better perf to hold small pieces in memory. Hash along the dag to verify integrity of the whole thing. The big DOS problem with huge leaves is that malicious nodes can serve bogus stuff for a long time before a node can detect the problem (imagine having to download 4GB before you can check whether any of it is valid). This was super harmful for bittorrent (when people started choosing huge piece sizes), attackers would routinely do this, very cheaply - just serve bogus random data. This is why smaller chunks are used in our approach. High-level architecture and metadata \u00b6 Figure 4. ML-Git high-level architecture and metadata relationships So IPLD/CID has been implemented on top of the storage. The chunking strategy is a recommendation to turn S3 interactions more efficient when dealing with large files. This approach is also valid when using the other supported storages: Azure, Google Drive, MinIO and SFTP. It's also interesting to note that ML-Git implements a Thread pool to concurrently upload & download files to the storage. Taking into account the use of an S3 bucket, it would be possible to further accelerate ML-Git interactions with the bucket through the AWS CloudFront (not implemented yet). ML-Git baseline performance numbers \u00b6 CamSeq01 under ML-Git \u00b6 CamSeq01 size : 92MB Locations: website in Cambridge -- S3 bucket in us-east-1 -- me in South Brazil Download from website: ~4min22s Upload to S3 with ml-git : 6m49s Download to S3 with ml-git : 1m11s MSCoco (all files) under ML-Git \u00b6 MSCoco : Size : 26GB Number of files : 164065 ; chunked into ~400-500K blobs (todo: exact blob count) Locations: original dataset: unknown -- S3 bucket in us-east-1 -- me in South Brazil Download from website: unknown Upload to S3 with ml-git : 12h30m Download to S3 with ml-git : 10h45m MSCoco (zip files) under ML-Git \u00b6 MSCoco : Size : 25GB number of files : 3 (train.zip, test.zip, val.zip) ; 102299 blobs Locations: original dataset: unknown -- S3 bucket in us-east-1 -- me in South Brazil Download from website: unknown Upload to S3 with ml-git : 4h35m Download to S3 with ml-git : 3h39m A couple of comments: Even though Python GIL is a challenge for true concurrency in the Python interpreter, it still is very helpful and provides a significant improvement for ML-Git performance. Not surprisingly, the number of files will affect the overall performance as it means there will be many more connections to AWS. However, ML-Git have an option to download some dataset partially (checkout with sampling) to enable CI/CD workflows for which some ML engineers may run some experiments locally on their own machine. For that reason, it is interesting to avoid downloading the full dataset if it's very large. This option is not applicable if the data set was loaded as some zip files.","title":"Design"},{"location":"mlgit_internals/#ml-git-architecture-and-internals","text":"","title":"ML-Git: architecture and internals"},{"location":"mlgit_internals/#metadata-data-decoupling","text":"ML-Git's first design concept is to decouple the ML entities' metadata management from the actual data. So, the tool has two main layers: The metadata management, is responsible for organize the ML entities (Models, Datasets, and Labels) through specification files. Then, these files are managed by a git repository to store and retrieve versions of the ML entities. The data storage, is responsible to keep the files of the ML entities. Figure 1. Decoupling Metadata & Data Management Layers","title":"Metadata &amp; data decoupling"},{"location":"mlgit_internals/#cas-for-ml-git","text":"ML-Git has been implemented as a Content Addressable Storage (CAS), meaning that we can retrieve the information based on the content and not based on the information's location. Figure 2. Self-Describing Content-Addressed ID Figure 2 shows the basic principle of multihash to obtain a Content Identifier (CID) which is used under the hood by ML-Git to implement its CAS layer. In a nutshell, CID is a self-describing content-addressed identifier that enables natural evolution and customization over simple and fixed cryptographic hashing schemes. An argument why multihash is a valuable feature is that any cryptographic function ultimately ends being weak. It's been a challenge for many software to use another cryptographic hash (including git). For example, when collisions have been proven with SHA-1. Summarizing, a CID is: A unique identifier/hash of \u201cmultihash\u201d content. Encoding the digest of the original content enabling anyone to retrieve thatcontent wherever it lies (through some routing). Enabling the integrity check of the retrieved content (thx to multihash and the encoded digest). Figure 3. IPLD - CID for a file There are a few steps to chunk a file to get an IPLD - CID format: Slide the file in piece of, say, 256KB. For each slice, compute its digest (currently, ml-git uses sha2-256). Obtain the CID for all these digests. These slice of files will be saved in a data storage with the computed CID as their filename. Build a json describing all the chunks of the file. Obtain the CID of that json. That json will also be saved in the data storage with the computed CID as its filename. Note that this last CID is the only piece of information you need to keep to retrieve the whole image.jpg file. And last but not least, one can ensure the integrity of the file while downloading by computing the digests of all downloaded chunks and checking against the digest encoded in their CID. Below, you can find useful links for more information on: Multihash CID IPLD","title":"CAS for ML-Git"},{"location":"mlgit_internals/#why-slicing-files-in-chunks","text":"IPFS uses small chunk size of 256KB \u2026 Why? security - easy to DOS nodes without forcing small chunks deduplication - small chunks can dedup. big ones effectively dont. latency - can externalize small pieces already (think a stream) bandwidth - optimize the use of bandwidth across many peers performance - better perf to hold small pieces in memory. Hash along the dag to verify integrity of the whole thing. The big DOS problem with huge leaves is that malicious nodes can serve bogus stuff for a long time before a node can detect the problem (imagine having to download 4GB before you can check whether any of it is valid). This was super harmful for bittorrent (when people started choosing huge piece sizes), attackers would routinely do this, very cheaply - just serve bogus random data. This is why smaller chunks are used in our approach.","title":"Why slicing files in chunks?"},{"location":"mlgit_internals/#high-level-architecture-and-metadata","text":"Figure 4. ML-Git high-level architecture and metadata relationships So IPLD/CID has been implemented on top of the storage. The chunking strategy is a recommendation to turn S3 interactions more efficient when dealing with large files. This approach is also valid when using the other supported storages: Azure, Google Drive, MinIO and SFTP. It's also interesting to note that ML-Git implements a Thread pool to concurrently upload & download files to the storage. Taking into account the use of an S3 bucket, it would be possible to further accelerate ML-Git interactions with the bucket through the AWS CloudFront (not implemented yet).","title":"High-level architecture and metadata"},{"location":"mlgit_internals/#ml-git-baseline-performance-numbers","text":"","title":"ML-Git baseline performance numbers"},{"location":"mlgit_internals/#camseq01-under-ml-git","text":"CamSeq01 size : 92MB Locations: website in Cambridge -- S3 bucket in us-east-1 -- me in South Brazil Download from website: ~4min22s Upload to S3 with ml-git : 6m49s Download to S3 with ml-git : 1m11s","title":"CamSeq01 under ML-Git"},{"location":"mlgit_internals/#mscoco-all-files-under-ml-git","text":"MSCoco : Size : 26GB Number of files : 164065 ; chunked into ~400-500K blobs (todo: exact blob count) Locations: original dataset: unknown -- S3 bucket in us-east-1 -- me in South Brazil Download from website: unknown Upload to S3 with ml-git : 12h30m Download to S3 with ml-git : 10h45m","title":"MSCoco (all files) under ML-Git"},{"location":"mlgit_internals/#mscoco-zip-files-under-ml-git","text":"MSCoco : Size : 25GB number of files : 3 (train.zip, test.zip, val.zip) ; 102299 blobs Locations: original dataset: unknown -- S3 bucket in us-east-1 -- me in South Brazil Download from website: unknown Upload to S3 with ml-git : 4h35m Download to S3 with ml-git : 3h39m A couple of comments: Even though Python GIL is a challenge for true concurrency in the Python interpreter, it still is very helpful and provides a significant improvement for ML-Git performance. Not surprisingly, the number of files will affect the overall performance as it means there will be many more connections to AWS. However, ML-Git have an option to download some dataset partially (checkout with sampling) to enable CI/CD workflows for which some ML engineers may run some experiments locally on their own machine. For that reason, it is interesting to avoid downloading the full dataset if it's very large. This option is not applicable if the data set was loaded as some zip files.","title":"MSCoco (zip files) under ML-Git"},{"location":"mutability_helper/","text":"Mutability \u00b6 What is the mutability? \u00b6 Mutability is the attribute that defines whether an entity's files can be changed by the user from one version to another. It is important to note that for all types of mutability the user is able to add and remove files, the mutability attribute refers to editing files already added. You must define carefully because once mutability is defined, it cannot be changed. Where to define mutability policy? \u00b6 Mutability is defined when creating a new entity. With the command ml-git (datasets|labels|models) create you must pass the mandatory attribute mutability to define the type of mutability for the created entity. Your entity specification file (.spec) should look like this: dataset: categories: - computer-vision - images mutability: flexible manifest: storage: s3h://mlgit-datasets name: imagenet8 version: 1 If you create an entity without using the create command and without mutability, when trying to perform the ml-git (datasets|labels|models) add the command will not be executed and you will be informed that you must define a mutability for that new entity. Note: For entities that were created before the mutability parameter became mandatory and that did not define mutability, ml-git treats these entities as strict. Because it is an attribute defined in the spec, you can define a type of mutability for each entity that the project has. For example, you can have in the same project a dataset-ex1 that has strict mutability while a dataset-ex2 has flexible mutability. Polices \u00b6 Currently the user can define one of the following three types of mutability for their entities: Mutable: Disable ml-git cache (will slow down some operations). Files that were already added may be changed and added again. Flexible: Added files will be set to read-only after added to avoid any changes. If you want to change a file, you MUST use ml-git <ml-entity> unlock <file> . About unlock command: Decouple the file from ml-git cache to avoid propagating changes and creating \"corruptions\". Enable file write permission, so that you can edit the file. If you modify a file without previously executing the unlock command, the file will be considered corrupted. Strict: Added files will be set to read-only after added to avoid any changes. Once added, the files could not be modified in any other tag. Choosing the type of mutability \u00b6 The type of mutability must be chosen based on the characteristics of the entity you are working with. You must define carefully because once mutability is defined, it cannot be changed. If you are working with images , it is recommended that the type of mutability chosen is strict , since it is not common for images to be changed. Rather, new images are added to the set. In a scenario such as data augmentation, new images will be created from the originals, but the originals must remain intact. If you are working with shared cache we encourage to use mutability strict only. Take a look at the document about centralized cache. When dealing with files that must be modified over time, such as a csv file with your dataset labels, or your model file , we encourage you to use flexible or mutable . The choice will depend on how often you believe these files will be modified.","title":"Mutability Helper"},{"location":"mutability_helper/#mutability","text":"","title":"Mutability"},{"location":"mutability_helper/#what-is-the-mutability","text":"Mutability is the attribute that defines whether an entity's files can be changed by the user from one version to another. It is important to note that for all types of mutability the user is able to add and remove files, the mutability attribute refers to editing files already added. You must define carefully because once mutability is defined, it cannot be changed.","title":"What is the mutability?"},{"location":"mutability_helper/#where-to-define-mutability-policy","text":"Mutability is defined when creating a new entity. With the command ml-git (datasets|labels|models) create you must pass the mandatory attribute mutability to define the type of mutability for the created entity. Your entity specification file (.spec) should look like this: dataset: categories: - computer-vision - images mutability: flexible manifest: storage: s3h://mlgit-datasets name: imagenet8 version: 1 If you create an entity without using the create command and without mutability, when trying to perform the ml-git (datasets|labels|models) add the command will not be executed and you will be informed that you must define a mutability for that new entity. Note: For entities that were created before the mutability parameter became mandatory and that did not define mutability, ml-git treats these entities as strict. Because it is an attribute defined in the spec, you can define a type of mutability for each entity that the project has. For example, you can have in the same project a dataset-ex1 that has strict mutability while a dataset-ex2 has flexible mutability.","title":"Where to define mutability policy?"},{"location":"mutability_helper/#polices","text":"Currently the user can define one of the following three types of mutability for their entities: Mutable: Disable ml-git cache (will slow down some operations). Files that were already added may be changed and added again. Flexible: Added files will be set to read-only after added to avoid any changes. If you want to change a file, you MUST use ml-git <ml-entity> unlock <file> . About unlock command: Decouple the file from ml-git cache to avoid propagating changes and creating \"corruptions\". Enable file write permission, so that you can edit the file. If you modify a file without previously executing the unlock command, the file will be considered corrupted. Strict: Added files will be set to read-only after added to avoid any changes. Once added, the files could not be modified in any other tag.","title":"Polices"},{"location":"mutability_helper/#choosing-the-type-of-mutability","text":"The type of mutability must be chosen based on the characteristics of the entity you are working with. You must define carefully because once mutability is defined, it cannot be changed. If you are working with images , it is recommended that the type of mutability chosen is strict , since it is not common for images to be changed. Rather, new images are added to the set. In a scenario such as data augmentation, new images will be created from the originals, but the originals must remain intact. If you are working with shared cache we encourage to use mutability strict only. Take a look at the document about centralized cache. When dealing with files that must be modified over time, such as a csv file with your dataset labels, or your model file , we encourage you to use flexible or mutable . The choice will depend on how often you believe these files will be modified.","title":"Choosing the type of mutability"},{"location":"plugins/","text":"ML-Git Data Specialization Plugins \u00b6 Data specialization plugins are resources that can be added to ML-Git providing specific processing and metadata collection for specific data formats. This document aims to provide instructions on how data specialization plugins can be developed for ML-Git, defining interface methods that must be implemented to provide the necessary functionalities for processing these data. Plugin contracts \u00b6 add_metadata This method is responsible for processing or gathering information about the versioned data and inserting it into the specification file. If the plugin is installed and properly configured, this signature will be triggered before the metadata is committed. Definition: def add_metadata ( work_space_path , metadata ): \"\"\" Args: work_space_path (str): Absolute path where the files managed by ml-git will be used to generate extra information that can be inserted in metadata. metadata (dict): Content of spec file that can be changed to add extra data. \"\"\" compare_metadata This method is responsible for displaying a formatted output containing the comparison of the information that was added by the plugin in the specification file for each version of the entity. If the plugin is installed and configured correctly, this signature will be triggered during the execution of the ml-git log command. *Definition:* def compare_metadata ( specs_to_compare ): \"\"\" Args: specs_to_compare (Iterator[dict]): List containing current spec file and predecessors to be compared for each version. \"\"\" get_status_output Responsible for generating status outputs for files in the user's workspace. Returns two lists containing the formatted status output for untracked and added files and a summarized output string for the total added. This signature will be triggered during the execution of the ml-git status command. *Definition:* def get_status_output ( path , untracked_files , files_to_be_commited , full_option = False ): \"\"\" Args: path (str): The path where the data is in the user workspace. files_to_be_commited (list): The list of files to be commited in the user workspace. untracked_files (list): The list of untracked files in the user workspace. full_option (bool): Option to show the entire files or summarized by path. Returns: output_untracked_data (list): List of strings formatted with the number of rows for each untracked file. output_to_be_commited_data (list): List of strings formatted with the number of rows for each added file to be commited. output_total_rows (str): String formatted with the sum of the rows for each file to be commited. \"\"\" Note: The plugin doesn't need to implement all methods defined in the plugin contract. How to create a plugin \u00b6 When developing the plugin we recommend that the user follow the structure proposed below: ml-git-plugin-project-name/ tests/ core_tests.py package_name/ <-- name of your main package. __init__.py core.py <-- main module where the entry function is located. setup.py In package_name/core.py it is expected to contain only the contract methods essential to the operation of the plugin. # package_name/core.py def add_metadata ( work_space_path , metadata ): ... ... In package_name/__init__.py it's necessary to import the implemented contract's methods. As in the following example: # package_name/__init__.py from package_name.core import add_metadata The main purpose of the setup script is to describe your module distribution. # setup.py from setuptools import setup , find_packages setup ( name = 'ml-git-plugin-project-name' , version = '0.1' , license = 'GNU General Public License v2.0' , author = '' , description = '' , packages = find_packages (), platforms = 'Any' , zip_safe = True , ) How to install a plugin \u00b6 cd plugin-project-name pip3 install --user . For an entity of your preference, change the spec file like below: (ex: dataset/dataset-ex/dataset-ex.spec) dataset : categories : - computer-vision - images manifest : data-plugin : package_name <-- type here the package name in your plugin project. files : MANIFEST.yaml storage : s3h://mlgit mutability : strict name : dataset-ex version : 1","title":"Plugins"},{"location":"plugins/#ml-git-data-specialization-plugins","text":"Data specialization plugins are resources that can be added to ML-Git providing specific processing and metadata collection for specific data formats. This document aims to provide instructions on how data specialization plugins can be developed for ML-Git, defining interface methods that must be implemented to provide the necessary functionalities for processing these data.","title":"ML-Git Data Specialization Plugins"},{"location":"plugins/#plugin-contracts","text":"add_metadata This method is responsible for processing or gathering information about the versioned data and inserting it into the specification file. If the plugin is installed and properly configured, this signature will be triggered before the metadata is committed. Definition: def add_metadata ( work_space_path , metadata ): \"\"\" Args: work_space_path (str): Absolute path where the files managed by ml-git will be used to generate extra information that can be inserted in metadata. metadata (dict): Content of spec file that can be changed to add extra data. \"\"\" compare_metadata This method is responsible for displaying a formatted output containing the comparison of the information that was added by the plugin in the specification file for each version of the entity. If the plugin is installed and configured correctly, this signature will be triggered during the execution of the ml-git log command. *Definition:* def compare_metadata ( specs_to_compare ): \"\"\" Args: specs_to_compare (Iterator[dict]): List containing current spec file and predecessors to be compared for each version. \"\"\" get_status_output Responsible for generating status outputs for files in the user's workspace. Returns two lists containing the formatted status output for untracked and added files and a summarized output string for the total added. This signature will be triggered during the execution of the ml-git status command. *Definition:* def get_status_output ( path , untracked_files , files_to_be_commited , full_option = False ): \"\"\" Args: path (str): The path where the data is in the user workspace. files_to_be_commited (list): The list of files to be commited in the user workspace. untracked_files (list): The list of untracked files in the user workspace. full_option (bool): Option to show the entire files or summarized by path. Returns: output_untracked_data (list): List of strings formatted with the number of rows for each untracked file. output_to_be_commited_data (list): List of strings formatted with the number of rows for each added file to be commited. output_total_rows (str): String formatted with the sum of the rows for each file to be commited. \"\"\" Note: The plugin doesn't need to implement all methods defined in the plugin contract.","title":"Plugin contracts"},{"location":"plugins/#how-to-create-a-plugin","text":"When developing the plugin we recommend that the user follow the structure proposed below: ml-git-plugin-project-name/ tests/ core_tests.py package_name/ <-- name of your main package. __init__.py core.py <-- main module where the entry function is located. setup.py In package_name/core.py it is expected to contain only the contract methods essential to the operation of the plugin. # package_name/core.py def add_metadata ( work_space_path , metadata ): ... ... In package_name/__init__.py it's necessary to import the implemented contract's methods. As in the following example: # package_name/__init__.py from package_name.core import add_metadata The main purpose of the setup script is to describe your module distribution. # setup.py from setuptools import setup , find_packages setup ( name = 'ml-git-plugin-project-name' , version = '0.1' , license = 'GNU General Public License v2.0' , author = '' , description = '' , packages = find_packages (), platforms = 'Any' , zip_safe = True , )","title":"How to create a plugin"},{"location":"plugins/#how-to-install-a-plugin","text":"cd plugin-project-name pip3 install --user . For an entity of your preference, change the spec file like below: (ex: dataset/dataset-ex/dataset-ex.spec) dataset : categories : - computer-vision - images manifest : data-plugin : package_name <-- type here the package name in your plugin project. files : MANIFEST.yaml storage : s3h://mlgit mutability : strict name : dataset-ex version : 1","title":"How to install a plugin"},{"location":"qs_checkout/","text":"Downloading a dataset from a configured repository \u00b6 To download a dataset, you need to be in an initialized and configured ML-Git project. If you have a repository with your saved settings, you can run the following command to set up your environment: ml-git clone git@github.com:example/your-mlgit-repository.git Then, go to the project folder: cd your-mlgit-repository Now you can retrieve a specific version of a dataset to run an experiment. To achieve that, you can use the version tag to download this version to your local environment using one of the following commands: ml-git datasets checkout computer-vision__images__faces__fddb__1 or ml-git datasets checkout fddb --version=1 If you want to get the latest available version of a dataset, you can pass its name in the checkout command, as shown below: ml-git datasets checkout fddb Then, your directory should look like this: computer-vision/ \u2514\u2500\u2500 images \u2514\u2500\u2500 faces \u2514\u2500\u2500 fddb \u251c\u2500\u2500 README.md \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 2002 \u2502 \u2514\u2500\u2500 2003 \u2514\u2500\u2500 fddb.spec","title":"Qs checkout"},{"location":"qs_checkout/#downloading-a-dataset-from-a-configured-repository","text":"To download a dataset, you need to be in an initialized and configured ML-Git project. If you have a repository with your saved settings, you can run the following command to set up your environment: ml-git clone git@github.com:example/your-mlgit-repository.git Then, go to the project folder: cd your-mlgit-repository Now you can retrieve a specific version of a dataset to run an experiment. To achieve that, you can use the version tag to download this version to your local environment using one of the following commands: ml-git datasets checkout computer-vision__images__faces__fddb__1 or ml-git datasets checkout fddb --version=1 If you want to get the latest available version of a dataset, you can pass its name in the checkout command, as shown below: ml-git datasets checkout fddb Then, your directory should look like this: computer-vision/ \u2514\u2500\u2500 images \u2514\u2500\u2500 faces \u2514\u2500\u2500 fddb \u251c\u2500\u2500 README.md \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 2002 \u2502 \u2514\u2500\u2500 2003 \u2514\u2500\u2500 fddb.spec","title":" Downloading a dataset from a configured repository "},{"location":"qs_configure_repository/","text":"Creating a configured repository \u00b6 It's recommended to version, in a git repository, the .ml-git folder containing the settings you frequently use. This way, you will be able to use it in future projects or share it with another ML-Git user if you want. To create the .ml-git folder that will be versioned, the following commands are necessary: Initialize the ML-Git project. ml-git repository init Configure remotes for the entities that will be used. ml-git datasets remote add git@github.com:example/your-mlgit-datasets.git Configure the storages which will be used. ml-git repository storage add mlgit-datasets --credentials=mlgit --endpoint-url=<minio-endpoint-url> After that, you should version, in a git repository, the .ml-git folder created during this process. To use these settings in a new project, all you have to do is execute the command ml-git clone to import the project's settings. NOTE : If you would like to share these settings with another ML-Git user, this user must have access to the git repository where the settings are stored.","title":"Qs configure repository"},{"location":"qs_configure_repository/#creating-a-configured-repository","text":"It's recommended to version, in a git repository, the .ml-git folder containing the settings you frequently use. This way, you will be able to use it in future projects or share it with another ML-Git user if you want. To create the .ml-git folder that will be versioned, the following commands are necessary: Initialize the ML-Git project. ml-git repository init Configure remotes for the entities that will be used. ml-git datasets remote add git@github.com:example/your-mlgit-datasets.git Configure the storages which will be used. ml-git repository storage add mlgit-datasets --credentials=mlgit --endpoint-url=<minio-endpoint-url> After that, you should version, in a git repository, the .ml-git folder created during this process. To use these settings in a new project, all you have to do is execute the command ml-git clone to import the project's settings. NOTE : If you would like to share these settings with another ML-Git user, this user must have access to the git repository where the settings are stored.","title":" Creating a configured repository"},{"location":"quick_start/","text":"Quick start \u00b6 In this document we describe all steps necessary to execute the following basic tasks with ML-Git: Downloading a dataset from a configured repository Having a repository that stores the settings used by ML-Git, learn how to download a dataset. Creating a configured repository Learn how to create a repository that stores the settings used by ML-Git.","title":"Quick start #"},{"location":"quick_start/#quick-start","text":"In this document we describe all steps necessary to execute the following basic tasks with ML-Git: Downloading a dataset from a configured repository Having a repository that stores the settings used by ML-Git, learn how to download a dataset. Creating a configured repository Learn how to create a repository that stores the settings used by ML-Git.","title":"Quick start"},{"location":"resources_initialization/","text":"Resource initialization script \u00b6 About \u00b6 As mentioned in ML-Git internals , the design concept about ML-Git is to decouple the ML entities metadata management from the actual data, such that there are two main layers in the tool: The metadata management: There are for each ML entities managed under ml-git, the user needs to define a small specification file. These files are then managed by a git repository to retrieve the different versions. The data store management: To store data from managed artifacts. This script aims to facilitate the creation of resources (buckets and repositories) that are needed to use ML-Git. Prerequisites \u00b6 To use this script, you must have configured it in your environment: Github Access Token: You must create a personal access token to use instead of a password with a command line or with an API. See github documentation to learn how to configure a token. Note: As this script uses the github API, it is necessary that you store the token in GITHUB_TOKEN environment variable. If you are setting up a bucket of S3 type: AWS CLI : The AWS Command Line Interface (CLI) is a unified tool for managing your AWS services. If you are setting up a bucket of azure type: Azure CLI : The Azure command-line interface (Azure CLI) is a set of commands used to create and manage Azure resources. If you are setting up a bucket of MinIO type: MinIO : In addition to having MinIO configured and running, you will also need the AWS Command Line Interface (CLI) to perform with it. How to use \u00b6 Once all the necessary requirements for the settings you want to make are installed, just run the command: Linux: Execute on terminal: cd ml-git ./scripts/resources_initialization/resources_initialization.sh Windows: Execute on Powershell or CMD: cd ml-git .\\ scripts \\ resources_initialization \\ resources_initialization . bat At the end of the script execution, the user must have configured the repositories to store the metadata, a repository available to perform the ml-git clone command and import these settings, in addition to having instantiated the buckets in the chosen services.","title":"Resources Initialization"},{"location":"resources_initialization/#resource-initialization-script","text":"","title":"Resource initialization script"},{"location":"resources_initialization/#about","text":"As mentioned in ML-Git internals , the design concept about ML-Git is to decouple the ML entities metadata management from the actual data, such that there are two main layers in the tool: The metadata management: There are for each ML entities managed under ml-git, the user needs to define a small specification file. These files are then managed by a git repository to retrieve the different versions. The data store management: To store data from managed artifacts. This script aims to facilitate the creation of resources (buckets and repositories) that are needed to use ML-Git.","title":"About"},{"location":"resources_initialization/#prerequisites","text":"To use this script, you must have configured it in your environment: Github Access Token: You must create a personal access token to use instead of a password with a command line or with an API. See github documentation to learn how to configure a token. Note: As this script uses the github API, it is necessary that you store the token in GITHUB_TOKEN environment variable. If you are setting up a bucket of S3 type: AWS CLI : The AWS Command Line Interface (CLI) is a unified tool for managing your AWS services. If you are setting up a bucket of azure type: Azure CLI : The Azure command-line interface (Azure CLI) is a set of commands used to create and manage Azure resources. If you are setting up a bucket of MinIO type: MinIO : In addition to having MinIO configured and running, you will also need the AWS Command Line Interface (CLI) to perform with it.","title":"Prerequisites"},{"location":"resources_initialization/#how-to-use","text":"Once all the necessary requirements for the settings you want to make are installed, just run the command: Linux: Execute on terminal: cd ml-git ./scripts/resources_initialization/resources_initialization.sh Windows: Execute on Powershell or CMD: cd ml-git .\\ scripts \\ resources_initialization \\ resources_initialization . bat At the end of the script execution, the user must have configured the repositories to store the metadata, a repository available to perform the ml-git clone command and import these settings, in addition to having instantiated the buckets in the chosen services.","title":"How to use"},{"location":"sftp_configurations/","text":"SFTP bucket configuration \u00b6 This section explains how to configure the settings that the ml-git uses to interact with your bucket using the SFTP storage. This requires you to have configured a public key in your SFTP server and use the private key pair to connect through ml-git. Setting up a ML-Git project with SFTP \u00b6 Add store configurations example: ml-git repository storage add path-in-your-sftp-server --type=sftph --username=your-user-name --endpoint-url=your-host --private-key=/home/profile/your_private_key After that initialize the metadata repository: ml-git datasets init","title":"SFTP bucket configuration #"},{"location":"sftp_configurations/#sftp-bucket-configuration","text":"This section explains how to configure the settings that the ml-git uses to interact with your bucket using the SFTP storage. This requires you to have configured a public key in your SFTP server and use the private key pair to connect through ml-git.","title":"SFTP bucket configuration"},{"location":"sftp_configurations/#setting-up-a-ml-git-project-with-sftp","text":"Add store configurations example: ml-git repository storage add path-in-your-sftp-server --type=sftph --username=your-user-name --endpoint-url=your-host --private-key=/home/profile/your_private_key After that initialize the metadata repository: ml-git datasets init","title":"Setting up a ML-Git project with SFTP"},{"location":"shell_completion_guide/","text":"ML-Git Shell Completion Support \u00b6 The Shell completion is a function that allows you to autocomplete your ml-git commands by partially typing the commands or options, then pressing the [Tab] key. This will help you when writing the command in the terminal. The shell completion support will complete commands and options. Options are only listed if at least a dash has been entered. In order to activate shell completion, you need to inform your shell that completion is available for the ML-Git. For this purpose, we provide the necessary modifications in the script for each type of terminal that is supported by the autocomplete functionality: - Bash - Fish - Windows PowerShell - Zsh Note : If you have the shell open before making the modification, you will need to restart it after modifying the script. For Bash, add this to ~/.bash_completion: \u00b6 _ml-git_completion() { local IFS=$'\\t' COMPREPLY=( $( env COMP_WORDS=\"${COMP_WORDS[*]}\" \\ COMP_CWORD=$COMP_CWORD \\ _ML_GIT_COMPLETE=complete-bash $1 ) ) return 0 } complete -F _ml-git_completion -o default ml-git For Fish, create the file ~/.config/fish/completions/ml-git.fish and add: \u00b6 complete --command ml-git --arguments \"(env _ML_GIT_COMPLETE=complete-fish COMMANDLINE=(commandline -cp) ml-git)\" -f For Windows PowerShell, add this to PowerShell Profile file*: \u00b6 if ((Test-Path Function:\\TabExpansion) -and -not (Test-Path Function:\\ml-gitTabExpansionBackup)) { Rename-Item Function:\\TabExpansion ml-gitTabExpansionBackup } function TabExpansion($line, $lastWord) { $lastBlock = [regex]::Split($line, '[|;]')[-1].TrimStart() $aliases = @(\"ml-git\") + @(Get-Alias | where { $_.Definition -eq \"ml-git\" } | select -Exp Name) $aliasPattern = \"($($aliases -join '|'))\" if($lastBlock -match \"^$aliasPattern \") { $Env:_ML_GIT_COMPLETE = \"complete-powershell\" $Env:COMMANDLINE = \"$lastBlock\" (ml-git) | ? {$_.trim() -ne \"\" } Remove-Item Env:_ML_GIT_COMPLETE Remove-Item Env:COMMANDLINE } elseif (Test-Path Function:\\ml-gitTabExpansionBackup) { # Fall back on existing tab expansion ml-gitTabExpansionBackup $line $lastWord } } *To find out where the file for your PowerShell Profile is located, you can run $profile in Windows Powershell. If you don't have such a file yet, follow the steps described in this link (How to create a profile) to create a new one. For Zsh, add this to ~/.zshrc: \u00b6 #compdef ml-git _ml-git() { eval $(env COMMANDLINE=\"${words[1,$CURRENT]}\" _ML_GIT_COMPLETE=complete-zsh ml-git) } if [[ \"$(basename -- ${(%):-%x})\" != \"_ml-git\" ]]; then compdef _ml-git ml-git fi","title":"ML-Git Shell Completion Support #"},{"location":"shell_completion_guide/#ml-git-shell-completion-support","text":"The Shell completion is a function that allows you to autocomplete your ml-git commands by partially typing the commands or options, then pressing the [Tab] key. This will help you when writing the command in the terminal. The shell completion support will complete commands and options. Options are only listed if at least a dash has been entered. In order to activate shell completion, you need to inform your shell that completion is available for the ML-Git. For this purpose, we provide the necessary modifications in the script for each type of terminal that is supported by the autocomplete functionality: - Bash - Fish - Windows PowerShell - Zsh Note : If you have the shell open before making the modification, you will need to restart it after modifying the script.","title":"ML-Git Shell Completion Support"},{"location":"shell_completion_guide/#for-bash-add-this-to-bash_completion","text":"_ml-git_completion() { local IFS=$'\\t' COMPREPLY=( $( env COMP_WORDS=\"${COMP_WORDS[*]}\" \\ COMP_CWORD=$COMP_CWORD \\ _ML_GIT_COMPLETE=complete-bash $1 ) ) return 0 } complete -F _ml-git_completion -o default ml-git","title":"For Bash, add this to ~/.bash_completion:"},{"location":"shell_completion_guide/#for-fish-create-the-file-configfishcompletionsml-gitfish-and-add","text":"complete --command ml-git --arguments \"(env _ML_GIT_COMPLETE=complete-fish COMMANDLINE=(commandline -cp) ml-git)\" -f","title":"For Fish, create the file ~/.config/fish/completions/ml-git.fish and add:"},{"location":"shell_completion_guide/#for-windows-powershell-add-this-to-powershell-profile-file","text":"if ((Test-Path Function:\\TabExpansion) -and -not (Test-Path Function:\\ml-gitTabExpansionBackup)) { Rename-Item Function:\\TabExpansion ml-gitTabExpansionBackup } function TabExpansion($line, $lastWord) { $lastBlock = [regex]::Split($line, '[|;]')[-1].TrimStart() $aliases = @(\"ml-git\") + @(Get-Alias | where { $_.Definition -eq \"ml-git\" } | select -Exp Name) $aliasPattern = \"($($aliases -join '|'))\" if($lastBlock -match \"^$aliasPattern \") { $Env:_ML_GIT_COMPLETE = \"complete-powershell\" $Env:COMMANDLINE = \"$lastBlock\" (ml-git) | ? {$_.trim() -ne \"\" } Remove-Item Env:_ML_GIT_COMPLETE Remove-Item Env:COMMANDLINE } elseif (Test-Path Function:\\ml-gitTabExpansionBackup) { # Fall back on existing tab expansion ml-gitTabExpansionBackup $line $lastWord } } *To find out where the file for your PowerShell Profile is located, you can run $profile in Windows Powershell. If you don't have such a file yet, follow the steps described in this link (How to create a profile) to create a new one.","title":"For Windows PowerShell, add this to PowerShell Profile file*:"},{"location":"shell_completion_guide/#for-zsh-add-this-to-zshrc","text":"#compdef ml-git _ml-git() { eval $(env COMMANDLINE=\"${words[1,$CURRENT]}\" _ML_GIT_COMPLETE=complete-zsh ml-git) } if [[ \"$(basename -- ${(%):-%x})\" != \"_ml-git\" ]]; then compdef _ml-git ml-git fi","title":"For Zsh, add this to ~/.zshrc:"},{"location":"step_by_step_guides/","text":"ML-Git Step-by-Step Guide \u00b6 About \u00b6 In order to facilitate the learning process of using the ML-Git API, we offer a series of step-by-step guides that emulate real situations in the learning context by applying the correct API command sequences that should be used in each scenario. The guides were created using Jupyter notebooks, as they offer the possibility to run, step-by-step, code snippets in a format similar to an explanatory document, thus becoming ideal for teaching new users how to use the API in real case scenarios. How execute notebooks: \u00b6 To run notebooks more easily, a docker environment has been created that performs all the environment settings required by the user. So make sure that the procedures in the local environment configuration section have been performed. Summary of existing notebooks: \u00b6 basic_flow This notebook describes a basic execution flow of ML-Git with its API. GitHub link clone_repository This notebook describes how to clone an ML-Git repository. GitHub link multiple_projects This notebook describes how to work with multiple projects in the ML-Git API. GitHub link relationship_api_commands This notebook describes a basic flow in the context of relationships between entities with the API provided by ML-Git. GitHub link mnist_random_forest_api This notebook describes a basic execution flow of ML-Git with its API using the MNIST dataset. GitHub link mnist_random_forest_cli This notebook describes a basic execution flow of ML-Git with its CLI using the MNIST dataset. GitHub link checkout_with_sample This notebook describes how to perform a checkout operation with ML-Git using samples of a dataset. GitHub link multiple_datasets This notebook describes how to handle the scenario where the same file is present in more than one dataset. GitHub link","title":"Summary"},{"location":"step_by_step_guides/#ml-git-step-by-step-guide","text":"","title":"ML-Git Step-by-Step Guide"},{"location":"step_by_step_guides/#about","text":"In order to facilitate the learning process of using the ML-Git API, we offer a series of step-by-step guides that emulate real situations in the learning context by applying the correct API command sequences that should be used in each scenario. The guides were created using Jupyter notebooks, as they offer the possibility to run, step-by-step, code snippets in a format similar to an explanatory document, thus becoming ideal for teaching new users how to use the API in real case scenarios.","title":"About"},{"location":"step_by_step_guides/#how-execute-notebooks","text":"To run notebooks more easily, a docker environment has been created that performs all the environment settings required by the user. So make sure that the procedures in the local environment configuration section have been performed.","title":"How execute notebooks:"},{"location":"step_by_step_guides/#summary-of-existing-notebooks","text":"basic_flow This notebook describes a basic execution flow of ML-Git with its API. GitHub link clone_repository This notebook describes how to clone an ML-Git repository. GitHub link multiple_projects This notebook describes how to work with multiple projects in the ML-Git API. GitHub link relationship_api_commands This notebook describes a basic flow in the context of relationships between entities with the API provided by ML-Git. GitHub link mnist_random_forest_api This notebook describes a basic execution flow of ML-Git with its API using the MNIST dataset. GitHub link mnist_random_forest_cli This notebook describes a basic execution flow of ML-Git with its CLI using the MNIST dataset. GitHub link checkout_with_sample This notebook describes how to perform a checkout operation with ML-Git using samples of a dataset. GitHub link multiple_datasets This notebook describes how to handle the scenario where the same file is present in more than one dataset. GitHub link","title":"Summary of existing notebooks:"},{"location":"storage_configurations/","text":"Storage configurations \u00b6 Currently, ML-Git supports five types of storage (S3, MinIO, Azure, GoogleDrive and SFTP). You can find files describing how to configure each of these types of storage below: MinIO S3 Azure Google Drive SFTP","title":"Configurations"},{"location":"storage_configurations/#storage-configurations","text":"Currently, ML-Git supports five types of storage (S3, MinIO, Azure, GoogleDrive and SFTP). You can find files describing how to configure each of these types of storage below: MinIO S3 Azure Google Drive SFTP","title":"Storage configurations"},{"location":"api/","text":"ML-Git API \u00b6 The ML-Git API offers the developer the possibility to work with ML-Git programmatically by using the MLGitAPI class. Methods available in the MLGitAPI class \u00b6 add def add ( self , entity_type , entity_name , bumpversion = False , fsck = False , file_path = None , metric = None , metrics_file = '' ): \"\"\"This command will add all the files under the directory into the ml-git index/staging area. Example: api = MLGitApi() api.add('datasets', 'dataset-ex', bumpversion=True) Args: entity_type (str): The type of an ML entity (datasets, labels or models). entity_name (str): The name of the ML entity you want to add the files. bumpversion (bool, optional): Increment the entity version number when adding more files [default: False]. fsck (bool, optional): Run fsck after command execution [default: False]. file_path (list, optional): List of files that must be added by the command [default: all files]. metric (dictionary, optional): The metric dictionary, example: { 'metric': value } [default: empty]. metrics_file (str, optional): The metrics file path. It is expected a CSV file containing the metric names in the header and the values in the next line [default: empty]. \"\"\" checkout def checkout ( self , entity , tag , sampling = None , retries = 2 , force = False , dataset = False , labels = False , version =- 1 , fail_limit = None , full = False ): \"\"\"This command allows retrieving the data of a specific version of an ML entity. Example: api = MLGitApi() api.checkout('datasets', 'computer-vision__images3__imagenet__1') Args: entity (str): The type of an ML entity (datasets, labels or models). tag (str): An ml-git tag to identify a specific version of an ML entity. sampling (dict): group: <amount>:<group> The group sample option consists of amount and group used to download a sample.\\n range: <start:stop:step> The range sample option consists of start, stop and step used to download a sample. The start parameter can be equal or greater than zero. The stop parameter can be 'all', -1 or any integer above zero.\\n random: <amount:frequency> The random sample option consists of amount and frequency used to download a sample. seed: The seed is used to initialize the pseudorandom numbers. retries (int, optional): Number of retries to download the files from the storage [default: 2]. force (bool, optional): Force checkout command to delete untracked/uncommitted files from the local repository [default: False]. dataset (bool, optional): If exist a dataset related with the model or labels, this one must be downloaded [default: False]. labels (bool, optional): If exist labels related with the model, they must be downloaded [default: False]. version (int, optional): The entity version [default: -1]. fail_limit (int, optional): Number of failures before aborting the command [default: no limit]. full (bool, optional): Show all contents for each directory. [default: False]. Returns: str: Return the path where the data was checked out. \"\"\" clone def clone ( self , repository_url , untracked = False ): \"\"\"This command will clone minimal configuration files from repository-url with valid .ml-git/config.yaml, then initialize the metadata according to configurations. Example: api = MLGitApi() api.clone('https://git@github.com/mlgit-repository') Args: repository_url (str): The git repository that will be cloned. untracked (bool, optional): Set whether cloned repository trace should not be kept [default: False]. \"\"\" commit def commit ( self , entity , ml_entity_name , commit_message = None , related_dataset = None , related_labels = None ): \"\"\"That command commits the index / staging area to the local repository. Example: api = MLGitApi() api.commit('datasets', 'dataset-ex') Args: entity (str): The type of an ML entity (datasets, labels or models). ml_entity_name (str): Artefact name to commit. commit_message (str, optional): Message of commit. related_dataset (str, optional): Artefact name of dataset related to commit. related_labels (str, optional): Artefact name of labels related to commit. \"\"\" create def create ( self , entity , entity_name , categories , mutability , ** kwargs ): \"\"\"This command will create the workspace structure with data and spec file for an entity and set the storage configurations. Example: api = MLGitApi()\\n api.create('datasets', 'dataset-ex', categories=['computer-vision', 'images'], mutability='strict') Args: entity (str): The type of an ML entity (datasets, labels or models). entity_name (str): An ml-git entity name to identify a ML entity. categories (list): Artifact's categories name. mutability (str): Mutability type. The mutability options are strict, flexible and mutable. storage_type (str, optional): Data storage type [default: s3h]. version (int, optional): Number of artifact version [default: 1]. import_path (str, optional): Path to be imported to the project. bucket_name (str, optional): Bucket name. import_url (str, optional): Import data from a google drive url. credentials_path (str, optional): Directory of credentials.json. unzip (bool, optional): Unzip imported zipped files [default: False]. entity_dir (str, optional): The relative path where the entity will be created inside the ml entity directory [default: empty]. \"\"\" init def init ( self , entity ): \"\"\"This command will start the ml-git entity. Examples: api = MLGitApi()\\n api.init('repository')\\n api.init('datasets') Args: entity (str): The type of an ML entity (datasets, labels or models). \"\"\" models metrics def get_models_metrics ( self , entity_name , export_path = None , export_type = FileType . JSON . value ): \"\"\"Get metrics information for each tag of the entity. Examples: api = MLGitApi()\\n api.get_models_metrics('model-ex', export_type='csv') Args: entity_name (str): An ml-git entity name to identify a ML entity. export_path(str, optional): Set the path to export metrics to a file. export_type (str, optional): Choose the format of the file that will be generated with the metrics [default: json]. \"\"\" push def push ( self , entity , entity_name , retries = 2 , clear_on_fail = False , fail_limit = None ): \"\"\"This command allows pushing the data of a specific version of an ML entity. Example: api = MLGitApi()\\n api.push('datasets', 'dataset-ex') Args: entity (str): The type of an ML entity. (datasets, labels or models) entity_name (str): An ml-git entity name to identify a ML entity. retries (int, optional): Number of retries to upload the files to the storage [default: 2]. clear_on_fail (bool, optional): Remove the files from the storage in case of failure during the push operation [default: False]. fail_limit (int, optional): Number of failures before aborting the command [default: no limit]. \"\"\" remote add def remote_add ( self , entity , remote_url , global_configuration = False ): \"\"\"This command will add a remote to store the metadata from this ml-git project. Examples: api = MLGitApi()\\n api.remote_add('datasets', 'https://git@github.com/mlgit-datasets') Args: entity (str): The type of an ML entity (datasets, labels or models). remote_url(str): URL of an existing remote git repository. global_configuration (bool, optional): Use this option to set configuration at global level [default: False]. \"\"\" storage add def storage_add ( self , bucket_name , bucket_type = StorageType . S3H . value , credentials = None , global_configuration = False , endpoint_url = None , username = None , private_key = None , port = 22 , region = None ): \"\"\"This command will add a storage to the ml-git project. Examples: api = MLGitApi()\\n api.storage_add('my-bucket', bucket_type='s3h') Args: bucket_name (str): The name of the bucket in the storage. bucket_type (str, optional): Storage type (s3h, azureblobh or gdriveh) [default: s3h]. credentials (str, optional): Name of the profile that stores the credentials or the path to the credentials. global_configuration (bool, optional): Use this option to set configuration at global level [default: False]. endpoint_url (str, optional): Storage endpoint url. username (str, optional): The username for the sftp login. private_key (str, optional): Full path for the private key file. port (int, optional): The port to be used when connecting to the storage. region (str, optional): AWS region for S3 bucket. \"\"\" init entity manager def init_entity_manager ( github_token , url ): \"\"\"Initialize an entity manager to operate over github API. Examples: init_entity_manager('github_token', 'https://api.github.com') Args: github_token (str): The personal access github token. url (str): The github api url. Returns: object of class EntityManager. \"\"\" init local entity manager def init_local_entity_manager (): \"\"\"Initialize an entity manager to operate over local git repository. Returns: object of class LocalEntityManager. \"\"\" Classes used by the API. \u00b6 Some methods available in the API use the classes described below: EntityManager class EntityManager : \"\"\"Class that operate over github api to manage entity's operations\"\"\" def get_entities ( self , config_path = None , config_repo_name = None ): \"\"\"Get a list of entities found in config.yaml. Args: config_path (str): The absolute path of the config.yaml file. config_repo_name (str): The repository name where is the config.yaml located in github. Returns: list of class Entity. \"\"\" def get_entity_versions ( self , name , metadata_repo_name ): \"\"\"Get a list of spec versions found for an especific entity. Args: name (str): The name of the entity you want to get the versions. metadata_repo_name (str): The repository name where the entity metadata is located in GitHub. Returns: list of class SpecVersion. \"\"\" def get_linked_entities ( self , name , version , metadata_repo_name ): \"\"\"Get a list of linked entities found for an entity version. Args: name (str): The name of the entity you want to get the linked entities. version (str): The version of the entity you want to get the linked entities. metadata_repo_name (str): The repository name where the entity metadata is located in GitHub. Returns: list of LinkedEntity. \"\"\" def get_entity_relationships ( self , name , metadata_repo_name , export_type = FileType . JSON . value , export_path = None ): \"\"\"Get a list of relationships for an entity. Args: name (str): The name of the entity you want to get the linked entities. metadata_repo_name (str): The repository name where the entity metadata is located in GitHub. export_type (str): Set the format of the return (json, csv, dot) [default: json]. export_path (str): Set the path to export metrics to a file. Returns: list of EntityVersionRelationships. \"\"\" def get_project_entities_relationships ( self , config_repo_name , export_type = FileType . JSON . value , export_path = None ): \"\"\"Get a list of relationships for all project entities. Args: config_repo_name (str): The repository name where the config file is located in GitHub. export_type (str): Set the format of the return (json, csv, dot) [default: json]. export_path (str): Set the path to export metrics to a file. Returns: list of EntityVersionRelationships. \"\"\" LocalEntityManager class LocalEntityManager : \"\"\"Class that operate over local git repository to manage entity's operations\"\"\" def get_entities ( self ): \"\"\"Get a list of entities found in config.yaml. Returns: list of class Entity. \"\"\" def get_entity_versions ( self , name , type_entity ): \"\"\"Get a list of spec versions found for an especific entity. Args: name (str): The name of the entity you want to get the versions. type_entity (str): The type of the ml-entity (datasets, models, labels). Returns: list of class SpecVersion. \"\"\" def get_linked_entities ( self , name , version , type_entity ): \"\"\"Get a list of linked entities found for an entity version. Args: name (str): The name of the entity you want to get the linked entities. version (str): The version of the entity you want to get the linked entities. type_entity (str): The type of the ml-entity (datasets, models, labels). Returns: list of LinkedEntity. \"\"\" def get_entity_relationships ( self , name , type_entity , export_type = FileType . JSON . value , export_path = None ): \"\"\"Get a list of relationships for an entity. Args: name (str): The name of the entity you want to get the linked entities. type_entity (str): The type of the ml-entity (datasets, models, labels). export_type (str): Set the format of the return (json, csv, dot) [default: json]. export_path (str): Set the path to export metrics to a file. Returns: list of EntityVersionRelationships. \"\"\" def get_project_entities_relationships ( self , export_type = FileType . JSON . value , export_path = None ): \"\"\"Get a list of relationships for all project entities. Args: export_type (str): Set the format of the return [default: json]. export_path (str): Set the path to export metrics to a file. Returns: list of EntityVersionRelationships. \"\"\" def export_graph ( self , dot_graph , export_path ): \"\"\"Creates a graph of all entity relations as an HTML file. Args: dot_graph (str): String of graph in DOT language format. export_path (str): Set the path to export the HTML with the graph. [default: project root path] Returns: Path of HTML file. \"\"\" Entity class Entity : \"\"\"Class that represents an ml-entity. Attributes: name (str): The name of the entity. type (str): The type of the ml-entity (datasets, models, labels). private (str): The access of entity metadata. metadata (Metadata): The metadata of the entity. last_spec_version (SpecVersion): The specification file of the entity last version. \"\"\" SpecVersion class SpecVersion : \"\"\"Class that represents an ml-entity spec version. Attributes: name (str): The name of the entity. type (str): The type of the ml-entity (datasets, models, labels). version (str): The version of the ml-entity. tag (str): The tag of the ml-entity spec version. mutability (str): The mutability of the ml-entity. categories (list): Labels to categorize the entity. storage (Storage): The storage of the ml-entity. total_versioned_files (int): The amount of versioned files. size (str): The size of the version files. \"\"\" Metadata class Metadata : \"\"\"Class that represents an ml-entity metadata. Attributes: full_name (str): The full name of the metadata. git_url (str): The git url of the metadata. html_url (str): The html url of the metadata. owner_email (str): The owner email of the ml-entity metadata. owner_name (str): The owner name of the ml-entity metadata. \"\"\" Storage class Storage : \"\"\"Class that represents an ml-entity storage. Attributes: type (str): The storage type (s3h|azureblobh|gdriveh|sftph). bucket (str): The name of the bucket. \"\"\" EntityVersionRelationships class EntityVersionRelationships : \"\"\"Class that represents the relationships of an ml-entity in a specified version. Attributes: version (str): The version of the ml-entity. tag (str): The tag of the ml-entity. relationships (list): List of linked entities of the ml-entity in the specified version. \"\"\" LinkedEntity class LinkedEntity : \"\"\"Class that represents a linked ml-entity. Attributes: name (str): The name of the entity. type (str): The type of the ml-entity (datasets, models, labels). version (str): The version of the ml-entity. tag (str): The tag of the ml-entity spec version. \"\"\" API notebooks \u00b6 In the api_scripts directory, you can find notebooks running the ML-Git API for some scenarios. To run them, you need to boot the jupyter notebook server in an environment with ML-Git installed and navigate to the notebook of your choice.","title":"Methods"},{"location":"api/#ml-git-api","text":"The ML-Git API offers the developer the possibility to work with ML-Git programmatically by using the MLGitAPI class.","title":"ML-Git API"},{"location":"api/#methods-available-in-the-mlgitapi-class","text":"add def add ( self , entity_type , entity_name , bumpversion = False , fsck = False , file_path = None , metric = None , metrics_file = '' ): \"\"\"This command will add all the files under the directory into the ml-git index/staging area. Example: api = MLGitApi() api.add('datasets', 'dataset-ex', bumpversion=True) Args: entity_type (str): The type of an ML entity (datasets, labels or models). entity_name (str): The name of the ML entity you want to add the files. bumpversion (bool, optional): Increment the entity version number when adding more files [default: False]. fsck (bool, optional): Run fsck after command execution [default: False]. file_path (list, optional): List of files that must be added by the command [default: all files]. metric (dictionary, optional): The metric dictionary, example: { 'metric': value } [default: empty]. metrics_file (str, optional): The metrics file path. It is expected a CSV file containing the metric names in the header and the values in the next line [default: empty]. \"\"\" checkout def checkout ( self , entity , tag , sampling = None , retries = 2 , force = False , dataset = False , labels = False , version =- 1 , fail_limit = None , full = False ): \"\"\"This command allows retrieving the data of a specific version of an ML entity. Example: api = MLGitApi() api.checkout('datasets', 'computer-vision__images3__imagenet__1') Args: entity (str): The type of an ML entity (datasets, labels or models). tag (str): An ml-git tag to identify a specific version of an ML entity. sampling (dict): group: <amount>:<group> The group sample option consists of amount and group used to download a sample.\\n range: <start:stop:step> The range sample option consists of start, stop and step used to download a sample. The start parameter can be equal or greater than zero. The stop parameter can be 'all', -1 or any integer above zero.\\n random: <amount:frequency> The random sample option consists of amount and frequency used to download a sample. seed: The seed is used to initialize the pseudorandom numbers. retries (int, optional): Number of retries to download the files from the storage [default: 2]. force (bool, optional): Force checkout command to delete untracked/uncommitted files from the local repository [default: False]. dataset (bool, optional): If exist a dataset related with the model or labels, this one must be downloaded [default: False]. labels (bool, optional): If exist labels related with the model, they must be downloaded [default: False]. version (int, optional): The entity version [default: -1]. fail_limit (int, optional): Number of failures before aborting the command [default: no limit]. full (bool, optional): Show all contents for each directory. [default: False]. Returns: str: Return the path where the data was checked out. \"\"\" clone def clone ( self , repository_url , untracked = False ): \"\"\"This command will clone minimal configuration files from repository-url with valid .ml-git/config.yaml, then initialize the metadata according to configurations. Example: api = MLGitApi() api.clone('https://git@github.com/mlgit-repository') Args: repository_url (str): The git repository that will be cloned. untracked (bool, optional): Set whether cloned repository trace should not be kept [default: False]. \"\"\" commit def commit ( self , entity , ml_entity_name , commit_message = None , related_dataset = None , related_labels = None ): \"\"\"That command commits the index / staging area to the local repository. Example: api = MLGitApi() api.commit('datasets', 'dataset-ex') Args: entity (str): The type of an ML entity (datasets, labels or models). ml_entity_name (str): Artefact name to commit. commit_message (str, optional): Message of commit. related_dataset (str, optional): Artefact name of dataset related to commit. related_labels (str, optional): Artefact name of labels related to commit. \"\"\" create def create ( self , entity , entity_name , categories , mutability , ** kwargs ): \"\"\"This command will create the workspace structure with data and spec file for an entity and set the storage configurations. Example: api = MLGitApi()\\n api.create('datasets', 'dataset-ex', categories=['computer-vision', 'images'], mutability='strict') Args: entity (str): The type of an ML entity (datasets, labels or models). entity_name (str): An ml-git entity name to identify a ML entity. categories (list): Artifact's categories name. mutability (str): Mutability type. The mutability options are strict, flexible and mutable. storage_type (str, optional): Data storage type [default: s3h]. version (int, optional): Number of artifact version [default: 1]. import_path (str, optional): Path to be imported to the project. bucket_name (str, optional): Bucket name. import_url (str, optional): Import data from a google drive url. credentials_path (str, optional): Directory of credentials.json. unzip (bool, optional): Unzip imported zipped files [default: False]. entity_dir (str, optional): The relative path where the entity will be created inside the ml entity directory [default: empty]. \"\"\" init def init ( self , entity ): \"\"\"This command will start the ml-git entity. Examples: api = MLGitApi()\\n api.init('repository')\\n api.init('datasets') Args: entity (str): The type of an ML entity (datasets, labels or models). \"\"\" models metrics def get_models_metrics ( self , entity_name , export_path = None , export_type = FileType . JSON . value ): \"\"\"Get metrics information for each tag of the entity. Examples: api = MLGitApi()\\n api.get_models_metrics('model-ex', export_type='csv') Args: entity_name (str): An ml-git entity name to identify a ML entity. export_path(str, optional): Set the path to export metrics to a file. export_type (str, optional): Choose the format of the file that will be generated with the metrics [default: json]. \"\"\" push def push ( self , entity , entity_name , retries = 2 , clear_on_fail = False , fail_limit = None ): \"\"\"This command allows pushing the data of a specific version of an ML entity. Example: api = MLGitApi()\\n api.push('datasets', 'dataset-ex') Args: entity (str): The type of an ML entity. (datasets, labels or models) entity_name (str): An ml-git entity name to identify a ML entity. retries (int, optional): Number of retries to upload the files to the storage [default: 2]. clear_on_fail (bool, optional): Remove the files from the storage in case of failure during the push operation [default: False]. fail_limit (int, optional): Number of failures before aborting the command [default: no limit]. \"\"\" remote add def remote_add ( self , entity , remote_url , global_configuration = False ): \"\"\"This command will add a remote to store the metadata from this ml-git project. Examples: api = MLGitApi()\\n api.remote_add('datasets', 'https://git@github.com/mlgit-datasets') Args: entity (str): The type of an ML entity (datasets, labels or models). remote_url(str): URL of an existing remote git repository. global_configuration (bool, optional): Use this option to set configuration at global level [default: False]. \"\"\" storage add def storage_add ( self , bucket_name , bucket_type = StorageType . S3H . value , credentials = None , global_configuration = False , endpoint_url = None , username = None , private_key = None , port = 22 , region = None ): \"\"\"This command will add a storage to the ml-git project. Examples: api = MLGitApi()\\n api.storage_add('my-bucket', bucket_type='s3h') Args: bucket_name (str): The name of the bucket in the storage. bucket_type (str, optional): Storage type (s3h, azureblobh or gdriveh) [default: s3h]. credentials (str, optional): Name of the profile that stores the credentials or the path to the credentials. global_configuration (bool, optional): Use this option to set configuration at global level [default: False]. endpoint_url (str, optional): Storage endpoint url. username (str, optional): The username for the sftp login. private_key (str, optional): Full path for the private key file. port (int, optional): The port to be used when connecting to the storage. region (str, optional): AWS region for S3 bucket. \"\"\" init entity manager def init_entity_manager ( github_token , url ): \"\"\"Initialize an entity manager to operate over github API. Examples: init_entity_manager('github_token', 'https://api.github.com') Args: github_token (str): The personal access github token. url (str): The github api url. Returns: object of class EntityManager. \"\"\" init local entity manager def init_local_entity_manager (): \"\"\"Initialize an entity manager to operate over local git repository. Returns: object of class LocalEntityManager. \"\"\"","title":" Methods available in the MLGitAPI class "},{"location":"api/#classes-used-by-the-api","text":"Some methods available in the API use the classes described below: EntityManager class EntityManager : \"\"\"Class that operate over github api to manage entity's operations\"\"\" def get_entities ( self , config_path = None , config_repo_name = None ): \"\"\"Get a list of entities found in config.yaml. Args: config_path (str): The absolute path of the config.yaml file. config_repo_name (str): The repository name where is the config.yaml located in github. Returns: list of class Entity. \"\"\" def get_entity_versions ( self , name , metadata_repo_name ): \"\"\"Get a list of spec versions found for an especific entity. Args: name (str): The name of the entity you want to get the versions. metadata_repo_name (str): The repository name where the entity metadata is located in GitHub. Returns: list of class SpecVersion. \"\"\" def get_linked_entities ( self , name , version , metadata_repo_name ): \"\"\"Get a list of linked entities found for an entity version. Args: name (str): The name of the entity you want to get the linked entities. version (str): The version of the entity you want to get the linked entities. metadata_repo_name (str): The repository name where the entity metadata is located in GitHub. Returns: list of LinkedEntity. \"\"\" def get_entity_relationships ( self , name , metadata_repo_name , export_type = FileType . JSON . value , export_path = None ): \"\"\"Get a list of relationships for an entity. Args: name (str): The name of the entity you want to get the linked entities. metadata_repo_name (str): The repository name where the entity metadata is located in GitHub. export_type (str): Set the format of the return (json, csv, dot) [default: json]. export_path (str): Set the path to export metrics to a file. Returns: list of EntityVersionRelationships. \"\"\" def get_project_entities_relationships ( self , config_repo_name , export_type = FileType . JSON . value , export_path = None ): \"\"\"Get a list of relationships for all project entities. Args: config_repo_name (str): The repository name where the config file is located in GitHub. export_type (str): Set the format of the return (json, csv, dot) [default: json]. export_path (str): Set the path to export metrics to a file. Returns: list of EntityVersionRelationships. \"\"\" LocalEntityManager class LocalEntityManager : \"\"\"Class that operate over local git repository to manage entity's operations\"\"\" def get_entities ( self ): \"\"\"Get a list of entities found in config.yaml. Returns: list of class Entity. \"\"\" def get_entity_versions ( self , name , type_entity ): \"\"\"Get a list of spec versions found for an especific entity. Args: name (str): The name of the entity you want to get the versions. type_entity (str): The type of the ml-entity (datasets, models, labels). Returns: list of class SpecVersion. \"\"\" def get_linked_entities ( self , name , version , type_entity ): \"\"\"Get a list of linked entities found for an entity version. Args: name (str): The name of the entity you want to get the linked entities. version (str): The version of the entity you want to get the linked entities. type_entity (str): The type of the ml-entity (datasets, models, labels). Returns: list of LinkedEntity. \"\"\" def get_entity_relationships ( self , name , type_entity , export_type = FileType . JSON . value , export_path = None ): \"\"\"Get a list of relationships for an entity. Args: name (str): The name of the entity you want to get the linked entities. type_entity (str): The type of the ml-entity (datasets, models, labels). export_type (str): Set the format of the return (json, csv, dot) [default: json]. export_path (str): Set the path to export metrics to a file. Returns: list of EntityVersionRelationships. \"\"\" def get_project_entities_relationships ( self , export_type = FileType . JSON . value , export_path = None ): \"\"\"Get a list of relationships for all project entities. Args: export_type (str): Set the format of the return [default: json]. export_path (str): Set the path to export metrics to a file. Returns: list of EntityVersionRelationships. \"\"\" def export_graph ( self , dot_graph , export_path ): \"\"\"Creates a graph of all entity relations as an HTML file. Args: dot_graph (str): String of graph in DOT language format. export_path (str): Set the path to export the HTML with the graph. [default: project root path] Returns: Path of HTML file. \"\"\" Entity class Entity : \"\"\"Class that represents an ml-entity. Attributes: name (str): The name of the entity. type (str): The type of the ml-entity (datasets, models, labels). private (str): The access of entity metadata. metadata (Metadata): The metadata of the entity. last_spec_version (SpecVersion): The specification file of the entity last version. \"\"\" SpecVersion class SpecVersion : \"\"\"Class that represents an ml-entity spec version. Attributes: name (str): The name of the entity. type (str): The type of the ml-entity (datasets, models, labels). version (str): The version of the ml-entity. tag (str): The tag of the ml-entity spec version. mutability (str): The mutability of the ml-entity. categories (list): Labels to categorize the entity. storage (Storage): The storage of the ml-entity. total_versioned_files (int): The amount of versioned files. size (str): The size of the version files. \"\"\" Metadata class Metadata : \"\"\"Class that represents an ml-entity metadata. Attributes: full_name (str): The full name of the metadata. git_url (str): The git url of the metadata. html_url (str): The html url of the metadata. owner_email (str): The owner email of the ml-entity metadata. owner_name (str): The owner name of the ml-entity metadata. \"\"\" Storage class Storage : \"\"\"Class that represents an ml-entity storage. Attributes: type (str): The storage type (s3h|azureblobh|gdriveh|sftph). bucket (str): The name of the bucket. \"\"\" EntityVersionRelationships class EntityVersionRelationships : \"\"\"Class that represents the relationships of an ml-entity in a specified version. Attributes: version (str): The version of the ml-entity. tag (str): The tag of the ml-entity. relationships (list): List of linked entities of the ml-entity in the specified version. \"\"\" LinkedEntity class LinkedEntity : \"\"\"Class that represents a linked ml-entity. Attributes: name (str): The name of the entity. type (str): The type of the ml-entity (datasets, models, labels). version (str): The version of the ml-entity. tag (str): The tag of the ml-entity spec version. \"\"\"","title":"Classes used by the API."},{"location":"api/#api-notebooks","text":"In the api_scripts directory, you can find notebooks running the ML-Git API for some scenarios. To run them, you need to boot the jupyter notebook server in an environment with ML-Git installed and navigate to the notebook of your choice.","title":" API notebooks "},{"location":"api/quick_start/","text":"ML-Git API \u00b6 Quick start \u00b6 To use the ML-Git API, it's necessary to have ML-Git installed in the environment that will be executed. When instantiating the MLGitAPI class, it's required to either inform an existing directory in the root_path parameter or not pass any value at all. Instantiating the API \u00b6 You can inform the root directory of your ML-Git Project by passing an absolute path or a relative path to the root_path parameter. from ml_git.api import MLGitAPI api = MLGitAPI ( root_path = '/absolute/path/to/your/project' ) # or api = MLGitAPI ( root_path = './relative/path/to/your/project' ) Note: The root_path parameter can receive any string accepted by the pathlib.Path class. You can also work with your current working directory (CWD) by not passing any value. from ml_git.api import MLGitAPI api = MLGitAPI () Multiple ML-Git Projects \u00b6 It's also possible to work with multiple projects in the same python script by instantiating the MLGitAPI class for each project. from ml_git.api import MLGitAPI api_project_1 = MLGitAPI ( root_path = '/path/to/project_1' ) api_project_2 = MLGitAPI ( root_path = '/path/to/project_2' ) Each instance will run its commands in the context of its project. It's important to note that these instances are not aware of each other nor follow the singleton pattern, so it's possible to have multiple instances pointing to the same directory, so if you end up in this situation, be careful not to run commands that can be conflicting, like trying to create the same entity in more than one instance. ML-Git Repository \u00b6 To use most of the commands available in the API, you need to be working with a directory containing a valid ML-Git Project. For that, you can clone a repository by using the clone command, or you can start a new repository with the init command. Clone \u00b6 repository_url = 'https://git@github.com/mlgit-repository' api . clone ( repository_url ) output: INFO - Metadata Manager: Metadata init [https://git@github.com/mlgit-repository] @ [/home/user/Documentos/mlgit-api/mlgit/.ml-git/dataset/metadata] INFO - Metadata: Successfully loaded configuration files! Init \u00b6 api . init ( 'repository' ) output: INFO - Admin: Initialized empty ml-git repository in /home/user/Documentos/project/.ml-git Note: To use these commands, the instance must be pointing to an empty (or not previously initialized) directory. Checkout \u00b6 Checkout dataset \u00b6 entity = 'datasets' tag = 'computer-vision__images__imagenet__1' data_path = api . checkout ( entity , tag ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/dataset/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 2.87kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 2.35kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 3.00kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 1.72kfiles into workspace/s] Checkout labels with dataset \u00b6 entity = 'labels' tag = 'computer-vision__images__mscoco__2' data_path = api . checkout ( entity , tag , dataset = True ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/labels/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00/6.00 [00:00<00:00, 205blobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00/6.00 [00:00<00:00, 173chunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00/6.00 [00:00<00:00, 788files into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00/6.00 [00:00<00:00, 1.28kfiles into workspace/s] INFO - Repository: Initializing related dataset download blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 3.27kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 2.37kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 2.40kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 1.72kfiles into workspace/s] Checkout dataset with sample \u00b6 Group-Sample \u00b6 entity = 'datasets' tag = 'computer-vision__images__imagenet__1' sampling = { 'group' : '1:2' , 'seed' : '10' } data_path = api . checkout ( entity , tag , sampling ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/dataset/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.04kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.83kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.09kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.16kfiles into workspace/s] Range-Sample \u00b6 entity = 'datasets' tag = 'computer-vision__images__imagenet__1' sampling = { 'range' : '0:4:3' } data_path = api . checkout ( entity , tag , sampling ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/dataset/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.71kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.54kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.22kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.55kfiles into workspace/s] Random-Sample \u00b6 entity = 'datasets' tag = 'computer-vision__images__imagenet__1' sampling = { 'random' : '1:2' , 'seed' : '1' } data_path = api . checkout ( entity , tag , sampling ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/dataset/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.47kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.00kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 3.77kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.17kfiles into workspace/s] Add \u00b6 api . add ( 'datasets' , 'dataset-ex' ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/mlgit-api/mlgit/.ml-git/dataset/metadata] INFO - Repository: dataset adding path [[/home/user/Documentos/mlgit-api/mlgit/dataset//dataset-ex] to ml-git index files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 381files/s] Commit \u00b6 entity = 'datasets' entity_name = 'dataset-ex' message = 'Commit example' api . commit ( entity , entity_name , message ) output: INFO - Metadata Manager: Commit repo[/home/user/Documentos/project/.ml-git/dataset/metadata] --- file[computer-vision/images/dataset-ex] Push \u00b6 entity = 'datasets' spec = 'dataset-ex' api . push ( entity , spec ) output: files: 100%|##########| 24.0/24.0 [00:00<00:00, 34.3files/s] Create \u00b6 entity = 'datasets' spec = 'dataset-ex' categories = [ 'computer-vision' , 'images' ] mutability = 'strict' api . create ( entity , spec , categories , mutability , import_path = '/path/to/dataset' , unzip = True , version = 2 ) output: INFO - MLGit: Dataset artifact created. Init \u00b6 The init command is used to start either an entity or, as shown before, a repository. Repository \u00b6 api . init ( 'repository' ) output: INFO - Admin: Initialized empty ml-git repository in /home/user/Documentos/project/.ml-git Entity \u00b6 entity_type = 'datasets' api . init ( entity_type ) output: INFO - Metadata Manager: Metadata init [https://git@github.com/mlgit-datasets] @ [/home/user/Documentos/project/.ml-git/dataset/metadata] Remote add \u00b6 entity_type = 'datasets' datasets_repository = 'https://git@github.com/mlgit-datasets' api . remote_add ( entity_type , datasets_repository ) output: INFO - Admin: Add remote repository [https://git@github.com/mlgit-datasets] for [dataset] Storage add \u00b6 bucket_name = 'minio' bucket_type = 's3h' endpoint_url = 'http://127.0.0.1:9000/' api . storage_add ( bucket_name = bucket_name , bucket_type = bucket_type , endpoint_url = endpoint_url ) output: INFO - Admin: Add storage [s3h://minio] List entities \u00b6 List entities from a config file \u00b6 github_token = '' api_url = 'https://api.github.com' manager = api . init_entity_manager ( github_token , api_url ) entities = manager . get_entities ( config_path = 'path/to/config.yaml' ) List entities from a repository \u00b6 github_token = '' api_url = 'https://api.github.com' manager = api . init_entity_manager ( github_token , api_url ) entities = manager . get_entities ( config_repo_name = 'user/config_repository' ) List versions from a entity \u00b6 github_token = '' api_url = 'https://api.github.com' manager = api . init_entity_manager ( github_token , api_url ) versions = manager . get_entity_versions ( 'entity_name' , metadata_repo_name = 'user/metadata_repository' ) List linked entities from a entity version \u00b6 github_token = '' api_url = 'https://api.github.com' manager = api . init_entity_manager ( github_token , api_url ) linked_entities = manager . get_linked_entities ( 'entity_name' , 'entity_version' , metadata_repo_name = 'user/metadata_repository' ) List relationships from a entity \u00b6 github_token = '' api_url = 'https://api.github.com' manager = api . init_entity_manager ( github_token , api_url ) relationships = manager . get_entity_relationships ( 'entity_name' , metadata_repo_name = 'user/metadata_repository' ) List relationships from all project entities \u00b6 github_token = '' api_url = 'https://api.github.com' manager = api . init_entity_manager ( github_token , api_url ) relationships = manager . get_project_entities_relationships ( config_repo_name = 'user/config_repository' )","title":"Quick Start"},{"location":"api/quick_start/#ml-git-api","text":"","title":"ML-Git API"},{"location":"api/quick_start/#quick-start","text":"To use the ML-Git API, it's necessary to have ML-Git installed in the environment that will be executed. When instantiating the MLGitAPI class, it's required to either inform an existing directory in the root_path parameter or not pass any value at all.","title":" Quick start "},{"location":"api/quick_start/#instantiating-the-api","text":"You can inform the root directory of your ML-Git Project by passing an absolute path or a relative path to the root_path parameter. from ml_git.api import MLGitAPI api = MLGitAPI ( root_path = '/absolute/path/to/your/project' ) # or api = MLGitAPI ( root_path = './relative/path/to/your/project' ) Note: The root_path parameter can receive any string accepted by the pathlib.Path class. You can also work with your current working directory (CWD) by not passing any value. from ml_git.api import MLGitAPI api = MLGitAPI ()","title":"Instantiating the API"},{"location":"api/quick_start/#multiple-ml-git-projects","text":"It's also possible to work with multiple projects in the same python script by instantiating the MLGitAPI class for each project. from ml_git.api import MLGitAPI api_project_1 = MLGitAPI ( root_path = '/path/to/project_1' ) api_project_2 = MLGitAPI ( root_path = '/path/to/project_2' ) Each instance will run its commands in the context of its project. It's important to note that these instances are not aware of each other nor follow the singleton pattern, so it's possible to have multiple instances pointing to the same directory, so if you end up in this situation, be careful not to run commands that can be conflicting, like trying to create the same entity in more than one instance.","title":"Multiple ML-Git Projects"},{"location":"api/quick_start/#ml-git-repository","text":"To use most of the commands available in the API, you need to be working with a directory containing a valid ML-Git Project. For that, you can clone a repository by using the clone command, or you can start a new repository with the init command.","title":"ML-Git Repository"},{"location":"api/quick_start/#clone","text":"repository_url = 'https://git@github.com/mlgit-repository' api . clone ( repository_url ) output: INFO - Metadata Manager: Metadata init [https://git@github.com/mlgit-repository] @ [/home/user/Documentos/mlgit-api/mlgit/.ml-git/dataset/metadata] INFO - Metadata: Successfully loaded configuration files!","title":"Clone"},{"location":"api/quick_start/#init","text":"api . init ( 'repository' ) output: INFO - Admin: Initialized empty ml-git repository in /home/user/Documentos/project/.ml-git Note: To use these commands, the instance must be pointing to an empty (or not previously initialized) directory.","title":"Init"},{"location":"api/quick_start/#checkout","text":"","title":"Checkout"},{"location":"api/quick_start/#checkout-dataset","text":"entity = 'datasets' tag = 'computer-vision__images__imagenet__1' data_path = api . checkout ( entity , tag ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/dataset/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 2.87kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 2.35kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 3.00kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 1.72kfiles into workspace/s]","title":"Checkout dataset"},{"location":"api/quick_start/#checkout-labels-with-dataset","text":"entity = 'labels' tag = 'computer-vision__images__mscoco__2' data_path = api . checkout ( entity , tag , dataset = True ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/labels/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00/6.00 [00:00<00:00, 205blobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00/6.00 [00:00<00:00, 173chunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00/6.00 [00:00<00:00, 788files into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00/6.00 [00:00<00:00, 1.28kfiles into workspace/s] INFO - Repository: Initializing related dataset download blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 3.27kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 2.37kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 2.40kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 1.72kfiles into workspace/s]","title":"Checkout labels with dataset"},{"location":"api/quick_start/#checkout-dataset-with-sample","text":"","title":"Checkout dataset with sample"},{"location":"api/quick_start/#group-sample","text":"entity = 'datasets' tag = 'computer-vision__images__imagenet__1' sampling = { 'group' : '1:2' , 'seed' : '10' } data_path = api . checkout ( entity , tag , sampling ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/dataset/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.04kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.83kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.09kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.16kfiles into workspace/s]","title":"Group-Sample"},{"location":"api/quick_start/#range-sample","text":"entity = 'datasets' tag = 'computer-vision__images__imagenet__1' sampling = { 'range' : '0:4:3' } data_path = api . checkout ( entity , tag , sampling ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/dataset/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.71kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.54kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.22kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.55kfiles into workspace/s]","title":"Range-Sample"},{"location":"api/quick_start/#random-sample","text":"entity = 'datasets' tag = 'computer-vision__images__imagenet__1' sampling = { 'random' : '1:2' , 'seed' : '1' } data_path = api . checkout ( entity , tag , sampling ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/dataset/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.47kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.00kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 3.77kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.17kfiles into workspace/s]","title":"Random-Sample"},{"location":"api/quick_start/#add","text":"api . add ( 'datasets' , 'dataset-ex' ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/mlgit-api/mlgit/.ml-git/dataset/metadata] INFO - Repository: dataset adding path [[/home/user/Documentos/mlgit-api/mlgit/dataset//dataset-ex] to ml-git index files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 381files/s]","title":"Add"},{"location":"api/quick_start/#commit","text":"entity = 'datasets' entity_name = 'dataset-ex' message = 'Commit example' api . commit ( entity , entity_name , message ) output: INFO - Metadata Manager: Commit repo[/home/user/Documentos/project/.ml-git/dataset/metadata] --- file[computer-vision/images/dataset-ex]","title":"Commit"},{"location":"api/quick_start/#push","text":"entity = 'datasets' spec = 'dataset-ex' api . push ( entity , spec ) output: files: 100%|##########| 24.0/24.0 [00:00<00:00, 34.3files/s]","title":"Push"},{"location":"api/quick_start/#create","text":"entity = 'datasets' spec = 'dataset-ex' categories = [ 'computer-vision' , 'images' ] mutability = 'strict' api . create ( entity , spec , categories , mutability , import_path = '/path/to/dataset' , unzip = True , version = 2 ) output: INFO - MLGit: Dataset artifact created.","title":"Create"},{"location":"api/quick_start/#init_1","text":"The init command is used to start either an entity or, as shown before, a repository.","title":"Init"},{"location":"api/quick_start/#repository","text":"api . init ( 'repository' ) output: INFO - Admin: Initialized empty ml-git repository in /home/user/Documentos/project/.ml-git","title":"Repository"},{"location":"api/quick_start/#entity","text":"entity_type = 'datasets' api . init ( entity_type ) output: INFO - Metadata Manager: Metadata init [https://git@github.com/mlgit-datasets] @ [/home/user/Documentos/project/.ml-git/dataset/metadata]","title":"Entity"},{"location":"api/quick_start/#remote-add","text":"entity_type = 'datasets' datasets_repository = 'https://git@github.com/mlgit-datasets' api . remote_add ( entity_type , datasets_repository ) output: INFO - Admin: Add remote repository [https://git@github.com/mlgit-datasets] for [dataset]","title":"Remote add"},{"location":"api/quick_start/#storage-add","text":"bucket_name = 'minio' bucket_type = 's3h' endpoint_url = 'http://127.0.0.1:9000/' api . storage_add ( bucket_name = bucket_name , bucket_type = bucket_type , endpoint_url = endpoint_url ) output: INFO - Admin: Add storage [s3h://minio]","title":"Storage add"},{"location":"api/quick_start/#list-entities","text":"","title":"List entities"},{"location":"api/quick_start/#list-entities-from-a-config-file","text":"github_token = '' api_url = 'https://api.github.com' manager = api . init_entity_manager ( github_token , api_url ) entities = manager . get_entities ( config_path = 'path/to/config.yaml' )","title":"List entities from a config file"},{"location":"api/quick_start/#list-entities-from-a-repository","text":"github_token = '' api_url = 'https://api.github.com' manager = api . init_entity_manager ( github_token , api_url ) entities = manager . get_entities ( config_repo_name = 'user/config_repository' )","title":"List entities from a repository"},{"location":"api/quick_start/#list-versions-from-a-entity","text":"github_token = '' api_url = 'https://api.github.com' manager = api . init_entity_manager ( github_token , api_url ) versions = manager . get_entity_versions ( 'entity_name' , metadata_repo_name = 'user/metadata_repository' )","title":"List versions from a entity"},{"location":"api/quick_start/#list-linked-entities-from-a-entity-version","text":"github_token = '' api_url = 'https://api.github.com' manager = api . init_entity_manager ( github_token , api_url ) linked_entities = manager . get_linked_entities ( 'entity_name' , 'entity_version' , metadata_repo_name = 'user/metadata_repository' )","title":"List linked entities from a entity version"},{"location":"api/quick_start/#list-relationships-from-a-entity","text":"github_token = '' api_url = 'https://api.github.com' manager = api . init_entity_manager ( github_token , api_url ) relationships = manager . get_entity_relationships ( 'entity_name' , metadata_repo_name = 'user/metadata_repository' )","title":"List relationships from a entity"},{"location":"api/quick_start/#list-relationships-from-all-project-entities","text":"github_token = '' api_url = 'https://api.github.com' manager = api . init_entity_manager ( github_token , api_url ) relationships = manager . get_project_entities_relationships ( config_repo_name = 'user/config_repository' )","title":"List relationships from all project entities"},{"location":"api/api_scripts/","text":"ML-Git Step-by-Step Guide \u00b6 About \u00b6 In order to facilitate the learning process of using the ML-Git API, we offer a series of step-by-step guides that emulate real situations in the learning context by applying the correct API command sequences that should be used in each scenario. The guides were created using Jupyter notebooks, as they offer the possibility to run, step-by-step, code snippets in a format similar to an explanatory document, thus becoming ideal for teaching new users how to use the API in real case scenarios. How execute notebooks: \u00b6 To run notebooks more easily, a docker environment has been created that performs all the environment settings required by the user. So make sure that the procedures in the local environment configuration section have been performed. Summary of existing notebooks: \u00b6 basic_flow This notebook describes a basic execution flow of ML-Git with its API. GitHub link clone_repository This notebook describes how to clone an ML-Git repository. GitHub link multiple_projects This notebook describes how to work with multiple projects in the ML-Git API. GitHub link relationship_api_commands This notebook describes a basic flow in the context of relationships between entities with the API provided by ML-Git. GitHub link mnist_random_forest_api This notebook describes a basic execution flow of ML-Git with its API using the MNIST dataset. GitHub link mnist_random_forest_cli This notebook describes a basic execution flow of ML-Git with its CLI using the MNIST dataset. GitHub link checkout_with_sample This notebook describes how to perform a checkout operation with ML-Git using samples of a dataset. GitHub link multiple_datasets This notebook describes how to handle the scenario where the same file is present in more than one dataset. GitHub link","title":"ML-Git Step-by-Step Guide"},{"location":"api/api_scripts/#ml-git-step-by-step-guide","text":"","title":"ML-Git Step-by-Step Guide"},{"location":"api/api_scripts/#about","text":"In order to facilitate the learning process of using the ML-Git API, we offer a series of step-by-step guides that emulate real situations in the learning context by applying the correct API command sequences that should be used in each scenario. The guides were created using Jupyter notebooks, as they offer the possibility to run, step-by-step, code snippets in a format similar to an explanatory document, thus becoming ideal for teaching new users how to use the API in real case scenarios.","title":"About"},{"location":"api/api_scripts/#how-execute-notebooks","text":"To run notebooks more easily, a docker environment has been created that performs all the environment settings required by the user. So make sure that the procedures in the local environment configuration section have been performed.","title":"How execute notebooks:"},{"location":"api/api_scripts/#summary-of-existing-notebooks","text":"basic_flow This notebook describes a basic execution flow of ML-Git with its API. GitHub link clone_repository This notebook describes how to clone an ML-Git repository. GitHub link multiple_projects This notebook describes how to work with multiple projects in the ML-Git API. GitHub link relationship_api_commands This notebook describes a basic flow in the context of relationships between entities with the API provided by ML-Git. GitHub link mnist_random_forest_api This notebook describes a basic execution flow of ML-Git with its API using the MNIST dataset. GitHub link mnist_random_forest_cli This notebook describes a basic execution flow of ML-Git with its CLI using the MNIST dataset. GitHub link checkout_with_sample This notebook describes how to perform a checkout operation with ML-Git using samples of a dataset. GitHub link multiple_datasets This notebook describes how to handle the scenario where the same file is present in more than one dataset. GitHub link","title":"Summary of existing notebooks:"},{"location":"api/api_scripts/basic_flow/","text":"Basic Flow \u00b6 This notebook describes a basic execution flow of ml-git with its API. There, you will learn how to initialize an ML-Git project, how to perform all the necessary configuration steps and how to version a dataset. We will divide this quick howto into 3 main sections: ml-git repository configuation/intialization \u00b6 This section explains how to initialize and configure a repository for ml-git. versioning a dataset \u00b6 Having a repository initialized, this section explains how to create and upload a dataset to the storage. downloading a dataset \u00b6 This section describes how to download a versioned data set using ml-git. Notebook state management \u00b6 If you have already run this notebook or another in this same folder, it is recommended that you perform a state restart by executing the cell below, because previously performed state changes may interfere with the execution of the notebook. Be aware, that procedure does not affect any remote repository. % cd / api_scripts ! rm - rf ./ logs ! rm - rf . ml - git ! rm - rf ./ datasets 1 - ml-git repository configuation/intialization \u00b6 To start using the ml-git api we need to import it into our script from ml_git.api import MLGitAPI Then we must create a new instance of the API class api = MLGitAPI () To use ml-git, it is necessary to configure storages and remotes that will be used in a project. This configuration can be done through a sequence of commands, or if you already have a git repository with the stored settings, you can run the clone command to import those settings. The following subsections demonstrate how to configure these two forms. Note: You should only perform one of the following two subsections. 1.1 Configuring with clone command With the clone command all settings will be imported and initialized from the repository that was informed. repository_url = '/local_ml_git_config_server.git' api . clone ( repository_url ) After that, you can skip to section 2 which teaches you how to create a version of a dataset. 1.2 Configuring from start In this section we will consider the scenario of a user who wants to configure their project from scratch. The first step is to define that the directory we are working on will be an ml-git project, for this we execute the following command: api . init ( 'repository' ) INFO - Admin: Initialized empty ml-git repository in /api_scripts/.ml-git After initializing an ml-git project, it is necessary that you inform the remotes and storages that will be used by the entities to be versioned. If you want to better understand why ml-git uses these resources, please take a look at the architecture and internals documentation . In this notebook we will configure our ml-git project with a local git repository and a local minio as storage. For this, the following commands are necessary: remote_url = '/local_server.git/' bucket_name = 'mlgit' end_point = 'http://127.0.0.1:9000' # The type of entity we are working on entity_type = 'datasets' api . remote_add ( entity_type , remote_url ) api . storage_add ( bucket_name , endpoint_url = end_point ) INFO - Admin: Add remote repository [/local_server.git/] for [datasets] INFO - Admin: Add storage [s3h://mlgit] Last but not least, initialize the metadata repository api . init ( entity_type ) INFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/.ml-git/datasets/metadata] 2 - versioning a dataset \u00b6 After the entities have been initialized and are ready for use. We can continue with the process to version our first dataset. ml-git expects any dataset to be specified under dataset/ directory of your project and it expects a specification file with the name of the dataset. To create this specification file for a new entity you must run the following command: # The entity name we are working on entity_name = 'dataset-ex' api . create ( entity_type , entity_name , categories = [ 'computer-vision' , 'images' ], mutability = 'strict' , bucket_name = bucket_name ) INFO - MLGit: Dataset artifact created. Once we create our dataset entity we can add the data to be versioned within the entity's directory. For this, the following code generate a new file in our dataset path. import os def create_file ( file_name = 'file' ): file_path = os . path . join ( 'datasets' , 'dataset-ex' , 'data' , file_name ) open ( file_path , 'a' ) . close () create_file () We can now proceed with the necessary steps to send the new data to storage. api . add ( entity_type , entity_name , bumpversion = True ) INFO - Metadata Manager: Pull [/api_scripts/.ml-git/datasets/metadata] INFO - Repository: datasets adding path [/api_scripts/datasets/dataset-ex] to ml-git index files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 360files/s] \u280b Creating hard links in cache\u2819 Creating hard links in cache files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 4.13kfiles/s] After add the files, you need commit the metadata to the local repository. For this purpose type the following command: # Custom commit message message = 'Commit example' api . commit ( entity_type , entity_name , message ) \u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest INFO - Metadata Manager: Commit repo[/api_scripts/.ml-git/datasets/metadata] --- file[dataset-ex] Last but not least, ml-git dataset push will update the remote metadata repository just after storing all actual data under management in the specified remote data storage. api . push ( entity_type , entity_name ) files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 57.4files/s] \u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository As you can observe, ml-git follows very similar workflows as for git. 3 - downloading a dataset \u00b6 Once you have an entity versioned by ml-git, and being within an initialized directory, it is really simple to obtain data from a specific entity. As an example, in this notebook we will checkout an entity that was previously versioned, the mnist. For this, the following command is necessary: entity_name = 'mnist' data_path = api . checkout ( entity_type , entity_name , version = 1 ) INFO - Metadata Manager: Pull [/api_scripts/.ml-git/datasets/metadata] INFO - Metadata: Performing checkout in tag handwritten__digits__mnist__1 blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 186blobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.24chunks/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 12.6files into workspace/s] Getting the data will auto-create a directory structure under dataset directory. That structure computer-vision/images is actually coming from the categories defined in the dataset spec file. Doing that way allows for easy download of many datasets in one single ml-git project without creating any conflicts. Now the user can perform the processes he wants with the data that was downloaded in the workspace.","title":"Basic Flow"},{"location":"api/api_scripts/basic_flow/#basic-flow","text":"This notebook describes a basic execution flow of ml-git with its API. There, you will learn how to initialize an ML-Git project, how to perform all the necessary configuration steps and how to version a dataset. We will divide this quick howto into 3 main sections:","title":"Basic Flow"},{"location":"api/api_scripts/basic_flow/#ml-git-repository-configuationintialization","text":"This section explains how to initialize and configure a repository for ml-git.","title":"ml-git repository configuation/intialization"},{"location":"api/api_scripts/basic_flow/#versioning-a-dataset","text":"Having a repository initialized, this section explains how to create and upload a dataset to the storage.","title":"versioning a dataset"},{"location":"api/api_scripts/basic_flow/#downloading-a-dataset","text":"This section describes how to download a versioned data set using ml-git.","title":"downloading a dataset"},{"location":"api/api_scripts/basic_flow/#notebook-state-management","text":"If you have already run this notebook or another in this same folder, it is recommended that you perform a state restart by executing the cell below, because previously performed state changes may interfere with the execution of the notebook. Be aware, that procedure does not affect any remote repository. % cd / api_scripts ! rm - rf ./ logs ! rm - rf . ml - git ! rm - rf ./ datasets","title":"Notebook state management"},{"location":"api/api_scripts/basic_flow/#1-ml-git-repository-configuationintialization","text":"To start using the ml-git api we need to import it into our script from ml_git.api import MLGitAPI Then we must create a new instance of the API class api = MLGitAPI () To use ml-git, it is necessary to configure storages and remotes that will be used in a project. This configuration can be done through a sequence of commands, or if you already have a git repository with the stored settings, you can run the clone command to import those settings. The following subsections demonstrate how to configure these two forms. Note: You should only perform one of the following two subsections. 1.1 Configuring with clone command With the clone command all settings will be imported and initialized from the repository that was informed. repository_url = '/local_ml_git_config_server.git' api . clone ( repository_url ) After that, you can skip to section 2 which teaches you how to create a version of a dataset. 1.2 Configuring from start In this section we will consider the scenario of a user who wants to configure their project from scratch. The first step is to define that the directory we are working on will be an ml-git project, for this we execute the following command: api . init ( 'repository' ) INFO - Admin: Initialized empty ml-git repository in /api_scripts/.ml-git After initializing an ml-git project, it is necessary that you inform the remotes and storages that will be used by the entities to be versioned. If you want to better understand why ml-git uses these resources, please take a look at the architecture and internals documentation . In this notebook we will configure our ml-git project with a local git repository and a local minio as storage. For this, the following commands are necessary: remote_url = '/local_server.git/' bucket_name = 'mlgit' end_point = 'http://127.0.0.1:9000' # The type of entity we are working on entity_type = 'datasets' api . remote_add ( entity_type , remote_url ) api . storage_add ( bucket_name , endpoint_url = end_point ) INFO - Admin: Add remote repository [/local_server.git/] for [datasets] INFO - Admin: Add storage [s3h://mlgit] Last but not least, initialize the metadata repository api . init ( entity_type ) INFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/.ml-git/datasets/metadata]","title":"1 - ml-git repository configuation/intialization"},{"location":"api/api_scripts/basic_flow/#2-versioning-a-dataset","text":"After the entities have been initialized and are ready for use. We can continue with the process to version our first dataset. ml-git expects any dataset to be specified under dataset/ directory of your project and it expects a specification file with the name of the dataset. To create this specification file for a new entity you must run the following command: # The entity name we are working on entity_name = 'dataset-ex' api . create ( entity_type , entity_name , categories = [ 'computer-vision' , 'images' ], mutability = 'strict' , bucket_name = bucket_name ) INFO - MLGit: Dataset artifact created. Once we create our dataset entity we can add the data to be versioned within the entity's directory. For this, the following code generate a new file in our dataset path. import os def create_file ( file_name = 'file' ): file_path = os . path . join ( 'datasets' , 'dataset-ex' , 'data' , file_name ) open ( file_path , 'a' ) . close () create_file () We can now proceed with the necessary steps to send the new data to storage. api . add ( entity_type , entity_name , bumpversion = True ) INFO - Metadata Manager: Pull [/api_scripts/.ml-git/datasets/metadata] INFO - Repository: datasets adding path [/api_scripts/datasets/dataset-ex] to ml-git index files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 360files/s] \u280b Creating hard links in cache\u2819 Creating hard links in cache files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 4.13kfiles/s] After add the files, you need commit the metadata to the local repository. For this purpose type the following command: # Custom commit message message = 'Commit example' api . commit ( entity_type , entity_name , message ) \u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest INFO - Metadata Manager: Commit repo[/api_scripts/.ml-git/datasets/metadata] --- file[dataset-ex] Last but not least, ml-git dataset push will update the remote metadata repository just after storing all actual data under management in the specified remote data storage. api . push ( entity_type , entity_name ) files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 57.4files/s] \u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository As you can observe, ml-git follows very similar workflows as for git.","title":"2 - versioning a dataset"},{"location":"api/api_scripts/basic_flow/#3-downloading-a-dataset","text":"Once you have an entity versioned by ml-git, and being within an initialized directory, it is really simple to obtain data from a specific entity. As an example, in this notebook we will checkout an entity that was previously versioned, the mnist. For this, the following command is necessary: entity_name = 'mnist' data_path = api . checkout ( entity_type , entity_name , version = 1 ) INFO - Metadata Manager: Pull [/api_scripts/.ml-git/datasets/metadata] INFO - Metadata: Performing checkout in tag handwritten__digits__mnist__1 blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 186blobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.24chunks/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 12.6files into workspace/s] Getting the data will auto-create a directory structure under dataset directory. That structure computer-vision/images is actually coming from the categories defined in the dataset spec file. Doing that way allows for easy download of many datasets in one single ml-git project without creating any conflicts. Now the user can perform the processes he wants with the data that was downloaded in the workspace.","title":"3 - downloading a dataset"},{"location":"api/api_scripts/clone_repository/","text":"Cloning an ml-git repository \u00b6 This notebook describes how to clone an ML-Git repository. Notebook state management \u00b6 If you have already run this notebook or another in this same folder, it is recommended that you perform a state restart by executing the cell below, because previously performed state changes may interfere with the execution of the notebook. Be aware, that procedure does not affect any remote repository. % cd / api_scripts ! rm - rf ./ logs ! rm - rf . ml - git ! rm - rf ./ datasets To start using the ml-git api we need to import it into our script \u00b6 from ml_git.api import MLGitAPI After importing you can use the api clone method, passing the url of the git repository as a parameter. \u00b6 repository_url = '/local_ml_git_config_server.git' api = MLGitAPI () api . clone ( repository_url ) INFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/tmprr5gsm40/mlgit/.ml-git/datasets/metadata] INFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/tmprr5gsm40/mlgit/.ml-git/models/metadata] INFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/tmprr5gsm40/mlgit/.ml-git/labels/metadata] INFO - Metadata: Successfully loaded configuration files! When the clone is successfully completed, the entities are initialized and ready for use. \u00b6","title":"Clone Repository"},{"location":"api/api_scripts/clone_repository/#cloning-an-ml-git-repository","text":"This notebook describes how to clone an ML-Git repository.","title":"Cloning an ml-git repository"},{"location":"api/api_scripts/clone_repository/#notebook-state-management","text":"If you have already run this notebook or another in this same folder, it is recommended that you perform a state restart by executing the cell below, because previously performed state changes may interfere with the execution of the notebook. Be aware, that procedure does not affect any remote repository. % cd / api_scripts ! rm - rf ./ logs ! rm - rf . ml - git ! rm - rf ./ datasets","title":"Notebook state management"},{"location":"api/api_scripts/clone_repository/#to-start-using-the-ml-git-api-we-need-to-import-it-into-our-script","text":"from ml_git.api import MLGitAPI","title":"To start using the ml-git api we need to import it into our script"},{"location":"api/api_scripts/clone_repository/#after-importing-you-can-use-the-api-clone-method-passing-the-url-of-the-git-repository-as-a-parameter","text":"repository_url = '/local_ml_git_config_server.git' api = MLGitAPI () api . clone ( repository_url ) INFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/tmprr5gsm40/mlgit/.ml-git/datasets/metadata] INFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/tmprr5gsm40/mlgit/.ml-git/models/metadata] INFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/tmprr5gsm40/mlgit/.ml-git/labels/metadata] INFO - Metadata: Successfully loaded configuration files!","title":"After importing you can use the api clone method, passing the url of the git repository as a parameter."},{"location":"api/api_scripts/clone_repository/#when-the-clone-is-successfully-completed-the-entities-are-initialized-and-ready-for-use","text":"","title":"When the clone is successfully completed, the entities are initialized and ready for use."},{"location":"api/api_scripts/multiple_projects/","text":"Multiple Projects \u00b6 This notebook describes how to work with multiple projects in the ML-Git API. Notebook state management \u00b6 If you have already run this notebook or another in this same folder, it is recommended that you perform a state restart by executing the cell below, because previously performed state changes may interfere with the execution of the notebook. Be aware, that procedure does not affect any remote repository. % cd / api_scripts ! rm - rf ./ logs ! rm - rf . ml - git ! rm - rf ./ datasets ! rm - rf / api_scripts / project_1 ! rm - rf / api_scripts / project_2 Using multiples projects \u00b6 Let's start creating, for each project, a folder to work with. # importing os module import os os . mkdir ( 'project_1' ) os . mkdir ( 'project_2' ) To start using the ML-Git API, we need to import it into our script. from ml_git.api import MLGitAPI Then we must create a new instance of the API for each project. You can inform the root directory of the project as a parameter. In this scenario, we will be using the relative path to each directory. project_1 = MLGitAPI ( root_path = './project_1' ) project_2 = MLGitAPI ( root_path = './project_2' ) We will consider the scenario of a user who wants to configure their projects from scratch. The first step is to define that the directory we are working on will be an ml-git project. To do that, execute the following command: project_1 . init ( 'repository' ) project_2 . init ( 'repository' ) INFO - Admin: Initialized empty ml-git repository in C:\\Git\\HP\\ml-git\\docs\\api\\api_scripts\\project_1\\.ml-git INFO - Admin: Initialized empty ml-git repository in C:\\Git\\HP\\ml-git\\docs\\api\\api_scripts\\project_2\\.ml-git We will configure each project with a local git repository and a local MinIO as storage. For this, the following commands are necessary: remote_url = '/local_server.git/' bucket_name = 'mlgit' end_point = 'http://127.0.0.1:9000' # The type of entity we are working on entity_type = 'datasets' project_1 . remote_add ( entity_type , remote_url ) project_1 . storage_add ( bucket_name , endpoint_url = end_point ) print ( '/n' ) project_2 . remote_add ( entity_type , remote_url ) project_2 . storage_add ( bucket_name , endpoint_url = end_point ) INFO - Admin: Add remote repository [/local_server.git/] for [datasets] INFO - Repository: When making changes to the config file we strongly recommend that you upload these changes to the Git repository. For this, see: ml-git repository config push --help INFO - Admin: Add storage [s3h://mlgit] INFO - Admin: When making changes to the config file we strongly recommend that you upload these changes to the Git repository. For this, see: ml-git repository config push --help /n INFO - Admin: Add remote repository [/local_server.git/] for [datasets] INFO - Repository: When making changes to the config file we strongly recommend that you upload these changes to the Git repository. For this, see: ml-git repository config push --help INFO - Admin: Add storage [s3h://mlgit] INFO - Admin: When making changes to the config file we strongly recommend that you upload these changes to the Git repository. For this, see: ml-git repository config push --help After the projects have been initialized we can continue with the process to create new datasets. To create the specification file for new entities you must run the following commands: # The entity name we are working on project 1 entity_name_1 = 'dataset-ex-1' # The entity name we are working on project 2 entity_name_2 = 'dataset-ex-2' project_1 . create ( entity_type , entity_name_1 , categories = [ 'img' ], mutability = 'strict' ) project_2 . create ( entity_type , entity_name_2 , categories = [ 'img' ], mutability = 'strict' ) INFO - MLGit: Dataset artifact created. INFO - MLGit: Dataset artifact created. In this example, we demonstrated how to work with multiple projects in the ML-Git API. You can use all commands available in the API with this concept of multiple projects, a complete flow of how to version an entity can be found in the Basic Flow Notebook .","title":"Multiple Projects"},{"location":"api/api_scripts/multiple_projects/#multiple-projects","text":"This notebook describes how to work with multiple projects in the ML-Git API.","title":"Multiple Projects"},{"location":"api/api_scripts/multiple_projects/#notebook-state-management","text":"If you have already run this notebook or another in this same folder, it is recommended that you perform a state restart by executing the cell below, because previously performed state changes may interfere with the execution of the notebook. Be aware, that procedure does not affect any remote repository. % cd / api_scripts ! rm - rf ./ logs ! rm - rf . ml - git ! rm - rf ./ datasets ! rm - rf / api_scripts / project_1 ! rm - rf / api_scripts / project_2","title":"Notebook state management"},{"location":"api/api_scripts/multiple_projects/#using-multiples-projects","text":"Let's start creating, for each project, a folder to work with. # importing os module import os os . mkdir ( 'project_1' ) os . mkdir ( 'project_2' ) To start using the ML-Git API, we need to import it into our script. from ml_git.api import MLGitAPI Then we must create a new instance of the API for each project. You can inform the root directory of the project as a parameter. In this scenario, we will be using the relative path to each directory. project_1 = MLGitAPI ( root_path = './project_1' ) project_2 = MLGitAPI ( root_path = './project_2' ) We will consider the scenario of a user who wants to configure their projects from scratch. The first step is to define that the directory we are working on will be an ml-git project. To do that, execute the following command: project_1 . init ( 'repository' ) project_2 . init ( 'repository' ) INFO - Admin: Initialized empty ml-git repository in C:\\Git\\HP\\ml-git\\docs\\api\\api_scripts\\project_1\\.ml-git INFO - Admin: Initialized empty ml-git repository in C:\\Git\\HP\\ml-git\\docs\\api\\api_scripts\\project_2\\.ml-git We will configure each project with a local git repository and a local MinIO as storage. For this, the following commands are necessary: remote_url = '/local_server.git/' bucket_name = 'mlgit' end_point = 'http://127.0.0.1:9000' # The type of entity we are working on entity_type = 'datasets' project_1 . remote_add ( entity_type , remote_url ) project_1 . storage_add ( bucket_name , endpoint_url = end_point ) print ( '/n' ) project_2 . remote_add ( entity_type , remote_url ) project_2 . storage_add ( bucket_name , endpoint_url = end_point ) INFO - Admin: Add remote repository [/local_server.git/] for [datasets] INFO - Repository: When making changes to the config file we strongly recommend that you upload these changes to the Git repository. For this, see: ml-git repository config push --help INFO - Admin: Add storage [s3h://mlgit] INFO - Admin: When making changes to the config file we strongly recommend that you upload these changes to the Git repository. For this, see: ml-git repository config push --help /n INFO - Admin: Add remote repository [/local_server.git/] for [datasets] INFO - Repository: When making changes to the config file we strongly recommend that you upload these changes to the Git repository. For this, see: ml-git repository config push --help INFO - Admin: Add storage [s3h://mlgit] INFO - Admin: When making changes to the config file we strongly recommend that you upload these changes to the Git repository. For this, see: ml-git repository config push --help After the projects have been initialized we can continue with the process to create new datasets. To create the specification file for new entities you must run the following commands: # The entity name we are working on project 1 entity_name_1 = 'dataset-ex-1' # The entity name we are working on project 2 entity_name_2 = 'dataset-ex-2' project_1 . create ( entity_type , entity_name_1 , categories = [ 'img' ], mutability = 'strict' ) project_2 . create ( entity_type , entity_name_2 , categories = [ 'img' ], mutability = 'strict' ) INFO - MLGit: Dataset artifact created. INFO - MLGit: Dataset artifact created. In this example, we demonstrated how to work with multiple projects in the ML-Git API. You can use all commands available in the API with this concept of multiple projects, a complete flow of how to version an entity can be found in the Basic Flow Notebook .","title":"Using multiples projects"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/","text":"ML-Git \u00b6 This notebook describes a basic execution flow of ml-git with its API. In it, we show how to obtain a dataset already versioned by ml-git, how to perform the versioning process of a model and the new data generated, using the MNIST dataset. Notebook state management \u00b6 If you have already run this notebook or another in this same folder, it is recommended that you perform a state restart by executing the cell below, because previously performed state changes may interfere with the execution of the notebook. Be aware, that procedure does not affect any remote repository. % cd / api_scripts / mnist_notebook ! rm - rf ./ logs ! rm - rf . ml - git ! rm - rf ./ datasets ! rm - rf ./ models ! rm - rf ./ labels ! rm - rf . ipynb_checkpoints ! rm - rf . git ! rm - rf . gitignore ! rm - rf ./ local_ml_git_config_server ! ml - git clone '/local_ml_git_config_server.git' ! cp ./ train - images . idx3 - ubyte ./ local_ml_git_config_server / train - images . idx3 - ubyte ! cp ./ train - labels . idx1 - ubyte ./ local_ml_git_config_server / train - labels . idx1 % cd ./ local_ml_git_config_server 1 - The dataset \u00b6 Dataset MNIST is a set of small images of handwritten digits, in the version available in our docker environment, the set has a total of 70,000 images from numbers 0 to 9. Look at the below image which has a few examples instances: 2 - Getting the data \u00b6 To start working with our dataset it is necessary to carry out the checkout command of ml-git in order to bring the data from our storage to the user's workspace. from ml_git.api import MLGitAPI api = MLGitAPI () # def checkout(entity, tag, sampling=None, retries=2, force=False, dataset=False, labels=False, version=-1) api . checkout ( 'labels' , 'labelsmnist' , dataset = True ) mnist_dataset_path = 'datasets/handwritten/digits/mnist/data/' mnist_labels_path = 'labels/handwritten/digits/labelsmnist/data/' INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/labels/metadata] INFO - Metadata: Performing checkout on the entity's lastest tag (handwritten__digits__labelsmnist__1) blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 184blobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 206chunks/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 885files into workspace/s] INFO - Repository: Initializing related datasets download blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 226blobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.24chunks/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 13.2files into workspace/s] Some important points to highlight here are that the tag parameter can be the name of the entity, this way the ml-git will get the latest version available for this entity. With the dataset=True signals that ml-git should look for the dataset associated with these labels Once we have the data in the workspace, we can load it into variables Training data \u00b6 from mlxtend.data import loadlocal_mnist import numpy as np import pickle X_train = pickle . load ( open ( mnist_dataset_path + 'train-images.idx3-ubyte' , 'rb' )) y_train = pickle . load ( open ( mnist_labels_path + 'train-labels.idx1-ubyte' , 'rb' )) print ( 'Training data: ' ) print ( 'Dimensions: %s x %s ' % ( X_train . shape [ 0 ], X_train . shape [ 1 ])) print ( 'Digits: %s ' % np . unique ( y_train )) print ( 'Class distribution: %s ' % np . bincount ( y_train )) Training data: Dimensions: 60000 x 784 Digits: [0 1 2 3 4 5 6 7 8 9] Class distribution: [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949] The training data consists of 60,000 entries of 784 pixels, distributed among the possible values \u200b\u200baccording to the output above. Test data \u00b6 X_test , y_test = loadlocal_mnist ( images_path = mnist_dataset_path + 't10k-images.idx3-ubyte' , labels_path = mnist_labels_path + 't10k-labels.idx1-ubyte' ) print ( 'Test data: ' ) print ( 'Dimensions: %s x %s ' % ( X_test . shape [ 0 ], X_test . shape [ 1 ])) print ( 'Digits: %s ' % np . unique ( y_test )) print ( 'Class distribution: %s ' % np . bincount ( y_test )) Test data: Dimensions: 10000 x 784 Digits: [0 1 2 3 4 5 6 7 8 9] Class distribution: [ 980 1135 1032 1010 982 892 958 1028 974 1009] The test data consists of 10,000 entries of 784 pixels, distributed among the possible values according to the output above. 3 - Training and evaluating \u00b6 Let\u2019s take an example of RandomForest Classifier and train it on the dataset and evaluate it. from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score # Training on the existing dataset rf_clf = RandomForestClassifier ( random_state = 42 ) rf_clf . fit ( X_train , y_train ) # Evaluating the model y_pred = rf_clf . predict ( X_test ) score = accuracy_score ( y_test , y_pred ) print ( 'Accuracy score after training on existing dataset' , score ) Accuracy score after training on existing dataset 0.9705 4 - Versioning our model \u00b6 As we do not have any previously versioned models, it will be necessary to create a new entity. For this we use the following command: # def create(entity, entity_name, categories, mutability, **kwargs) api . create ( 'models' , 'modelmnist' , categories = [ 'handwritten' , 'digits' ], mutability = 'mutable' , bucket_name = 'mlgit' , entity_dir = 'handwritten/digits' ) INFO - MLGit: Model artifact created. Once we have our model trained and evaluated, we will version it with ml-git. For that we need to save it in a file. def save_model ( model ): filename = 'models/handwritten/digits/modelmnist/data/rf_mnist.sav' pickle . dump ( model , open ( filename , 'wb' )) save_model ( rf_clf ) With the file in the workspace we use the following commands to create a version: entity_type = 'models' entity_name = 'modelmnist' # def add(entity_type, entity_name, bumpversion=False, fsck=False, file_path=[]) api . add ( entity_type , entity_name , metric = { 'accuracy' : score }) # def commit(entity, ml_entity_name, commit_message=None, related_dataset=None, related_labels=None) api . commit ( entity_type , entity_name , related_dataset = 'mnist' , related_labels = 'labelsmnist' ) # def push(entity, entity_name, retries=2, clear_on_fail=False) api . push ( entity_type , entity_name ) INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/models/metadata] INFO - Repository: models adding path [/api_scripts/mnist_notebook/models/handwritten/digits/modelmnist] to ml-git index files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 1.56files/s] \u280b Creating hard links in cache\u2819 Creating hard links in cache files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 7.13kfiles/s] \u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest INFO - Local Repository: Associate datasets [mnist]-[handwritten__digits__mnist__1] to the models. INFO - Local Repository: Associate labels [labelsmnist]-[handwritten__digits__labelsmnist__1] to the models. INFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/models/metadata] --- file[handwritten/digits/modelmnist] files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 520/520 [00:06<00:00, 82.0files/s] \u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository 5 - Adding new data \u00b6 At some point after training a model it may be the case that new data is available. It is interesting that this new data is added to our entity to generate a second version of our dataset. Let's add this data to our entity's directory: ! cp train - images . idx3 - ubyte datasets / handwritten / digits / mnist / data /. ! cp train - labels . idx1 - ubyte labels / handwritten / digits / labelsmnist / data /. Let's take a look at our new dataset # loading the dataset X_train = pickle . load ( open ( mnist_dataset_path + 'train-images.idx3-ubyte' , 'rb' )) y_train = pickle . load ( open ( mnist_labels_path + 'train-labels.idx1-ubyte' , 'rb' )) print ( 'Test data: ' ) print ( 'Dimensions: %s x %s ' % ( X_train . shape [ 0 ], X_train . shape [ 1 ])) print ( 'Digits: %s ' % np . unique ( y_train )) print ( 'Class distribution: %s ' % np . bincount ( y_train )) Test data: Dimensions: 180000 x 784 Digits: [0 1 2 3 4 5 6 7 8 9] Class distribution: [17769 20226 17874 18393 17526 16263 17754 18795 17553 17847] The train data now consists of 180,000 entries of 784 pixels, distributed among the possible values according to the output above. 6 - Versioning the dataset and labels with the new entries \u00b6 dataset_file = 'datasets/handwritten/digits/mnist/data/train-images.idx3-ubyte' pickle . dump ( X_train , open ( dataset_file , 'wb' )) labels_file = 'labels/handwritten/digits/labelsmnist/data/train-labels.idx1-ubyte' pickle . dump ( y_train , open ( labels_file , 'wb' )) Versioning the dataset \u00b6 entity_type = 'datasets' entity_name = 'mnist' api . add ( entity_type , entity_name , bumpversion = True ) api . commit ( entity_type , entity_name ) api . push ( entity_type , entity_name ) INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/datasets/metadata] INFO - Repository: datasets adding path [/api_scripts/mnist_notebook/datasets/handwritten/digits/mnist] to ml-git index files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:05<00:00, 2.63s/files] 'zdj7WmvrKwjWMCQMFFgcMFmGxEgQS2uXatYWxcE3ByNJeDmze' \u280b Creating hard links in cache\u2819 Creating hard links in cache files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.24kfiles/s] \u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest INFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/datasets/metadata] --- file[handwritten/digits/mnist] files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 540/540 [00:17<00:00, 31.0files/s] \u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository Versioning the labels \u00b6 entity_type = 'labels' entity_name = 'labelsmnist' api . add ( entity_type , entity_name , bumpversion = True ) api . commit ( entity_type , entity_name , related_dataset = 'mnist' ) api . push ( entity_type , entity_name ) INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/labels/metadata] INFO - Repository: labels adding path [/api_scripts/mnist_notebook/labels/handwritten/digits/labelsmnist] to ml-git index files: 0.00files [00:00, ?files/s] 'zdj7WaeJerH7ACFKfFgh9itrCoWt3iBCYagzYS2M7VhidBAmR' files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 227files/s] \u280b Creating hard links in cache\u2819 Creating hard links in cache files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 8.93kfiles/s] \u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest INFO - Local Repository: Associate datasets [mnist]-[handwritten__digits__mnist__2] to the labels. INFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/labels/metadata] --- file[handwritten/digits/labelsmnist] files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 71.8files/s] \u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository 7 - Training and evaluating \u00b6 # Training on new data rf_clf = RandomForestClassifier ( random_state = 42 ) rf_clf . fit ( X_train , y_train ) # Evaluating the model y_pred = rf_clf . predict ( X_test ) score = accuracy_score ( y_test , y_pred ) print ( \"Accuracy score after training on augmented dataset\" , score ) Accuracy score after training on augmented dataset 0.9746 We\u2019ve improved the accuracy by ~0.4%. This is great. 8 - Versioning our model \u00b6 save_model ( rf_clf ) entity_type = 'models' entity_name = 'modelmnist' api . add ( entity_type , entity_name , bumpversion = True , metric = { 'accuracy' : score }) api . commit ( entity_type , entity_name , related_dataset = 'mnist' , related_labels = 'labelsmnist' ) api . push ( entity_type , entity_name ) INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/models/metadata] INFO - Repository: models adding path [/api_scripts/mnist_notebook/models/handwritten/digits/modelmnist] to ml-git index files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:04<00:00, 4.46s/files] 'zdj7WfjwEm3XVzoggyG1kSPQhYMSMpbBdLzYMx5Z61StSbu5H' \u280b Creating hard links in cache\u2819 Creating hard links in cache files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 6.36kfiles/s] \u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest INFO - Local Repository: Associate datasets [mnist]-[handwritten__digits__mnist__2] to the models. INFO - Local Repository: Associate labels [labelsmnist]-[handwritten__digits__labelsmnist__2] to the models. INFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/models/metadata] --- file[handwritten/digits/modelmnist] files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.39k/1.39k [00:22<00:00, 62.1files/s] \u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository\u2839 Pushing metadata to the git repository 9 - Reproducing our experiment with ml-git \u00b6 Once the experiment data is versioned, it is common that it is necessary to re-evaluate the result, or that someone else wants to see the result of an already trained model. For this, we will perform the model checkout in version 1 (without the data augmentation), to get the test data and the trained model. mnist_dataset_path = 'datasets/handwritten/digits/mnist/data/' mnist_labels_path = 'labels/handwritten/digits/labelsmnist/data/' mnist_model_path = 'models/handwritten/digits/modelmnist/data/' api . checkout ( 'models' , 'handwritten__digits__modelmnist__1' , dataset = True , labels = True ) # Getting test data X_test , y_test = loadlocal_mnist ( images_path = mnist_dataset_path + 't10k-images.idx3-ubyte' , labels_path = mnist_labels_path + 't10k-labels.idx1-ubyte' ) INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/models/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 95.1blobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 3.44chunks/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:14<00:00, 14.1s/files into workspace] INFO - Repository: Initializing related datasets download blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 228blobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 20.6chunks/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:04<00:00, 2.15s/files into workspace] INFO - Repository: Initializing related labels download blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 810blobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 234chunks/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 761files into workspace/s] With the test data in hand, let's upload the model and evaluate it for our dataset. loaded_model = pickle . load ( open ( mnist_model_path + 'rf_mnist.sav' , 'rb' )) y_pred = loaded_model . predict ( X_test ) score = accuracy_score ( y_test , y_pred ) print ( 'Accuracy score for version 1: ' , score ) Accuracy score for version 1: 0.9705 Now let's take the model from the version 2 (model trained with more data) and evaluate it for the test set. api . checkout ( 'models' , 'handwritten__digits__modelmnist__2' ) loaded_model = pickle . load ( open ( mnist_model_path + 'rf_mnist.sav' , 'rb' )) y_pred = loaded_model . predict ( X_test ) score = accuracy_score ( y_test , y_pred ) print ( 'Accuracy score for version 2: ' , score ) INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/models/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 1.96kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 18.4chunks/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:05<00:00, 5.36s/files into workspace] Accuracy score for version 2: 0.9746 In a quick and practical way it was possible to obtain the models generated in the experiments and to evaluate them again. Conclusions \u00b6 At the end of this execution we have two versions of each entity. If someone else wants to replicate this experiment, they can check out the model with the related dataset and labels. info_data = api . get_models_metrics ( 'modelmnist' , export_type = 'csv' ) import pandas as pd info_table = pd . read_csv ( info_data ) # Displays whole table info_table .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Tag Related dataset - (version) Related labels - (version) accuracy 0 2021-03-25 14:52:42 handwritten__digits__modelmnist__1 mnist - (1) labelsmnist - (1) 0.9705 1 2021-03-25 14:55:52 handwritten__digits__modelmnist__2 mnist - (2) labelsmnist - (2) 0.9746","title":"MNIST Random Forest API"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#ml-git","text":"This notebook describes a basic execution flow of ml-git with its API. In it, we show how to obtain a dataset already versioned by ml-git, how to perform the versioning process of a model and the new data generated, using the MNIST dataset.","title":"ML-Git"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#notebook-state-management","text":"If you have already run this notebook or another in this same folder, it is recommended that you perform a state restart by executing the cell below, because previously performed state changes may interfere with the execution of the notebook. Be aware, that procedure does not affect any remote repository. % cd / api_scripts / mnist_notebook ! rm - rf ./ logs ! rm - rf . ml - git ! rm - rf ./ datasets ! rm - rf ./ models ! rm - rf ./ labels ! rm - rf . ipynb_checkpoints ! rm - rf . git ! rm - rf . gitignore ! rm - rf ./ local_ml_git_config_server ! ml - git clone '/local_ml_git_config_server.git' ! cp ./ train - images . idx3 - ubyte ./ local_ml_git_config_server / train - images . idx3 - ubyte ! cp ./ train - labels . idx1 - ubyte ./ local_ml_git_config_server / train - labels . idx1 % cd ./ local_ml_git_config_server","title":"Notebook state management"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#1-the-dataset","text":"Dataset MNIST is a set of small images of handwritten digits, in the version available in our docker environment, the set has a total of 70,000 images from numbers 0 to 9. Look at the below image which has a few examples instances:","title":"1 - The dataset"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#2-getting-the-data","text":"To start working with our dataset it is necessary to carry out the checkout command of ml-git in order to bring the data from our storage to the user's workspace. from ml_git.api import MLGitAPI api = MLGitAPI () # def checkout(entity, tag, sampling=None, retries=2, force=False, dataset=False, labels=False, version=-1) api . checkout ( 'labels' , 'labelsmnist' , dataset = True ) mnist_dataset_path = 'datasets/handwritten/digits/mnist/data/' mnist_labels_path = 'labels/handwritten/digits/labelsmnist/data/' INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/labels/metadata] INFO - Metadata: Performing checkout on the entity's lastest tag (handwritten__digits__labelsmnist__1) blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 184blobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 206chunks/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 885files into workspace/s] INFO - Repository: Initializing related datasets download blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 226blobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.24chunks/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 13.2files into workspace/s] Some important points to highlight here are that the tag parameter can be the name of the entity, this way the ml-git will get the latest version available for this entity. With the dataset=True signals that ml-git should look for the dataset associated with these labels Once we have the data in the workspace, we can load it into variables","title":"2 - Getting the data"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#training-data","text":"from mlxtend.data import loadlocal_mnist import numpy as np import pickle X_train = pickle . load ( open ( mnist_dataset_path + 'train-images.idx3-ubyte' , 'rb' )) y_train = pickle . load ( open ( mnist_labels_path + 'train-labels.idx1-ubyte' , 'rb' )) print ( 'Training data: ' ) print ( 'Dimensions: %s x %s ' % ( X_train . shape [ 0 ], X_train . shape [ 1 ])) print ( 'Digits: %s ' % np . unique ( y_train )) print ( 'Class distribution: %s ' % np . bincount ( y_train )) Training data: Dimensions: 60000 x 784 Digits: [0 1 2 3 4 5 6 7 8 9] Class distribution: [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949] The training data consists of 60,000 entries of 784 pixels, distributed among the possible values \u200b\u200baccording to the output above.","title":"Training data"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#test-data","text":"X_test , y_test = loadlocal_mnist ( images_path = mnist_dataset_path + 't10k-images.idx3-ubyte' , labels_path = mnist_labels_path + 't10k-labels.idx1-ubyte' ) print ( 'Test data: ' ) print ( 'Dimensions: %s x %s ' % ( X_test . shape [ 0 ], X_test . shape [ 1 ])) print ( 'Digits: %s ' % np . unique ( y_test )) print ( 'Class distribution: %s ' % np . bincount ( y_test )) Test data: Dimensions: 10000 x 784 Digits: [0 1 2 3 4 5 6 7 8 9] Class distribution: [ 980 1135 1032 1010 982 892 958 1028 974 1009] The test data consists of 10,000 entries of 784 pixels, distributed among the possible values according to the output above.","title":"Test data"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#3-training-and-evaluating","text":"Let\u2019s take an example of RandomForest Classifier and train it on the dataset and evaluate it. from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score # Training on the existing dataset rf_clf = RandomForestClassifier ( random_state = 42 ) rf_clf . fit ( X_train , y_train ) # Evaluating the model y_pred = rf_clf . predict ( X_test ) score = accuracy_score ( y_test , y_pred ) print ( 'Accuracy score after training on existing dataset' , score ) Accuracy score after training on existing dataset 0.9705","title":"3 - Training and evaluating"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#4-versioning-our-model","text":"As we do not have any previously versioned models, it will be necessary to create a new entity. For this we use the following command: # def create(entity, entity_name, categories, mutability, **kwargs) api . create ( 'models' , 'modelmnist' , categories = [ 'handwritten' , 'digits' ], mutability = 'mutable' , bucket_name = 'mlgit' , entity_dir = 'handwritten/digits' ) INFO - MLGit: Model artifact created. Once we have our model trained and evaluated, we will version it with ml-git. For that we need to save it in a file. def save_model ( model ): filename = 'models/handwritten/digits/modelmnist/data/rf_mnist.sav' pickle . dump ( model , open ( filename , 'wb' )) save_model ( rf_clf ) With the file in the workspace we use the following commands to create a version: entity_type = 'models' entity_name = 'modelmnist' # def add(entity_type, entity_name, bumpversion=False, fsck=False, file_path=[]) api . add ( entity_type , entity_name , metric = { 'accuracy' : score }) # def commit(entity, ml_entity_name, commit_message=None, related_dataset=None, related_labels=None) api . commit ( entity_type , entity_name , related_dataset = 'mnist' , related_labels = 'labelsmnist' ) # def push(entity, entity_name, retries=2, clear_on_fail=False) api . push ( entity_type , entity_name ) INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/models/metadata] INFO - Repository: models adding path [/api_scripts/mnist_notebook/models/handwritten/digits/modelmnist] to ml-git index files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 1.56files/s] \u280b Creating hard links in cache\u2819 Creating hard links in cache files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 7.13kfiles/s] \u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest INFO - Local Repository: Associate datasets [mnist]-[handwritten__digits__mnist__1] to the models. INFO - Local Repository: Associate labels [labelsmnist]-[handwritten__digits__labelsmnist__1] to the models. INFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/models/metadata] --- file[handwritten/digits/modelmnist] files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 520/520 [00:06<00:00, 82.0files/s] \u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository","title":"4 - Versioning our model"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#5-adding-new-data","text":"At some point after training a model it may be the case that new data is available. It is interesting that this new data is added to our entity to generate a second version of our dataset. Let's add this data to our entity's directory: ! cp train - images . idx3 - ubyte datasets / handwritten / digits / mnist / data /. ! cp train - labels . idx1 - ubyte labels / handwritten / digits / labelsmnist / data /. Let's take a look at our new dataset # loading the dataset X_train = pickle . load ( open ( mnist_dataset_path + 'train-images.idx3-ubyte' , 'rb' )) y_train = pickle . load ( open ( mnist_labels_path + 'train-labels.idx1-ubyte' , 'rb' )) print ( 'Test data: ' ) print ( 'Dimensions: %s x %s ' % ( X_train . shape [ 0 ], X_train . shape [ 1 ])) print ( 'Digits: %s ' % np . unique ( y_train )) print ( 'Class distribution: %s ' % np . bincount ( y_train )) Test data: Dimensions: 180000 x 784 Digits: [0 1 2 3 4 5 6 7 8 9] Class distribution: [17769 20226 17874 18393 17526 16263 17754 18795 17553 17847] The train data now consists of 180,000 entries of 784 pixels, distributed among the possible values according to the output above.","title":"5 - Adding new data"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#6-versioning-the-dataset-and-labels-with-the-new-entries","text":"dataset_file = 'datasets/handwritten/digits/mnist/data/train-images.idx3-ubyte' pickle . dump ( X_train , open ( dataset_file , 'wb' )) labels_file = 'labels/handwritten/digits/labelsmnist/data/train-labels.idx1-ubyte' pickle . dump ( y_train , open ( labels_file , 'wb' ))","title":"6 - Versioning the dataset and labels with the new entries"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#versioning-the-dataset","text":"entity_type = 'datasets' entity_name = 'mnist' api . add ( entity_type , entity_name , bumpversion = True ) api . commit ( entity_type , entity_name ) api . push ( entity_type , entity_name ) INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/datasets/metadata] INFO - Repository: datasets adding path [/api_scripts/mnist_notebook/datasets/handwritten/digits/mnist] to ml-git index files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:05<00:00, 2.63s/files] 'zdj7WmvrKwjWMCQMFFgcMFmGxEgQS2uXatYWxcE3ByNJeDmze' \u280b Creating hard links in cache\u2819 Creating hard links in cache files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.24kfiles/s] \u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest INFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/datasets/metadata] --- file[handwritten/digits/mnist] files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 540/540 [00:17<00:00, 31.0files/s] \u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository","title":"Versioning the dataset"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#versioning-the-labels","text":"entity_type = 'labels' entity_name = 'labelsmnist' api . add ( entity_type , entity_name , bumpversion = True ) api . commit ( entity_type , entity_name , related_dataset = 'mnist' ) api . push ( entity_type , entity_name ) INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/labels/metadata] INFO - Repository: labels adding path [/api_scripts/mnist_notebook/labels/handwritten/digits/labelsmnist] to ml-git index files: 0.00files [00:00, ?files/s] 'zdj7WaeJerH7ACFKfFgh9itrCoWt3iBCYagzYS2M7VhidBAmR' files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 227files/s] \u280b Creating hard links in cache\u2819 Creating hard links in cache files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 8.93kfiles/s] \u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest INFO - Local Repository: Associate datasets [mnist]-[handwritten__digits__mnist__2] to the labels. INFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/labels/metadata] --- file[handwritten/digits/labelsmnist] files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 71.8files/s] \u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository","title":"Versioning the labels"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#7-training-and-evaluating","text":"# Training on new data rf_clf = RandomForestClassifier ( random_state = 42 ) rf_clf . fit ( X_train , y_train ) # Evaluating the model y_pred = rf_clf . predict ( X_test ) score = accuracy_score ( y_test , y_pred ) print ( \"Accuracy score after training on augmented dataset\" , score ) Accuracy score after training on augmented dataset 0.9746 We\u2019ve improved the accuracy by ~0.4%. This is great.","title":"7 - Training and evaluating"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#8-versioning-our-model","text":"save_model ( rf_clf ) entity_type = 'models' entity_name = 'modelmnist' api . add ( entity_type , entity_name , bumpversion = True , metric = { 'accuracy' : score }) api . commit ( entity_type , entity_name , related_dataset = 'mnist' , related_labels = 'labelsmnist' ) api . push ( entity_type , entity_name ) INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/models/metadata] INFO - Repository: models adding path [/api_scripts/mnist_notebook/models/handwritten/digits/modelmnist] to ml-git index files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:04<00:00, 4.46s/files] 'zdj7WfjwEm3XVzoggyG1kSPQhYMSMpbBdLzYMx5Z61StSbu5H' \u280b Creating hard links in cache\u2819 Creating hard links in cache files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 6.36kfiles/s] \u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest INFO - Local Repository: Associate datasets [mnist]-[handwritten__digits__mnist__2] to the models. INFO - Local Repository: Associate labels [labelsmnist]-[handwritten__digits__labelsmnist__2] to the models. INFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/models/metadata] --- file[handwritten/digits/modelmnist] files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.39k/1.39k [00:22<00:00, 62.1files/s] \u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository\u2839 Pushing metadata to the git repository","title":"8 - Versioning our model"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#9-reproducing-our-experiment-with-ml-git","text":"Once the experiment data is versioned, it is common that it is necessary to re-evaluate the result, or that someone else wants to see the result of an already trained model. For this, we will perform the model checkout in version 1 (without the data augmentation), to get the test data and the trained model. mnist_dataset_path = 'datasets/handwritten/digits/mnist/data/' mnist_labels_path = 'labels/handwritten/digits/labelsmnist/data/' mnist_model_path = 'models/handwritten/digits/modelmnist/data/' api . checkout ( 'models' , 'handwritten__digits__modelmnist__1' , dataset = True , labels = True ) # Getting test data X_test , y_test = loadlocal_mnist ( images_path = mnist_dataset_path + 't10k-images.idx3-ubyte' , labels_path = mnist_labels_path + 't10k-labels.idx1-ubyte' ) INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/models/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 95.1blobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 3.44chunks/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:14<00:00, 14.1s/files into workspace] INFO - Repository: Initializing related datasets download blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 228blobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 20.6chunks/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:04<00:00, 2.15s/files into workspace] INFO - Repository: Initializing related labels download blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 810blobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 234chunks/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 761files into workspace/s] With the test data in hand, let's upload the model and evaluate it for our dataset. loaded_model = pickle . load ( open ( mnist_model_path + 'rf_mnist.sav' , 'rb' )) y_pred = loaded_model . predict ( X_test ) score = accuracy_score ( y_test , y_pred ) print ( 'Accuracy score for version 1: ' , score ) Accuracy score for version 1: 0.9705 Now let's take the model from the version 2 (model trained with more data) and evaluate it for the test set. api . checkout ( 'models' , 'handwritten__digits__modelmnist__2' ) loaded_model = pickle . load ( open ( mnist_model_path + 'rf_mnist.sav' , 'rb' )) y_pred = loaded_model . predict ( X_test ) score = accuracy_score ( y_test , y_pred ) print ( 'Accuracy score for version 2: ' , score ) INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/models/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 1.96kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 18.4chunks/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:05<00:00, 5.36s/files into workspace] Accuracy score for version 2: 0.9746 In a quick and practical way it was possible to obtain the models generated in the experiments and to evaluate them again.","title":" 9 - Reproducing our experiment with ml-git"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#conclusions","text":"At the end of this execution we have two versions of each entity. If someone else wants to replicate this experiment, they can check out the model with the related dataset and labels. info_data = api . get_models_metrics ( 'modelmnist' , export_type = 'csv' ) import pandas as pd info_table = pd . read_csv ( info_data ) # Displays whole table info_table .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Tag Related dataset - (version) Related labels - (version) accuracy 0 2021-03-25 14:52:42 handwritten__digits__modelmnist__1 mnist - (1) labelsmnist - (1) 0.9705 1 2021-03-25 14:55:52 handwritten__digits__modelmnist__2 mnist - (2) labelsmnist - (2) 0.9746","title":"Conclusions"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/","text":"ML-Git \u00b6 This notebook describes a basic execution flow with ml-git. In it, we show how to obtain a dataset already versioned by ml-git, how to perform the versioning process of a model and the new data generated, using the MNIST dataset. Notebook state management \u00b6 If you have already run this notebook or another in this same folder, it is recommended that you perform a state restart by executing the cell below, because previously performed state changes may interfere with the execution of the notebook. Be aware, that procedure does not affect any remote repository. % cd / api_scripts / mnist_notebook ! rm - rf ./ logs ! rm - rf . ml - git ! rm - rf ./ datasets ! rm - rf ./ models ! rm - rf ./ labels ! rm - rf . ipynb_checkpoints ! rm - rf . git ! rm - rf . gitignore ! rm - rf ./ local_ml_git_config_server ! ml - git clone '/local_ml_git_config_server.git' ! cp ./ train - images . idx3 - ubyte ./ local_ml_git_config_server / train - images . idx3 - ubyte ! cp ./ train - labels . idx1 - ubyte ./ local_ml_git_config_server / train - labels . idx1 % cd ./ local_ml_git_config_server 1 - The dataset \u00b6 Dataset MNIST is a set of small images of handwritten digits, in the version available in our docker environment, the set has a total of 70,000 images from numbers 0 to 9. Look at the below image which has a few examples instances: 2 - Getting the data \u00b6 To start working with our dataset it is necessary to carry out the checkout command of ml-git in order to bring the data from our storage to the user's workspace. ! ml - git labels checkout labelsmnist - d mnist_dataset_path = 'datasets/handwritten/digits/mnist/data/' mnist_labels_path = 'labels/handwritten/digits/labelsmnist/data/' INFO - Metadata: Performing checkout on the entity's lastest tag (handwritten__digits__labelsmnist__1) blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 198blobs/s]\u001b[0m\u001b[0m chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 207chunks/s]\u001b[0m\u001b[0m files into workspace: 100%|\u2588| 2.00/2.00 [00:00<00:00, 919files into workspace/s]\u001b[0m\u001b[0m \u001b[0mINFO - Repository: Initializing related datasets download blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 216blobs/s]\u001b[0m\u001b[0m chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.31chunks/s]\u001b[0m\u001b[0m files into workspace: 100%|\u2588| 2.00/2.00 [00:00<00:00, 12.9files into workspace/s]\u001b[0m[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0m Some important points to highlight here are that the tag parameter can be the name of the entity, this way the ml-git will get the latest version available for this entity. With the -d signals that ml-git should look for the dataset associated with these labels Once we have the data in the workspace, we can load it into variables Training data \u00b6 from mlxtend.data import loadlocal_mnist import numpy as np import pickle X_train = pickle . load ( open ( mnist_dataset_path + 'train-images.idx3-ubyte' , 'rb' )) y_train = pickle . load ( open ( mnist_labels_path + 'train-labels.idx1-ubyte' , 'rb' )) print ( 'Training data: ' ) print ( 'Dimensions: %s x %s ' % ( X_train . shape [ 0 ], X_train . shape [ 1 ])) print ( 'Digits: %s ' % np . unique ( y_train )) print ( 'Class distribution: %s ' % np . bincount ( y_train )) Training data: Dimensions: 60000 x 784 Digits: [0 1 2 3 4 5 6 7 8 9] Class distribution: [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949] The training data consists of 60,000 entries of 784 pixels, distributed among the possible values \u200b\u200baccording to the output above. Test data \u00b6 X_test , y_test = loadlocal_mnist ( images_path = mnist_dataset_path + 't10k-images.idx3-ubyte' , labels_path = mnist_labels_path + 't10k-labels.idx1-ubyte' ) print ( 'Test data: ' ) print ( 'Dimensions: %s x %s ' % ( X_test . shape [ 0 ], X_test . shape [ 1 ])) print ( 'Digits: %s ' % np . unique ( y_test )) print ( 'Class distribution: %s ' % np . bincount ( y_test )) Test data: Dimensions: 10000 x 784 Digits: [0 1 2 3 4 5 6 7 8 9] Class distribution: [ 980 1135 1032 1010 982 892 958 1028 974 1009] The test data consists of 10,000 entries of 784 pixels, distributed among the possible values according to the output above. 3 - Training and evaluating \u00b6 Let\u2019s take an example of RandomForest Classifier and train it on the dataset and evaluate it. from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score # Training on the existing dataset rf_clf = RandomForestClassifier ( random_state = 42 ) rf_clf . fit ( X_train , y_train ) # Evaluating the model y_pred = rf_clf . predict ( X_test ) score = accuracy_score ( y_test , y_pred ) print ( 'Accuracy score after training on existing dataset' , score ) Accuracy score after training on existing dataset 0.9705 4 - Versioning our model \u00b6 As we do not have any previously versioned models, it will be necessary to create a new entity. For this we use the following command: ! ml - git models create modelmnist -- categories = \"handwritten, digits\" -- bucket - name = mlgit -- mutability = mutable -- entity - dir = 'handwritten/digits' INFO - MLGit: Model artifact created. \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0m Once we have our model trained and evaluated, we will version it with ml-git. For that we need to save it in a file. def save_model ( model ): filename = 'models/handwritten/digits/modelmnist/data/rf_mnist.sav' pickle . dump ( model , open ( filename , 'wb' )) save_model ( rf_clf ) With the file in the workspace we use the following commands to create a version: ! ml - git models add modelmnist -- metric accuracy $ score ! ml - git models commit modelmnist -- dataset = mnist -- labels = labelsmnist ! ml - git models push modelmnist INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/models/metadata] \u001b[0mINFO - Repository: models adding path [/api_scripts/mnist_notebook/models/handwritten/digits/modelmnist] to ml-git index files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 2.14files/s]\u001b[0m files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 6.44kfiles/s]\u001b[0m\u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0mINFO - Local Repository: Associate datasets [mnist]-[handwritten__digits__mnist__1] to the models. \u001b[0mINFO - Local Repository: Associate labels [labelsmnist]-[handwritten__digits__labelsmnist__1] to the models. \u001b[0mINFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/models/metadata] --- file[handwritten/digits/modelmnist] files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 520/520 [00:06<00:00, 84.8files/s]\u001b[0m\u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0mtadata to the git repository\u001b[0m\u001b[0m 5 - Adding new data \u00b6 At some point after training a model it may be the case that new data is available. It is interesting that this new data is added to our entity to generate a second version of our dataset. Let's add this data to our entity's directory: ! cp train - images . idx3 - ubyte datasets / handwritten / digits / mnist / data /. ! cp train - labels . idx1 - ubyte labels / handwritten / digits / labelsmnist / data /. Let's take a look at our new dataset # loading the dataset X_train = pickle . load ( open ( mnist_dataset_path + 'train-images.idx3-ubyte' , 'rb' )) y_train = pickle . load ( open ( mnist_labels_path + 'train-labels.idx1-ubyte' , 'rb' )) print ( 'Test data: ' ) print ( 'Dimensions: %s x %s ' % ( X_train . shape [ 0 ], X_train . shape [ 1 ])) print ( 'Digits: %s ' % np . unique ( y_train )) print ( 'Class distribution: %s ' % np . bincount ( y_train )) Test data: Dimensions: 180000 x 784 Digits: [0 1 2 3 4 5 6 7 8 9] Class distribution: [17769 20226 17874 18393 17526 16263 17754 18795 17553 17847] The train data now consists of 180,000 entries of 784 pixels, distributed among the possible values according to the output above. 6 - Versioning the dataset and labels with the new entries \u00b6 dataset_file = 'datasets/handwritten/digits/mnist/data/train-images.idx3-ubyte' pickle . dump ( X_train , open ( dataset_file , 'wb' )) labels_file = 'labels/handwritten/digits/labelsmnist/data/train-labels.idx1-ubyte' pickle . dump ( y_train , open ( labels_file , 'wb' )) Versioning the dataset \u00b6 ! ml - git datasets add mnist -- bumpversion ! ml - git datasets commit mnist ! ml - git datasets push mnist INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/datasets/metadata] \u001b[0mINFO - Repository: datasets adding path [/api_scripts/mnist_notebook/datasets/handwritten/digits/mnist] to ml-git index files: 0.00files [00:00, ?files/s]\u001b[0m'zdj7WmvrKwjWMCQMFFgcMFmGxEgQS2uXatYWxcE3ByNJeDmze'\u001b[0m files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.10files/s]\u001b[0m files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 13.3kfiles/s]\u001b[0m\u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0mINFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/datasets/metadata] --- file[handwritten/digits/mnist] files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 540/540 [00:06<00:00, 86.2files/s]\u001b[0m\u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0mtadata to the git repository\u001b[0m\u001b[0m Versioning the labels \u00b6 ! ml - git labels add labelsmnist -- bumpversion ! ml - git labels commit labelsmnist -- dataset = mnist ! ml - git labels push labelsmnist INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/labels/metadata] \u001b[0mINFO - Repository: labels adding path [/api_scripts/mnist_notebook/labels/handwritten/digits/labelsmnist] to ml-git index files: 0.00files [00:00, ?files/s]\u001b[0m'zdj7WaeJerH7ACFKfFgh9itrCoWt3iBCYagzYS2M7VhidBAmR'\u001b[0m files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 410files/s]\u001b[0m files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 12.7kfiles/s]\u001b[0m\u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0mINFO - Local Repository: Associate datasets [mnist]-[handwritten__digits__mnist__2] to the labels. \u001b[0mINFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/labels/metadata] --- file[handwritten/digits/labelsmnist] files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 64.7files/s]\u001b[0m\u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0mtadata to the git repository\u001b[0m\u001b[0m 7 - Training and evaluating \u00b6 # Training on new data rf_clf = RandomForestClassifier ( random_state = 42 ) rf_clf . fit ( X_train , y_train ) # Evaluating the model y_pred = rf_clf . predict ( X_test ) score = accuracy_score ( y_test , y_pred ) print ( \"Accuracy score after training on augmented dataset\" , score ) Accuracy score after training on augmented dataset 0.9746 We\u2019ve improved the accuracy by ~0.4%. This is great. 8 - Versioning our model \u00b6 save_model ( rf_clf ) ! ml - git models add modelmnist -- bumpversion -- metric accuracy $ score ! ml - git models commit modelmnist -- dataset = mnist -- labels = labelsmnist ! ml - git models push modelmnist INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/models/metadata] \u001b[0mINFO - Repository: models adding path [/api_scripts/mnist_notebook/models/handwritten/digits/modelmnist] to ml-git index files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:05<00:00, 5.41s/files]\u001b[0m'zdj7WfjwEm3XVzoggyG1kSPQhYMSMpbBdLzYMx5Z61StSbu5H'\u001b[0m \u001b[0m files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 9.71kfiles/s]\u001b[0m\u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0mINFO - Local Repository: Associate datasets [mnist]-[handwritten__digits__mnist__2] to the models. \u001b[0mINFO - Local Repository: Associate labels [labelsmnist]-[handwritten__digits__labelsmnist__2] to the models. \u001b[0mINFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/models/metadata] --- file[handwritten/digits/modelmnist] files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.39k/1.39k [00:36<00:00, 38.0files/s]\u001b[0m\u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0mtadata to the git repository\u001b[0m\u001b[0m 9 - Reproducing our experiment with ml-git \u00b6 Once the experiment data is versioned, it is common that it is necessary to re-evaluate the result, or that someone else wants to see the result of an already trained model. For this, we will perform the model checkout in version 1, to get the test data and the trained model. mnist_dataset_path = 'datasets/handwritten/digits/mnist/data/' mnist_labels_path = 'labels/handwritten/digits/labelsmnist/data/' mnist_model_path = 'models/handwritten/digits/modelmnist/data/' ! ml - git models checkout modelmnist -- version = 1 - d - l # Getting test data X_test , y_test = loadlocal_mnist ( images_path = mnist_dataset_path + 't10k-images.idx3-ubyte' , labels_path = mnist_labels_path + 't10k-labels.idx1-ubyte' ) INFO - Metadata: Performing checkout in tag handwritten__digits__modelmnist__1 blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 912blobs/s]\u001b[0m\u001b[0m chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 3.59chunks/s]\u001b[0m\u001b[0m files into workspace: 100%|\u2588| 1.00/1.00 [00:01<00:00, 1.60s/files into workspace]\u001b[0m\u001b[0m \u001b[0mINFO - Repository: Initializing related datasets download blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.05kblobs/s]\u001b[0m\u001b[0m chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 17.4chunks/s]\u001b[0m\u001b[0m files into workspace: 100%|\u2588| 2.00/2.00 [00:00<00:00, 3.17files into workspace/s]\u001b[0m[0m \u001b[0mINFO - Repository: Initializing related labels download blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.19kblobs/s]\u001b[0m\u001b[0m chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 625chunks/s]\u001b[0m\u001b[0m files into workspace: 100%|\u2588| 2.00/2.00 [00:00<00:00, 806files into workspace/s]\u001b[0m\u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0m With the test data in hand, let's upload the model and evaluate it for our dataset. loaded_model = pickle . load ( open ( mnist_model_path + 'rf_mnist.sav' , 'rb' )) y_pred = loaded_model . predict ( X_test ) score = accuracy_score ( y_test , y_pred ) print ( 'Accuracy score for version 1: ' , score ) Accuracy score for version 1: 0.9705 Now let's take the model from the version 2 (model trained with more data) and evaluate it for the test set. ! ml - git models checkout modelmnist -- version = 2 loaded_model = pickle . load ( open ( mnist_model_path + 'rf_mnist.sav' , 'rb' )) y_pred = loaded_model . predict ( X_test ) score = accuracy_score ( y_test , y_pred ) print ( 'Accuracy score for version 2: ' , score ) INFO - Metadata: Performing checkout in tag handwritten__digits__modelmnist__2 blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 1.70kblobs/s]\u001b[0m\u001b[0m chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 11.1chunks/s]\u001b[0m\u001b[0m files into workspace: 100%|\u2588| 1.00/1.00 [00:04<00:00, 4.32s/files into workspace]\u001b[0m\u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0mAccuracy score for version 2: 0.9746 In a quick and practical way it was possible to obtain the models generated in the experiments and to evaluate them again. Conclusions \u00b6 At the end of this execution we have two versions of each entity. If someone else wants to replicate this experiment, they can check out the model with the related dataset and labels. ! ml - git models metrics modelmnist Tag: handwritten__digits__modelmnist__1 +-----------------------------+---------------------+ | Name | Value | +-----------------------------+---------------------+ | Date | 2021-03-25 13:54:04 | | Related dataset - (version) | mnist - (1) | | Related labels - (version) | labelsmnist - (1) | | accuracy | 0.9705 | +-----------------------------+---------------------+ \u001b[0m \u001b[0mTag: handwritten__digits__modelmnist__2 +-----------------------------+---------------------+ | Name | Value | +-----------------------------+---------------------+ | Date | 2021-03-25 13:56:48 | | Related dataset - (version) | mnist - (2) | | Related labels - (version) | labelsmnist - (2) | | accuracy | 0.9746 | +-----------------------------+---------------------+ \u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0m","title":"MNIST Random Forest CLI"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#ml-git","text":"This notebook describes a basic execution flow with ml-git. In it, we show how to obtain a dataset already versioned by ml-git, how to perform the versioning process of a model and the new data generated, using the MNIST dataset.","title":"ML-Git"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#notebook-state-management","text":"If you have already run this notebook or another in this same folder, it is recommended that you perform a state restart by executing the cell below, because previously performed state changes may interfere with the execution of the notebook. Be aware, that procedure does not affect any remote repository. % cd / api_scripts / mnist_notebook ! rm - rf ./ logs ! rm - rf . ml - git ! rm - rf ./ datasets ! rm - rf ./ models ! rm - rf ./ labels ! rm - rf . ipynb_checkpoints ! rm - rf . git ! rm - rf . gitignore ! rm - rf ./ local_ml_git_config_server ! ml - git clone '/local_ml_git_config_server.git' ! cp ./ train - images . idx3 - ubyte ./ local_ml_git_config_server / train - images . idx3 - ubyte ! cp ./ train - labels . idx1 - ubyte ./ local_ml_git_config_server / train - labels . idx1 % cd ./ local_ml_git_config_server","title":"Notebook state management"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#1-the-dataset","text":"Dataset MNIST is a set of small images of handwritten digits, in the version available in our docker environment, the set has a total of 70,000 images from numbers 0 to 9. Look at the below image which has a few examples instances:","title":"1 - The dataset"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#2-getting-the-data","text":"To start working with our dataset it is necessary to carry out the checkout command of ml-git in order to bring the data from our storage to the user's workspace. ! ml - git labels checkout labelsmnist - d mnist_dataset_path = 'datasets/handwritten/digits/mnist/data/' mnist_labels_path = 'labels/handwritten/digits/labelsmnist/data/' INFO - Metadata: Performing checkout on the entity's lastest tag (handwritten__digits__labelsmnist__1) blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 198blobs/s]\u001b[0m\u001b[0m chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 207chunks/s]\u001b[0m\u001b[0m files into workspace: 100%|\u2588| 2.00/2.00 [00:00<00:00, 919files into workspace/s]\u001b[0m\u001b[0m \u001b[0mINFO - Repository: Initializing related datasets download blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 216blobs/s]\u001b[0m\u001b[0m chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.31chunks/s]\u001b[0m\u001b[0m files into workspace: 100%|\u2588| 2.00/2.00 [00:00<00:00, 12.9files into workspace/s]\u001b[0m[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0m Some important points to highlight here are that the tag parameter can be the name of the entity, this way the ml-git will get the latest version available for this entity. With the -d signals that ml-git should look for the dataset associated with these labels Once we have the data in the workspace, we can load it into variables","title":"2 - Getting the data"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#training-data","text":"from mlxtend.data import loadlocal_mnist import numpy as np import pickle X_train = pickle . load ( open ( mnist_dataset_path + 'train-images.idx3-ubyte' , 'rb' )) y_train = pickle . load ( open ( mnist_labels_path + 'train-labels.idx1-ubyte' , 'rb' )) print ( 'Training data: ' ) print ( 'Dimensions: %s x %s ' % ( X_train . shape [ 0 ], X_train . shape [ 1 ])) print ( 'Digits: %s ' % np . unique ( y_train )) print ( 'Class distribution: %s ' % np . bincount ( y_train )) Training data: Dimensions: 60000 x 784 Digits: [0 1 2 3 4 5 6 7 8 9] Class distribution: [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949] The training data consists of 60,000 entries of 784 pixels, distributed among the possible values \u200b\u200baccording to the output above.","title":"Training data"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#test-data","text":"X_test , y_test = loadlocal_mnist ( images_path = mnist_dataset_path + 't10k-images.idx3-ubyte' , labels_path = mnist_labels_path + 't10k-labels.idx1-ubyte' ) print ( 'Test data: ' ) print ( 'Dimensions: %s x %s ' % ( X_test . shape [ 0 ], X_test . shape [ 1 ])) print ( 'Digits: %s ' % np . unique ( y_test )) print ( 'Class distribution: %s ' % np . bincount ( y_test )) Test data: Dimensions: 10000 x 784 Digits: [0 1 2 3 4 5 6 7 8 9] Class distribution: [ 980 1135 1032 1010 982 892 958 1028 974 1009] The test data consists of 10,000 entries of 784 pixels, distributed among the possible values according to the output above.","title":"Test data"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#3-training-and-evaluating","text":"Let\u2019s take an example of RandomForest Classifier and train it on the dataset and evaluate it. from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score # Training on the existing dataset rf_clf = RandomForestClassifier ( random_state = 42 ) rf_clf . fit ( X_train , y_train ) # Evaluating the model y_pred = rf_clf . predict ( X_test ) score = accuracy_score ( y_test , y_pred ) print ( 'Accuracy score after training on existing dataset' , score ) Accuracy score after training on existing dataset 0.9705","title":"3 - Training and evaluating"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#4-versioning-our-model","text":"As we do not have any previously versioned models, it will be necessary to create a new entity. For this we use the following command: ! ml - git models create modelmnist -- categories = \"handwritten, digits\" -- bucket - name = mlgit -- mutability = mutable -- entity - dir = 'handwritten/digits' INFO - MLGit: Model artifact created. \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0m Once we have our model trained and evaluated, we will version it with ml-git. For that we need to save it in a file. def save_model ( model ): filename = 'models/handwritten/digits/modelmnist/data/rf_mnist.sav' pickle . dump ( model , open ( filename , 'wb' )) save_model ( rf_clf ) With the file in the workspace we use the following commands to create a version: ! ml - git models add modelmnist -- metric accuracy $ score ! ml - git models commit modelmnist -- dataset = mnist -- labels = labelsmnist ! ml - git models push modelmnist INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/models/metadata] \u001b[0mINFO - Repository: models adding path [/api_scripts/mnist_notebook/models/handwritten/digits/modelmnist] to ml-git index files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 2.14files/s]\u001b[0m files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 6.44kfiles/s]\u001b[0m\u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0mINFO - Local Repository: Associate datasets [mnist]-[handwritten__digits__mnist__1] to the models. \u001b[0mINFO - Local Repository: Associate labels [labelsmnist]-[handwritten__digits__labelsmnist__1] to the models. \u001b[0mINFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/models/metadata] --- file[handwritten/digits/modelmnist] files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 520/520 [00:06<00:00, 84.8files/s]\u001b[0m\u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0mtadata to the git repository\u001b[0m\u001b[0m","title":"4 - Versioning our model"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#5-adding-new-data","text":"At some point after training a model it may be the case that new data is available. It is interesting that this new data is added to our entity to generate a second version of our dataset. Let's add this data to our entity's directory: ! cp train - images . idx3 - ubyte datasets / handwritten / digits / mnist / data /. ! cp train - labels . idx1 - ubyte labels / handwritten / digits / labelsmnist / data /. Let's take a look at our new dataset # loading the dataset X_train = pickle . load ( open ( mnist_dataset_path + 'train-images.idx3-ubyte' , 'rb' )) y_train = pickle . load ( open ( mnist_labels_path + 'train-labels.idx1-ubyte' , 'rb' )) print ( 'Test data: ' ) print ( 'Dimensions: %s x %s ' % ( X_train . shape [ 0 ], X_train . shape [ 1 ])) print ( 'Digits: %s ' % np . unique ( y_train )) print ( 'Class distribution: %s ' % np . bincount ( y_train )) Test data: Dimensions: 180000 x 784 Digits: [0 1 2 3 4 5 6 7 8 9] Class distribution: [17769 20226 17874 18393 17526 16263 17754 18795 17553 17847] The train data now consists of 180,000 entries of 784 pixels, distributed among the possible values according to the output above.","title":"5 - Adding new data"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#6-versioning-the-dataset-and-labels-with-the-new-entries","text":"dataset_file = 'datasets/handwritten/digits/mnist/data/train-images.idx3-ubyte' pickle . dump ( X_train , open ( dataset_file , 'wb' )) labels_file = 'labels/handwritten/digits/labelsmnist/data/train-labels.idx1-ubyte' pickle . dump ( y_train , open ( labels_file , 'wb' ))","title":"6 - Versioning the dataset and labels with the new entries"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#versioning-the-dataset","text":"! ml - git datasets add mnist -- bumpversion ! ml - git datasets commit mnist ! ml - git datasets push mnist INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/datasets/metadata] \u001b[0mINFO - Repository: datasets adding path [/api_scripts/mnist_notebook/datasets/handwritten/digits/mnist] to ml-git index files: 0.00files [00:00, ?files/s]\u001b[0m'zdj7WmvrKwjWMCQMFFgcMFmGxEgQS2uXatYWxcE3ByNJeDmze'\u001b[0m files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.10files/s]\u001b[0m files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 13.3kfiles/s]\u001b[0m\u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0mINFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/datasets/metadata] --- file[handwritten/digits/mnist] files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 540/540 [00:06<00:00, 86.2files/s]\u001b[0m\u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0mtadata to the git repository\u001b[0m\u001b[0m","title":"Versioning the dataset"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#versioning-the-labels","text":"! ml - git labels add labelsmnist -- bumpversion ! ml - git labels commit labelsmnist -- dataset = mnist ! ml - git labels push labelsmnist INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/labels/metadata] \u001b[0mINFO - Repository: labels adding path [/api_scripts/mnist_notebook/labels/handwritten/digits/labelsmnist] to ml-git index files: 0.00files [00:00, ?files/s]\u001b[0m'zdj7WaeJerH7ACFKfFgh9itrCoWt3iBCYagzYS2M7VhidBAmR'\u001b[0m files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 410files/s]\u001b[0m files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 12.7kfiles/s]\u001b[0m\u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0mINFO - Local Repository: Associate datasets [mnist]-[handwritten__digits__mnist__2] to the labels. \u001b[0mINFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/labels/metadata] --- file[handwritten/digits/labelsmnist] files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 64.7files/s]\u001b[0m\u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0mtadata to the git repository\u001b[0m\u001b[0m","title":"Versioning the labels"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#7-training-and-evaluating","text":"# Training on new data rf_clf = RandomForestClassifier ( random_state = 42 ) rf_clf . fit ( X_train , y_train ) # Evaluating the model y_pred = rf_clf . predict ( X_test ) score = accuracy_score ( y_test , y_pred ) print ( \"Accuracy score after training on augmented dataset\" , score ) Accuracy score after training on augmented dataset 0.9746 We\u2019ve improved the accuracy by ~0.4%. This is great.","title":"7 - Training and evaluating"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#8-versioning-our-model","text":"save_model ( rf_clf ) ! ml - git models add modelmnist -- bumpversion -- metric accuracy $ score ! ml - git models commit modelmnist -- dataset = mnist -- labels = labelsmnist ! ml - git models push modelmnist INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/models/metadata] \u001b[0mINFO - Repository: models adding path [/api_scripts/mnist_notebook/models/handwritten/digits/modelmnist] to ml-git index files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:05<00:00, 5.41s/files]\u001b[0m'zdj7WfjwEm3XVzoggyG1kSPQhYMSMpbBdLzYMx5Z61StSbu5H'\u001b[0m \u001b[0m files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 9.71kfiles/s]\u001b[0m\u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0mINFO - Local Repository: Associate datasets [mnist]-[handwritten__digits__mnist__2] to the models. \u001b[0mINFO - Local Repository: Associate labels [labelsmnist]-[handwritten__digits__labelsmnist__2] to the models. \u001b[0mINFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/models/metadata] --- file[handwritten/digits/modelmnist] files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.39k/1.39k [00:36<00:00, 38.0files/s]\u001b[0m\u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0mtadata to the git repository\u001b[0m\u001b[0m","title":"8 - Versioning our model"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#9-reproducing-our-experiment-with-ml-git","text":"Once the experiment data is versioned, it is common that it is necessary to re-evaluate the result, or that someone else wants to see the result of an already trained model. For this, we will perform the model checkout in version 1, to get the test data and the trained model. mnist_dataset_path = 'datasets/handwritten/digits/mnist/data/' mnist_labels_path = 'labels/handwritten/digits/labelsmnist/data/' mnist_model_path = 'models/handwritten/digits/modelmnist/data/' ! ml - git models checkout modelmnist -- version = 1 - d - l # Getting test data X_test , y_test = loadlocal_mnist ( images_path = mnist_dataset_path + 't10k-images.idx3-ubyte' , labels_path = mnist_labels_path + 't10k-labels.idx1-ubyte' ) INFO - Metadata: Performing checkout in tag handwritten__digits__modelmnist__1 blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 912blobs/s]\u001b[0m\u001b[0m chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 3.59chunks/s]\u001b[0m\u001b[0m files into workspace: 100%|\u2588| 1.00/1.00 [00:01<00:00, 1.60s/files into workspace]\u001b[0m\u001b[0m \u001b[0mINFO - Repository: Initializing related datasets download blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.05kblobs/s]\u001b[0m\u001b[0m chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 17.4chunks/s]\u001b[0m\u001b[0m files into workspace: 100%|\u2588| 2.00/2.00 [00:00<00:00, 3.17files into workspace/s]\u001b[0m[0m \u001b[0mINFO - Repository: Initializing related labels download blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.19kblobs/s]\u001b[0m\u001b[0m chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 625chunks/s]\u001b[0m\u001b[0m files into workspace: 100%|\u2588| 2.00/2.00 [00:00<00:00, 806files into workspace/s]\u001b[0m\u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0m With the test data in hand, let's upload the model and evaluate it for our dataset. loaded_model = pickle . load ( open ( mnist_model_path + 'rf_mnist.sav' , 'rb' )) y_pred = loaded_model . predict ( X_test ) score = accuracy_score ( y_test , y_pred ) print ( 'Accuracy score for version 1: ' , score ) Accuracy score for version 1: 0.9705 Now let's take the model from the version 2 (model trained with more data) and evaluate it for the test set. ! ml - git models checkout modelmnist -- version = 2 loaded_model = pickle . load ( open ( mnist_model_path + 'rf_mnist.sav' , 'rb' )) y_pred = loaded_model . predict ( X_test ) score = accuracy_score ( y_test , y_pred ) print ( 'Accuracy score for version 2: ' , score ) INFO - Metadata: Performing checkout in tag handwritten__digits__modelmnist__2 blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 1.70kblobs/s]\u001b[0m\u001b[0m chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 11.1chunks/s]\u001b[0m\u001b[0m files into workspace: 100%|\u2588| 1.00/1.00 [00:04<00:00, 4.32s/files into workspace]\u001b[0m\u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0mAccuracy score for version 2: 0.9746 In a quick and practical way it was possible to obtain the models generated in the experiments and to evaluate them again.","title":" 9 - Reproducing our experiment with ml-git"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#conclusions","text":"At the end of this execution we have two versions of each entity. If someone else wants to replicate this experiment, they can check out the model with the related dataset and labels. ! ml - git models metrics modelmnist Tag: handwritten__digits__modelmnist__1 +-----------------------------+---------------------+ | Name | Value | +-----------------------------+---------------------+ | Date | 2021-03-25 13:54:04 | | Related dataset - (version) | mnist - (1) | | Related labels - (version) | labelsmnist - (1) | | accuracy | 0.9705 | +-----------------------------+---------------------+ \u001b[0m \u001b[0mTag: handwritten__digits__modelmnist__2 +-----------------------------+---------------------+ | Name | Value | +-----------------------------+---------------------+ | Date | 2021-03-25 13:56:48 | | Related dataset - (version) | mnist - (2) | | Related labels - (version) | labelsmnist - (2) | | accuracy | 0.9746 | +-----------------------------+---------------------+ \u001b[0m \u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0m","title":"Conclusions"},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/","text":"Checkout with sample \u00b6 This notebook describes how to perform a checkout operation with ML-Git using samples of a dataset. The checkout command has three types of sampling options available only for dataset: --sample-type=group --seed , --sample-type=random --seed , --sample-type=range . We use random.sample(population, k) to return a sample of the size k from the population elements. We use random.seed() to set the seed so that the sample generated by random.sample() can be reproduced between experiments. We use the range() object to take samples from a given range. Example: \u00b6 Let's assume that we have a dataset that contains 12 files. ml-git datasets checkout computer-vision__images__dataset-ex__22 --sample-type=group --sampling=2:5 --seed=1 : This command selects 2 files randomly from every group of five files to download. ml-git datasets checkout computer-vision__images__dataset-ex__22 --sample-type=random --sampling=2:6 --seed=1 : This command makes a sample = (amount * len (dataset))% frequency ratio, sample = 4, so four files are selected randomly to download. ml-git datasets checkout computer-vision__images__dataset-ex__22 --sample-type=range --sampling=2:11:2 : This command selects the files at indexes generated by range(start=2, stop=11, step=2) . Notebook state management \u00b6 Before execute any cell from this notebook, make sure that you have executed all cells from notebook multiple_datasets in same folder. To start using the ml-git api we need to import it into our script, and make sure that we are in correct folder \u00b6 from ml_git.api import MLGitAPI % cd / api_scripts / multiple_datasets_notebook After that, we define some variables that will be used by the script \u00b6 # The type of entity we are working on entity = 'datasets' # Existing tag in our repository tag = 'peopleFaces' Before using the sample option, we will checkout the entity to check the files contained in the tag \u00b6 The datapath returned by the function tells us where the entity's data was downloaded. That way we can use the following method to print the files that are in the entity's directory import os import glob from IPython.display import Image data_type = '*.jpg' def print_files (): data_path = 'datasets/peopleFaces/data/people_faces' folder = os . path . join ( data_path , data_type ) print ( 'Downloaded files: ' ) for imageName in glob . glob ( folder ): print ( ' \\t {} ' . format ( imageName )) display ( Image ( filename = imageName , width = 150 , height = 150 )) print_files () Downloaded files: datasets/people_faces/data/people_faces/3.jpg datasets/people_faces/data/people_faces/4.jpg datasets/people_faces/data/people_faces/8.jpg datasets/people_faces/data/people_faces/5.jpg datasets/people_faces/data/people_faces/1.jpg datasets/people_faces/data/people_faces/9.jpg datasets/people_faces/data/people_faces/10.jpg datasets/people_faces/data/people_faces/7.jpg datasets/people_faces/data/people_faces/6.jpg datasets/people_faces/data/people_faces/2.jpg To be able to checkout the same tag, we use the following method to remove some files. \u00b6 import shutil import stat # function created to clear directory def clear_path ( path ): if not os . path . exists ( path ): return # SET the permission for files inside the .git directory to clean up for root , dirs , files in os . walk ( path ): for f in files : os . chmod ( os . path . join ( root , f ), stat . S_IRWXU | stat . S_IRWXG | stat . S_IRWXO ) try : shutil . rmtree ( path ) except Exception as e : print ( 'except: ' , e ) def clear_environment (): clear_path ( os . path . join ( '.ml-git' , entity , 'index' )) clear_path ( os . path . join ( '.ml-git' , entity , 'refs' )) clear_path ( os . path . join ( entity )) clear_environment () Checkout with group sample \u00b6 sampling = { 'group' : '1:5' , 'seed' : '10' } api = MLGitAPI () data_path = api . checkout ( entity , tag , sampling ) print_files () clear_environment () INFO - Metadata Manager: Pull [/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata] INFO - Metadata: Performing checkout on the entity's lastest tag (computer-vision__images__people_faces__2) blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.73kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.38kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.54kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.50kfiles into workspace/s] Downloaded files: datasets/people_faces/data/people_faces/1.jpg datasets/people_faces/data/people_faces/6.jpg Checkout with range sample \u00b6 sampling = { 'range' : '0:4:3' } data_path = api . checkout ( entity , tag , sampling ) print_files () clear_environment () INFO - Metadata Manager: Pull [/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata] INFO - Metadata: Performing checkout on the entity's lastest tag (computer-vision__images__people_faces__2) blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.28kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.37kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.16kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.13kfiles into workspace/s] Downloaded files: datasets/people_faces/data/people_faces/6.jpg datasets/people_faces/data/people_faces/2.jpg Checkout with random sample \u00b6 sampling = { 'random' : '1:5' , 'seed' : '1' } data_path = api . checkout ( entity , tag , sampling ) print_files () clear_environment () INFO - Metadata Manager: Pull [/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata] INFO - Metadata: Performing checkout on the entity's lastest tag (computer-vision__images__people_faces__2) blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.13kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.14kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.81kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.01kfiles into workspace/s] Downloaded files: datasets/people_faces/data/people_faces/4.jpg datasets/people_faces/data/people_faces/5.jpg","title":"Checkout With Sample"},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/#checkout-with-sample","text":"This notebook describes how to perform a checkout operation with ML-Git using samples of a dataset. The checkout command has three types of sampling options available only for dataset: --sample-type=group --seed , --sample-type=random --seed , --sample-type=range . We use random.sample(population, k) to return a sample of the size k from the population elements. We use random.seed() to set the seed so that the sample generated by random.sample() can be reproduced between experiments. We use the range() object to take samples from a given range.","title":"Checkout with sample"},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/#example","text":"Let's assume that we have a dataset that contains 12 files. ml-git datasets checkout computer-vision__images__dataset-ex__22 --sample-type=group --sampling=2:5 --seed=1 : This command selects 2 files randomly from every group of five files to download. ml-git datasets checkout computer-vision__images__dataset-ex__22 --sample-type=random --sampling=2:6 --seed=1 : This command makes a sample = (amount * len (dataset))% frequency ratio, sample = 4, so four files are selected randomly to download. ml-git datasets checkout computer-vision__images__dataset-ex__22 --sample-type=range --sampling=2:11:2 : This command selects the files at indexes generated by range(start=2, stop=11, step=2) .","title":"Example:"},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/#notebook-state-management","text":"Before execute any cell from this notebook, make sure that you have executed all cells from notebook multiple_datasets in same folder.","title":"Notebook state management"},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/#to-start-using-the-ml-git-api-we-need-to-import-it-into-our-script-and-make-sure-that-we-are-in-correct-folder","text":"from ml_git.api import MLGitAPI % cd / api_scripts / multiple_datasets_notebook","title":"To start using the ml-git api we need to import it into our script, and make sure that we are in correct folder"},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/#after-that-we-define-some-variables-that-will-be-used-by-the-script","text":"# The type of entity we are working on entity = 'datasets' # Existing tag in our repository tag = 'peopleFaces'","title":"After that, we define some variables that will be used by the script"},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/#before-using-the-sample-option-we-will-checkout-the-entity-to-check-the-files-contained-in-the-tag","text":"The datapath returned by the function tells us where the entity's data was downloaded. That way we can use the following method to print the files that are in the entity's directory import os import glob from IPython.display import Image data_type = '*.jpg' def print_files (): data_path = 'datasets/peopleFaces/data/people_faces' folder = os . path . join ( data_path , data_type ) print ( 'Downloaded files: ' ) for imageName in glob . glob ( folder ): print ( ' \\t {} ' . format ( imageName )) display ( Image ( filename = imageName , width = 150 , height = 150 )) print_files () Downloaded files: datasets/people_faces/data/people_faces/3.jpg datasets/people_faces/data/people_faces/4.jpg datasets/people_faces/data/people_faces/8.jpg datasets/people_faces/data/people_faces/5.jpg datasets/people_faces/data/people_faces/1.jpg datasets/people_faces/data/people_faces/9.jpg datasets/people_faces/data/people_faces/10.jpg datasets/people_faces/data/people_faces/7.jpg datasets/people_faces/data/people_faces/6.jpg datasets/people_faces/data/people_faces/2.jpg","title":"Before using the sample option, we will checkout the entity to check the files contained in the tag"},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/#to-be-able-to-checkout-the-same-tag-we-use-the-following-method-to-remove-some-files","text":"import shutil import stat # function created to clear directory def clear_path ( path ): if not os . path . exists ( path ): return # SET the permission for files inside the .git directory to clean up for root , dirs , files in os . walk ( path ): for f in files : os . chmod ( os . path . join ( root , f ), stat . S_IRWXU | stat . S_IRWXG | stat . S_IRWXO ) try : shutil . rmtree ( path ) except Exception as e : print ( 'except: ' , e ) def clear_environment (): clear_path ( os . path . join ( '.ml-git' , entity , 'index' )) clear_path ( os . path . join ( '.ml-git' , entity , 'refs' )) clear_path ( os . path . join ( entity )) clear_environment ()","title":"To be able to checkout the same tag, we use the following method to remove some files."},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/#checkout-with-group-sample","text":"sampling = { 'group' : '1:5' , 'seed' : '10' } api = MLGitAPI () data_path = api . checkout ( entity , tag , sampling ) print_files () clear_environment () INFO - Metadata Manager: Pull [/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata] INFO - Metadata: Performing checkout on the entity's lastest tag (computer-vision__images__people_faces__2) blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.73kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.38kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.54kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.50kfiles into workspace/s] Downloaded files: datasets/people_faces/data/people_faces/1.jpg datasets/people_faces/data/people_faces/6.jpg","title":"Checkout with group sample"},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/#checkout-with-range-sample","text":"sampling = { 'range' : '0:4:3' } data_path = api . checkout ( entity , tag , sampling ) print_files () clear_environment () INFO - Metadata Manager: Pull [/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata] INFO - Metadata: Performing checkout on the entity's lastest tag (computer-vision__images__people_faces__2) blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.28kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.37kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.16kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.13kfiles into workspace/s] Downloaded files: datasets/people_faces/data/people_faces/6.jpg datasets/people_faces/data/people_faces/2.jpg","title":"Checkout with range sample"},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/#checkout-with-random-sample","text":"sampling = { 'random' : '1:5' , 'seed' : '1' } data_path = api . checkout ( entity , tag , sampling ) print_files () clear_environment () INFO - Metadata Manager: Pull [/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata] INFO - Metadata: Performing checkout on the entity's lastest tag (computer-vision__images__people_faces__2) blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.13kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.14kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.81kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.01kfiles into workspace/s] Downloaded files: datasets/people_faces/data/people_faces/4.jpg datasets/people_faces/data/people_faces/5.jpg","title":"Checkout with random sample"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/","text":"Storage reuse with multiple datasets \u00b6 This notebook describes how to handle the scenario where the same file is present in more than one dataset. \u00b6 When the same file is used in multiple datasets, that file will be added to the bucket only once, in order to optimize the space usage in the bucket. To exemplify this use case, two entities will be created: the people entity contains 10 images with faces of people, while famous entity contains 7 images with faces of famous people, being 5 of them also contained in the people entity. This way, when sending the files to the repository, the 5 images that are being used in the two entities will not be duplicated in the bucket. The two entities will refer to the same image stored in the bucket. Notebook state management \u00b6 If you have already run this notebook or another in this same folder, it is recommended that you perform a state restart by executing the cell below, because previously performed state changes may interfere with the execution of the notebook. Be aware, that procedure does not affect any remote repository. % cd / api_scripts / multiple_datasets_notebook ! rm - rf ./ logs ! rm - rf . ml - git ! rm - rf ./ datasets To start using the ml-git api we need to import it into our script \u00b6 from ml_git.api import MLGitAPI After that, we define some variables that will be used by the notebook \u00b6 # The type of entity we are working on entity = 'datasets' # The entity name we are working on entity_name_people = 'peopleFaces' # The entity name we are working on entity_name_famous = 'famousFaces' To start, let's take into account that you have a repository with git settings to make the clone. If this is not your scenario, you will need to configure ml-git outside this notebook (At the moment the api does not have the necessary methods to perform this configuration). \u00b6 Or you can manually configure the repository using the command line, following the steps in the First Project documentation. \u00b6 repository_url = '/local_ml_git_config_server.git' api = MLGitAPI () api . clone ( repository_url ) INFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/multiple_datasets_notebook/tmpfyoxmtjy/mlgit/.ml-git/datasets/metadata] INFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/multiple_datasets_notebook/tmpfyoxmtjy/mlgit/.ml-git/models/metadata] INFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/multiple_datasets_notebook/tmpfyoxmtjy/mlgit/.ml-git/labels/metadata] INFO - Metadata: Successfully loaded configuration files! Create the people dataset \u00b6 ! ml - git datasets create peopleFaces -- categories = \"computer-vision, images\" -- bucket - name = faces_bucket -- mutability = strict -- import = 'people_faces' -- unzip \u001b[?25h\u280b Importing files\u2819 Importing filesINFO - MLGit: Unzipping files INFO - MLGit: Dataset artifact created. \u001b[?25h We can now proceed with the necessary steps to send the new data to storage. \u00b6 api . add ( entity , entity_name_people , bumpversion = True ) INFO - Metadata Manager: Pull [/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata] INFO - Repository: datasets adding path [/api_scripts/multiple_datasets_notebook/datasets/people_faces] to ml-git index files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.0/10.0 [00:00<00:00, 646files/s] \u280b Creating hard links in cache\u2819 Creating hard links in cache files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.0/10.0 [00:00<00:00, 32.7kfiles/s] Commit the changes # Custom commit message message = 'Commit example' api . commit ( entity , entity_name_people , message ) \u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest INFO - Metadata Manager: Commit repo[/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata] --- file[people_faces] As we are using MinIO locally to store the data in the bucket, we were able to check the number of files that are in the local bucket. \u00b6 import os def get_bucket_files_count (): print ( \"Number of files on bucket: \" + str ( len ( os . listdir ( '../../data/faces_bucket' )))) Amount of files in the buket before pushing the people dataset \u00b6 get_bucket_files_count () Number of files on bucket: 0 As we have not yet uploaded any version of our dataset, the bucket is empty. Pushing the people dataset \u00b6 api . push ( entity , entity_name_people ) files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.0/20.0 [00:00<00:00, 92.3files/s] \u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository Amount of files in the buket after pushing the people dataset \u00b6 get_bucket_files_count () Number of files on bucket: 20 After sending the data, we can observe the presence of 20 blobs related to the 10 images that were versioned. In this case, two blobs were added for each image in our dataset. Create the famous dataset \u00b6 Let's create our second dataset that has some images equals to the first dataset. ! ml - git datasets create famousFaces -- categories = \"computer-vision, images\" -- bucket - name = faces_bucket -- mutability = strict -- import = 'famous_faces' -- unzip \u001b[?25h\u280b Importing files\u2819 Importing filesINFO - MLGit: Unzipping files INFO - MLGit: Dataset artifact created. \u001b[?25h We can now proceed with the necessary steps to send the new data to storage. \u00b6 api . add ( entity , entity_name_famous , bumpversion = True ) INFO - Metadata Manager: Pull [/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata] INFO - Repository: datasets adding path [/api_scripts/multiple_datasets_notebook/datasets/famous_faces] to ml-git index files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.00/7.00 [00:00<00:00, 703files/s] \u280b Creating hard links in cache\u2819 Creating hard links in cache files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.00/7.00 [00:00<00:00, 40.2kfiles/s] Commit the changes # Custom commit message message = 'Commit example' api . commit ( entity , entity_name_famous , message ) \u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest INFO - Metadata Manager: Commit repo[/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata] --- file[famous_faces] And finally, sending the data api . push ( entity , entity_name_famous ) files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.0/14.0 [00:00<00:00, 177files/s] \u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository Amount of files in the buket after pushing the famous dataset \u00b6 get_bucket_files_count () Number of files on bucket: 24 As you can see, only 4 blobs were added to our bucket. Of the set of 7 images, only 2 images were different from the other dataset, so ml-git can optimize storage by adding blobs related only to these new images.","title":"Multiple Datasets"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#storage-reuse-with-multiple-datasets","text":"","title":"Storage reuse with multiple datasets"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#this-notebook-describes-how-to-handle-the-scenario-where-the-same-file-is-present-in-more-than-one-dataset","text":"When the same file is used in multiple datasets, that file will be added to the bucket only once, in order to optimize the space usage in the bucket. To exemplify this use case, two entities will be created: the people entity contains 10 images with faces of people, while famous entity contains 7 images with faces of famous people, being 5 of them also contained in the people entity. This way, when sending the files to the repository, the 5 images that are being used in the two entities will not be duplicated in the bucket. The two entities will refer to the same image stored in the bucket.","title":"This notebook describes how to handle the scenario where the same file is present in more than one dataset."},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#notebook-state-management","text":"If you have already run this notebook or another in this same folder, it is recommended that you perform a state restart by executing the cell below, because previously performed state changes may interfere with the execution of the notebook. Be aware, that procedure does not affect any remote repository. % cd / api_scripts / multiple_datasets_notebook ! rm - rf ./ logs ! rm - rf . ml - git ! rm - rf ./ datasets","title":"Notebook state management"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#to-start-using-the-ml-git-api-we-need-to-import-it-into-our-script","text":"from ml_git.api import MLGitAPI","title":"To start using the ml-git api we need to import it into our script"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#after-that-we-define-some-variables-that-will-be-used-by-the-notebook","text":"# The type of entity we are working on entity = 'datasets' # The entity name we are working on entity_name_people = 'peopleFaces' # The entity name we are working on entity_name_famous = 'famousFaces'","title":"After that, we define some variables that will be used by the notebook"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#to-start-lets-take-into-account-that-you-have-a-repository-with-git-settings-to-make-the-clone-if-this-is-not-your-scenario-you-will-need-to-configure-ml-git-outside-this-notebook-at-the-moment-the-api-does-not-have-the-necessary-methods-to-perform-this-configuration","text":"","title":"To start, let's take into account that you have a repository with git settings to make the clone. If this is not your scenario, you will need to configure ml-git outside this notebook (At the moment the api does not have the necessary methods to perform this configuration)."},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#or-you-can-manually-configure-the-repository-using-the-command-line-following-the-steps-in-the-first-project-documentation","text":"repository_url = '/local_ml_git_config_server.git' api = MLGitAPI () api . clone ( repository_url ) INFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/multiple_datasets_notebook/tmpfyoxmtjy/mlgit/.ml-git/datasets/metadata] INFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/multiple_datasets_notebook/tmpfyoxmtjy/mlgit/.ml-git/models/metadata] INFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/multiple_datasets_notebook/tmpfyoxmtjy/mlgit/.ml-git/labels/metadata] INFO - Metadata: Successfully loaded configuration files!","title":"Or you can manually configure the repository using the command line, following the steps in the First Project documentation."},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#create-the-people-dataset","text":"! ml - git datasets create peopleFaces -- categories = \"computer-vision, images\" -- bucket - name = faces_bucket -- mutability = strict -- import = 'people_faces' -- unzip \u001b[?25h\u280b Importing files\u2819 Importing filesINFO - MLGit: Unzipping files INFO - MLGit: Dataset artifact created. \u001b[?25h","title":"Create the people dataset"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#we-can-now-proceed-with-the-necessary-steps-to-send-the-new-data-to-storage","text":"api . add ( entity , entity_name_people , bumpversion = True ) INFO - Metadata Manager: Pull [/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata] INFO - Repository: datasets adding path [/api_scripts/multiple_datasets_notebook/datasets/people_faces] to ml-git index files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.0/10.0 [00:00<00:00, 646files/s] \u280b Creating hard links in cache\u2819 Creating hard links in cache files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.0/10.0 [00:00<00:00, 32.7kfiles/s] Commit the changes # Custom commit message message = 'Commit example' api . commit ( entity , entity_name_people , message ) \u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest INFO - Metadata Manager: Commit repo[/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata] --- file[people_faces]","title":"We can now proceed with the necessary steps to send the new data to storage."},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#as-we-are-using-minio-locally-to-store-the-data-in-the-bucket-we-were-able-to-check-the-number-of-files-that-are-in-the-local-bucket","text":"import os def get_bucket_files_count (): print ( \"Number of files on bucket: \" + str ( len ( os . listdir ( '../../data/faces_bucket' ))))","title":"As we are using MinIO locally to store the data in the bucket, we were able to check the number of files that are in the local bucket."},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#amount-of-files-in-the-buket-before-pushing-the-people-dataset","text":"get_bucket_files_count () Number of files on bucket: 0 As we have not yet uploaded any version of our dataset, the bucket is empty.","title":"Amount of files in the buket before pushing the people dataset"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#pushing-the-people-dataset","text":"api . push ( entity , entity_name_people ) files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.0/20.0 [00:00<00:00, 92.3files/s] \u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository","title":"Pushing the people dataset"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#amount-of-files-in-the-buket-after-pushing-the-people-dataset","text":"get_bucket_files_count () Number of files on bucket: 20 After sending the data, we can observe the presence of 20 blobs related to the 10 images that were versioned. In this case, two blobs were added for each image in our dataset.","title":"Amount of files in the buket after pushing the people dataset"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#create-the-famous-dataset","text":"Let's create our second dataset that has some images equals to the first dataset. ! ml - git datasets create famousFaces -- categories = \"computer-vision, images\" -- bucket - name = faces_bucket -- mutability = strict -- import = 'famous_faces' -- unzip \u001b[?25h\u280b Importing files\u2819 Importing filesINFO - MLGit: Unzipping files INFO - MLGit: Dataset artifact created. \u001b[?25h","title":"Create the famous dataset"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#we-can-now-proceed-with-the-necessary-steps-to-send-the-new-data-to-storage_1","text":"api . add ( entity , entity_name_famous , bumpversion = True ) INFO - Metadata Manager: Pull [/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata] INFO - Repository: datasets adding path [/api_scripts/multiple_datasets_notebook/datasets/famous_faces] to ml-git index files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.00/7.00 [00:00<00:00, 703files/s] \u280b Creating hard links in cache\u2819 Creating hard links in cache files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.00/7.00 [00:00<00:00, 40.2kfiles/s] Commit the changes # Custom commit message message = 'Commit example' api . commit ( entity , entity_name_famous , message ) \u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest INFO - Metadata Manager: Commit repo[/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata] --- file[famous_faces] And finally, sending the data api . push ( entity , entity_name_famous ) files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.0/14.0 [00:00<00:00, 177files/s] \u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository","title":"We can now proceed with the necessary steps to send the new data to storage."},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#amount-of-files-in-the-buket-after-pushing-the-famous-dataset","text":"get_bucket_files_count () Number of files on bucket: 24 As you can see, only 4 blobs were added to our bucket. Of the set of 7 images, only 2 images were different from the other dataset, so ml-git can optimize storage by adding blobs related only to these new images.","title":"Amount of files in the buket after pushing the famous dataset"},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/","text":"Relationships API methods \u00b6 This notebook describes a basic flow in the context of relationships between entities with the API provided by ML-Git. In it, we'll show you how to use the set of commands provided. You can check the documentation for more information: API documentation Notebook prerequisites \u00b6 This notebook uses the GitHub API to access a reset and does Ml-Git operations on its contents, so before running this notebook, take the following steps: Have a GitHub SHH access key so that you can use the repository information retrieval API. Have a GitHub repository that the SHH key has access to. 1 - Context \u00b6 In this notebook we consider a scenario of an ML-Git project with the following settings: A versioned config file in GitHub. Pointing to the entities' metadata repositories. Each entity type having its metadata repository. One mode entity (model-ex), two labels entities (labels-ex and labels-ex2) and one dataset entity (dataset-ex) Entities have relationships defined at versioning time. This settings mentioned above can be better visualized in the diagram below: 2 - Configuring \u00b6 To use the methods, you will need to import the API and define some constants related to the user's credential: Below are the constants described in the pre-requirements section, where:['removed'] should be replaced by the SHH access key and api_url can be modified if necessary as reported in the GitHub API documentation. from ml_git.api import MLGitAPI from ml_git import api github_token = [ 'removed' ] api_url = 'https://api.github.com' After defining the variables to configure, it will be possible to start a manager that will be responsible for operating on the github API. api = MLGitAPI () manager = api . init_entity_manager ( github_token , api_url ) We will use the manager to execute the commands in the next steps. 3 - Methods \u00b6 3.1 - Get Entities \u00b6 The get_entities method allows the user to get a list of entities being versioned in a project. For this, the user must inform the path to the configuration file, whether this path is a local directory or the name of a git repository. The path can be modified using the config_repository_name field, in our example case the configuration file is in 'user/mlgit-config-repository. config_repository_name = 'user/mlgit-config-repository' project_entities = manager . get_entities ( config_repo_name = config_repository_name ) print ( \"Entities found: {} \" . format ( len ( project_entities ))) print ( \"Example of output object: \\n {} \" . format ( project_entities [ 3 ])) Entities found: 4 Example of output object: { \"name\": \"model-ex\", \"entity_type\": \"model\", \"metadata\": { \"full_name\": \"user/mlgit-models\", \"git_url\": \"git@github.com:user/mlgit-models.git\", \"html_url\": \"https://github.com/user/mlgit-models\", \"owner_email\": \"user@gmail.com\", \"owner_name\": \"User Name\" }, \"last_spec_version\": { \"version\": 3, \"tag\": \"test__model-ex__3\", \"mutability\": \"flexible\", \"categories\": [ \"test\" ], \"amount\": 3, \"size\": \"27 Bytes\", \"storage\": { \"type\": \"s3h\", \"bucket\": \"mlgit-bucket\" } } } As expected the API found 4 entities in the repository (dataset-ex, model-ex, labels-ex, labels-ex2). 3.2 - Get Entity Versions \u00b6 The get_entity_version method allows the user to get a list of spec versions found for an especific entity. selected_entity = project_entities [ 3 ] entity_versions = manager . get_entity_versions ( selected_entity . name , selected_entity . metadata . full_name ) print ( \"Versions found: {} \" . format ( len ( entity_versions ))) print ( \"Example of output object: \\n {} \" . format ( entity_versions [ len ( entity_versions ) - 1 ])) Versions found: 3 Example of output object: { \"version\": 1, \"tag\": \"test__model-ex__1\", \"mutability\": \"flexible\", \"categories\": [ \"test\" ], \"amount\": 1, \"size\": \"9 Bytes\", \"storage\": { \"type\": \"s3h\", \"bucket\": \"mlgit-bucket\" } } As expected the API found 3 versions for the model-ex entity. 3.3 - Get Linked Entities \u00b6 The get_linked_entities method allows the user to get a list of linked entities found for an entity in a specific version. entity_version = 1 linked_entities_in_version = manager . get_linked_entities ( selected_entity . name , entity_version , selected_entity . metadata . full_name ) print ( \"Output: \\n {} \" . format ( linked_entities_in_version )) Output: [{ \"tag\": \"test__dataset-ex__1\", \"name\": \"dataset-ex\", \"version\": \"1\", \"entity_type\": \"dataset\" }, { \"tag\": \"test__labels-ex__1\", \"name\": \"labels-ex\", \"version\": \"1\", \"entity_type\": \"labels\" }] If we go back to the diagram, we can see that as shown in the output, version 1 of the model-ex entity is related to dataset-ex in version 1 and labels-ex in version 1. 3.4 - Get Entity Relationships \u00b6 The get_linked_entities method allows the user to get the list of all relationships that the specific entity has. For this it goes through all versions of the entity and checks the relationships that have been established. entity_relationships = manager . get_entity_relationships ( selected_entity . name , selected_entity . metadata . full_name ) count_relationships = 0 for version in entity_relationships [ selected_entity . name ]: count_relationships += len ( version . relationships ) print ( \"Relationships found: {} \" . format ( count_relationships )) print ( \"Example of output object: \\n {} \" . format ( entity_relationships [ selected_entity . name ][ 0 ])) Relationships found: 6 Example of output object: { \"version\": 3, \"tag\": \"test__model-ex__3\", \"relationships\": [ { \"tag\": \"test__dataset-ex__3\", \"name\": \"dataset-ex\", \"version\": \"3\", \"entity_type\": \"dataset\" }, { \"tag\": \"test__labels-ex2__2\", \"name\": \"labels-ex2\", \"version\": \"2\", \"entity_type\": \"labels\" } ] } In addition, this command allows the user to define the output format, which can be json (as in the previous example) or CSV. If he wants, he can also define the export_path to export the data to a file. An example of how to use the generated csv can be seen below: import pandas as pd entity_relationships_csv = manager . get_entity_relationships ( selected_entity . name , selected_entity . metadata . full_name , export_type = 'csv' ) df = pd . read_csv ( entity_relationships_csv ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } from_tag from_name from_version from_type to_tag to_name to_version to_type 0 test__model-ex__3 model-ex 3 model test__dataset-ex__3 dataset-ex 3 dataset 1 test__model-ex__3 model-ex 3 model test__labels-ex2__2 labels-ex2 2 labels 2 test__model-ex__2 model-ex 2 model test__dataset-ex__1 dataset-ex 1 dataset 3 test__model-ex__2 model-ex 2 model test__labels-ex__2 labels-ex 2 labels 4 test__model-ex__1 model-ex 1 model test__dataset-ex__1 dataset-ex 1 dataset 5 test__model-ex__1 model-ex 1 model test__labels-ex__1 labels-ex 1 labels 3.5 - Get Project Entities Relationships \u00b6 Like the previous command, the get_project_entities_relationships command aims to present the entity relationships, but with this single command the user can capture the relationships of all entities that are in the project. In our case we have 4 versioned entities, so the command will check the relationships of these 4 entities. project_entities_relationships = manager . get_project_entities_relationships ( config_repository_name ) count_relationships = 0 for entity in project_entities_relationships : for version in project_entities_relationships [ entity ]: count_relationships += len ( version . relationships ) print ( \"Relationships found: {} \" . format ( count_relationships )) print ( \"Example of output object: \\n {} \" . format ( project_entities_relationships [ entity ][ 0 ])) Relationships found: 10 Example of output object: { \"version\": 3, \"tag\": \"test__model-ex__3\", \"relationships\": [ { \"tag\": \"test__dataset-ex__3\", \"name\": \"dataset-ex\", \"version\": \"3\", \"entity_type\": \"dataset\" }, { \"tag\": \"test__labels-ex2__2\", \"name\": \"labels-ex2\", \"version\": \"2\", \"entity_type\": \"labels\" } ] } Like the previous one, it is possible to export the result in csv. project_entities_relationships_csv = manager . get_project_entities_relationships ( config_repository_name , export_type = 'csv' ) df = pd . read_csv ( project_entities_relationships_csv ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } from_tag from_name from_version from_type to_tag to_name to_version to_type 0 test__labels-ex2__2 labels-ex2 2 labels test__dataset-ex__3 dataset-ex 3 dataset 1 test__labels-ex2__1 labels-ex2 1 labels test__dataset-ex__3 dataset-ex 3 dataset 2 test__labels-ex__2 labels-ex 2 labels test__dataset-ex__1 dataset-ex 1 dataset 3 test__labels-ex__1 labels-ex 1 labels test__dataset-ex__1 dataset-ex 1 dataset 4 test__model-ex__3 model-ex 3 model test__dataset-ex__3 dataset-ex 3 dataset 5 test__model-ex__3 model-ex 3 model test__labels-ex2__2 labels-ex2 2 labels 6 test__model-ex__2 model-ex 2 model test__dataset-ex__1 dataset-ex 1 dataset 7 test__model-ex__2 model-ex 2 model test__labels-ex__2 labels-ex 2 labels 8 test__model-ex__1 model-ex 1 model test__dataset-ex__1 dataset-ex 1 dataset 9 test__model-ex__1 model-ex 1 model test__labels-ex__1 labels-ex 1 labels As expected, all the relationships that were highlighted in the diagram were captured by the API.","title":"Relationship API Commands"},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/#relationships-api-methods","text":"This notebook describes a basic flow in the context of relationships between entities with the API provided by ML-Git. In it, we'll show you how to use the set of commands provided. You can check the documentation for more information: API documentation","title":"Relationships API methods"},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/#notebook-prerequisites","text":"This notebook uses the GitHub API to access a reset and does Ml-Git operations on its contents, so before running this notebook, take the following steps: Have a GitHub SHH access key so that you can use the repository information retrieval API. Have a GitHub repository that the SHH key has access to.","title":"Notebook prerequisites"},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/#1-context","text":"In this notebook we consider a scenario of an ML-Git project with the following settings: A versioned config file in GitHub. Pointing to the entities' metadata repositories. Each entity type having its metadata repository. One mode entity (model-ex), two labels entities (labels-ex and labels-ex2) and one dataset entity (dataset-ex) Entities have relationships defined at versioning time. This settings mentioned above can be better visualized in the diagram below:","title":"1 - Context"},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/#2-configuring","text":"To use the methods, you will need to import the API and define some constants related to the user's credential: Below are the constants described in the pre-requirements section, where:['removed'] should be replaced by the SHH access key and api_url can be modified if necessary as reported in the GitHub API documentation. from ml_git.api import MLGitAPI from ml_git import api github_token = [ 'removed' ] api_url = 'https://api.github.com' After defining the variables to configure, it will be possible to start a manager that will be responsible for operating on the github API. api = MLGitAPI () manager = api . init_entity_manager ( github_token , api_url ) We will use the manager to execute the commands in the next steps.","title":"2 - Configuring"},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/#3-methods","text":"","title":"3 - Methods"},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/#31-get-entities","text":"The get_entities method allows the user to get a list of entities being versioned in a project. For this, the user must inform the path to the configuration file, whether this path is a local directory or the name of a git repository. The path can be modified using the config_repository_name field, in our example case the configuration file is in 'user/mlgit-config-repository. config_repository_name = 'user/mlgit-config-repository' project_entities = manager . get_entities ( config_repo_name = config_repository_name ) print ( \"Entities found: {} \" . format ( len ( project_entities ))) print ( \"Example of output object: \\n {} \" . format ( project_entities [ 3 ])) Entities found: 4 Example of output object: { \"name\": \"model-ex\", \"entity_type\": \"model\", \"metadata\": { \"full_name\": \"user/mlgit-models\", \"git_url\": \"git@github.com:user/mlgit-models.git\", \"html_url\": \"https://github.com/user/mlgit-models\", \"owner_email\": \"user@gmail.com\", \"owner_name\": \"User Name\" }, \"last_spec_version\": { \"version\": 3, \"tag\": \"test__model-ex__3\", \"mutability\": \"flexible\", \"categories\": [ \"test\" ], \"amount\": 3, \"size\": \"27 Bytes\", \"storage\": { \"type\": \"s3h\", \"bucket\": \"mlgit-bucket\" } } } As expected the API found 4 entities in the repository (dataset-ex, model-ex, labels-ex, labels-ex2).","title":"3.1 - Get Entities"},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/#32-get-entity-versions","text":"The get_entity_version method allows the user to get a list of spec versions found for an especific entity. selected_entity = project_entities [ 3 ] entity_versions = manager . get_entity_versions ( selected_entity . name , selected_entity . metadata . full_name ) print ( \"Versions found: {} \" . format ( len ( entity_versions ))) print ( \"Example of output object: \\n {} \" . format ( entity_versions [ len ( entity_versions ) - 1 ])) Versions found: 3 Example of output object: { \"version\": 1, \"tag\": \"test__model-ex__1\", \"mutability\": \"flexible\", \"categories\": [ \"test\" ], \"amount\": 1, \"size\": \"9 Bytes\", \"storage\": { \"type\": \"s3h\", \"bucket\": \"mlgit-bucket\" } } As expected the API found 3 versions for the model-ex entity.","title":"3.2 - Get Entity Versions"},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/#33-get-linked-entities","text":"The get_linked_entities method allows the user to get a list of linked entities found for an entity in a specific version. entity_version = 1 linked_entities_in_version = manager . get_linked_entities ( selected_entity . name , entity_version , selected_entity . metadata . full_name ) print ( \"Output: \\n {} \" . format ( linked_entities_in_version )) Output: [{ \"tag\": \"test__dataset-ex__1\", \"name\": \"dataset-ex\", \"version\": \"1\", \"entity_type\": \"dataset\" }, { \"tag\": \"test__labels-ex__1\", \"name\": \"labels-ex\", \"version\": \"1\", \"entity_type\": \"labels\" }] If we go back to the diagram, we can see that as shown in the output, version 1 of the model-ex entity is related to dataset-ex in version 1 and labels-ex in version 1.","title":"3.3 - Get Linked Entities"},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/#34-get-entity-relationships","text":"The get_linked_entities method allows the user to get the list of all relationships that the specific entity has. For this it goes through all versions of the entity and checks the relationships that have been established. entity_relationships = manager . get_entity_relationships ( selected_entity . name , selected_entity . metadata . full_name ) count_relationships = 0 for version in entity_relationships [ selected_entity . name ]: count_relationships += len ( version . relationships ) print ( \"Relationships found: {} \" . format ( count_relationships )) print ( \"Example of output object: \\n {} \" . format ( entity_relationships [ selected_entity . name ][ 0 ])) Relationships found: 6 Example of output object: { \"version\": 3, \"tag\": \"test__model-ex__3\", \"relationships\": [ { \"tag\": \"test__dataset-ex__3\", \"name\": \"dataset-ex\", \"version\": \"3\", \"entity_type\": \"dataset\" }, { \"tag\": \"test__labels-ex2__2\", \"name\": \"labels-ex2\", \"version\": \"2\", \"entity_type\": \"labels\" } ] } In addition, this command allows the user to define the output format, which can be json (as in the previous example) or CSV. If he wants, he can also define the export_path to export the data to a file. An example of how to use the generated csv can be seen below: import pandas as pd entity_relationships_csv = manager . get_entity_relationships ( selected_entity . name , selected_entity . metadata . full_name , export_type = 'csv' ) df = pd . read_csv ( entity_relationships_csv ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } from_tag from_name from_version from_type to_tag to_name to_version to_type 0 test__model-ex__3 model-ex 3 model test__dataset-ex__3 dataset-ex 3 dataset 1 test__model-ex__3 model-ex 3 model test__labels-ex2__2 labels-ex2 2 labels 2 test__model-ex__2 model-ex 2 model test__dataset-ex__1 dataset-ex 1 dataset 3 test__model-ex__2 model-ex 2 model test__labels-ex__2 labels-ex 2 labels 4 test__model-ex__1 model-ex 1 model test__dataset-ex__1 dataset-ex 1 dataset 5 test__model-ex__1 model-ex 1 model test__labels-ex__1 labels-ex 1 labels","title":"3.4 - Get Entity Relationships"},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/#35-get-project-entities-relationships","text":"Like the previous command, the get_project_entities_relationships command aims to present the entity relationships, but with this single command the user can capture the relationships of all entities that are in the project. In our case we have 4 versioned entities, so the command will check the relationships of these 4 entities. project_entities_relationships = manager . get_project_entities_relationships ( config_repository_name ) count_relationships = 0 for entity in project_entities_relationships : for version in project_entities_relationships [ entity ]: count_relationships += len ( version . relationships ) print ( \"Relationships found: {} \" . format ( count_relationships )) print ( \"Example of output object: \\n {} \" . format ( project_entities_relationships [ entity ][ 0 ])) Relationships found: 10 Example of output object: { \"version\": 3, \"tag\": \"test__model-ex__3\", \"relationships\": [ { \"tag\": \"test__dataset-ex__3\", \"name\": \"dataset-ex\", \"version\": \"3\", \"entity_type\": \"dataset\" }, { \"tag\": \"test__labels-ex2__2\", \"name\": \"labels-ex2\", \"version\": \"2\", \"entity_type\": \"labels\" } ] } Like the previous one, it is possible to export the result in csv. project_entities_relationships_csv = manager . get_project_entities_relationships ( config_repository_name , export_type = 'csv' ) df = pd . read_csv ( project_entities_relationships_csv ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } from_tag from_name from_version from_type to_tag to_name to_version to_type 0 test__labels-ex2__2 labels-ex2 2 labels test__dataset-ex__3 dataset-ex 3 dataset 1 test__labels-ex2__1 labels-ex2 1 labels test__dataset-ex__3 dataset-ex 3 dataset 2 test__labels-ex__2 labels-ex 2 labels test__dataset-ex__1 dataset-ex 1 dataset 3 test__labels-ex__1 labels-ex 1 labels test__dataset-ex__1 dataset-ex 1 dataset 4 test__model-ex__3 model-ex 3 model test__dataset-ex__3 dataset-ex 3 dataset 5 test__model-ex__3 model-ex 3 model test__labels-ex2__2 labels-ex2 2 labels 6 test__model-ex__2 model-ex 2 model test__dataset-ex__1 dataset-ex 1 dataset 7 test__model-ex__2 model-ex 2 model test__labels-ex__2 labels-ex 2 labels 8 test__model-ex__1 model-ex 1 model test__dataset-ex__1 dataset-ex 1 dataset 9 test__model-ex__1 model-ex 1 model test__labels-ex__1 labels-ex 1 labels As expected, all the relationships that were highlighted in the diagram were captured by the API.","title":"3.5 - Get Project Entities Relationships"},{"location":"tabular_data/tabular_data/","text":"Working with tabular data \u00b6 What is tabular data? \u00b6 For most people working with small amounts of data, the data table is the fundamental unit of organization. The data table, arguably the oldest data structure, is both a way of organizing data for processing by machines and of presenting data visually for consumption by humans. Tabular data is data that is structured into rows, each of which contains information about some thing. Each row contains the same number of cells (although some of these cells may be empty), which provide values of properties of the thing described by the row. In tabular data, cells within the same column provide values for the same property of the things described by each row. This is what differentiates tabular data from other line-oriented formats. An example of a tabular data structure can be seen below: Versioning \u00b6 Due to the way the data is versioned by ML-Git (see internals documentation ) the data organization structure can influence the performance and optimization of the data storage that ML-Git has. When ML-Git is dealing with tabular data, in order to obtain higher storage usage efficiency, it is recommended to avoid actions that edit data that were previously added.\u200b We strongly recommend that the user organize their data in such a way that the entry of new data into the set is done without changing the data already added. Examples of this type of organization is to partition the data by insertion date. This way, each partition should not be modified by future data insertions.\u200b One good way how we can achieve partitioning is using the folders structure to split data in different physical sets, even with several levels, with a part of the information of the table. As we can see in the picture, the name of each folder should contain the concrete value of the column and optionally also the name of the column. Some criteria must be met when choosing the key partition columns: Be used very frequently with the same conditions. Time-based data: combination of year, month, and day associated with time values. Location-based data: geographic region data associated with some place. Have a reasonable number of different values (cardinality). The number of possible values has to be reasonable to gain efficiency splitting the data. For example a valid range could be between 10 and 1000. Adding or modifying the data \u00b6 Once your data is versioned as suggested in the previous section, you may at some point wish to add new data to this dataset. Whenever this type of operation is to be performed, try to take into consideration editing the smallest number of files that have already been versioned. The increment of new data must be given by the creation of new files. One way to make these changes without modifying the data is to use the append save mode if you are working with parquet data. Using append save mode, you can append a dataframe to an existing parquet file. See more in this link . Note: In exploratory tests it was observed that the use of parquet data with the append writing mode is the most efficient in terms of performance and optimization for ML-Git, since this writing mode avoids the modification of previous files. If you are working with another type of data, such as CSV, whenever new data is added to your dataset you must create a new file for that data. Note: CSV format files are generally not recommended for large volumes of data. It is recommended to use a more efficient data structure, such as parquet.","title":"Working With Tabular Data"},{"location":"tabular_data/tabular_data/#working-with-tabular-data","text":"","title":"Working with tabular data"},{"location":"tabular_data/tabular_data/#what-is-tabular-data","text":"For most people working with small amounts of data, the data table is the fundamental unit of organization. The data table, arguably the oldest data structure, is both a way of organizing data for processing by machines and of presenting data visually for consumption by humans. Tabular data is data that is structured into rows, each of which contains information about some thing. Each row contains the same number of cells (although some of these cells may be empty), which provide values of properties of the thing described by the row. In tabular data, cells within the same column provide values for the same property of the things described by each row. This is what differentiates tabular data from other line-oriented formats. An example of a tabular data structure can be seen below:","title":" What is tabular data? "},{"location":"tabular_data/tabular_data/#versioning","text":"Due to the way the data is versioned by ML-Git (see internals documentation ) the data organization structure can influence the performance and optimization of the data storage that ML-Git has. When ML-Git is dealing with tabular data, in order to obtain higher storage usage efficiency, it is recommended to avoid actions that edit data that were previously added.\u200b We strongly recommend that the user organize their data in such a way that the entry of new data into the set is done without changing the data already added. Examples of this type of organization is to partition the data by insertion date. This way, each partition should not be modified by future data insertions.\u200b One good way how we can achieve partitioning is using the folders structure to split data in different physical sets, even with several levels, with a part of the information of the table. As we can see in the picture, the name of each folder should contain the concrete value of the column and optionally also the name of the column. Some criteria must be met when choosing the key partition columns: Be used very frequently with the same conditions. Time-based data: combination of year, month, and day associated with time values. Location-based data: geographic region data associated with some place. Have a reasonable number of different values (cardinality). The number of possible values has to be reasonable to gain efficiency splitting the data. For example a valid range could be between 10 and 1000.","title":" Versioning "},{"location":"tabular_data/tabular_data/#adding-or-modifying-the-data","text":"Once your data is versioned as suggested in the previous section, you may at some point wish to add new data to this dataset. Whenever this type of operation is to be performed, try to take into consideration editing the smallest number of files that have already been versioned. The increment of new data must be given by the creation of new files. One way to make these changes without modifying the data is to use the append save mode if you are working with parquet data. Using append save mode, you can append a dataframe to an existing parquet file. See more in this link . Note: In exploratory tests it was observed that the use of parquet data with the append writing mode is the most efficient in terms of performance and optimization for ML-Git, since this writing mode avoids the modification of previous files. If you are working with another type of data, such as CSV, whenever new data is added to your dataset you must create a new file for that data. Note: CSV format files are generally not recommended for large volumes of data. It is recommended to use a more efficient data structure, such as parquet.","title":" Adding or modifying the data "}]}